{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "567f0958",
   "metadata": {},
   "source": [
    "# Set Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "071c5f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "import random\n",
    "\n",
    "# Full deck with distinct face cards\n",
    "CARDS = [1, 2, 3, 4, 5, 6, 7, 8, 9, '10', 'J', 'Q', 'K'] * 4\n",
    "\n",
    "def card_value(card):\n",
    "    return 10 if card in ['10', 'J', 'Q', 'K'] else card\n",
    "\n",
    "def draw_card(deck):\n",
    "    return deck.pop()\n",
    "\n",
    "def draw_hand(deck):\n",
    "    return [draw_card(deck), draw_card(deck)]\n",
    "\n",
    "def usable_ace(hand):\n",
    "    return 1 in hand and sum(card_value(c) for c in hand) + 10 <= 21\n",
    "\n",
    "def sum_hand(hand):\n",
    "    total = sum(card_value(c) for c in hand)\n",
    "    return total + 10 if usable_ace(hand) else total\n",
    "\n",
    "def is_bust(hand):\n",
    "    return sum_hand(hand) > 21\n",
    "\n",
    "def score(hand):\n",
    "    return 0 if is_bust(hand) else sum_hand(hand)\n",
    "\n",
    "def is_natural(hand):\n",
    "    return set(hand) == {1, '10'} or set(hand) == {1, 'J'} or set(hand) == {1, 'Q'} or set(hand) == {1, 'K'}\n",
    "\n",
    "def can_double_down(hand, actionstaken):\n",
    "    return len(hand) == 2 and actionstaken == 0\n",
    "\n",
    "class BlackjackEnv(gym.Env):\n",
    "    metadata = {\"render.modes\": [\"human\"]}\n",
    "\n",
    "    def __init__(self, numdecks=4, natural=True):\n",
    "        super().__init__()\n",
    "        self.action_space = spaces.Discrete(4)  # 0: Stick, 1: Hit, 2: Double Down, 3: Split\n",
    "        self.observation_space = spaces.Tuple((\n",
    "            spaces.Tuple((spaces.Discrete(32), spaces.Discrete(32))),  # Player hand (2 cards)\n",
    "            spaces.Discrete(11),  # Dealer's showing card\n",
    "            spaces.Discrete(2),   # Usable ace\n",
    "            spaces.Discrete(2)    # Can double down\n",
    "        ))\n",
    "\n",
    "        self.natural = natural\n",
    "        self.numdecks = numdecks\n",
    "        self.decks = CARDS * self.numdecks\n",
    "        random.shuffle(self.decks)\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        random.seed(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        if seed is not None:\n",
    "            self.seed(seed)\n",
    "\n",
    "        if self._deck_is_out():\n",
    "            self.decks = CARDS * self.numdecks\n",
    "            random.shuffle(self.decks)\n",
    "\n",
    "        self.dealer = draw_hand(self.decks)\n",
    "        first_hand = draw_hand(self.decks)\n",
    "        self.hands = [first_hand]\n",
    "        self.current_hand = 0\n",
    "        self.actionstaken = 0\n",
    "        self.hand_results = []\n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action), f\"Invalid action: {action}\"\n",
    "        if self._deck_is_out():\n",
    "            self.decks = CARDS * self.numdecks\n",
    "            random.shuffle(self.decks)\n",
    "\n",
    "        done = False\n",
    "        reward = 0\n",
    "        hand = self.hands[self.current_hand]\n",
    "\n",
    "        if action == 0:  # Stick\n",
    "            self._finalize_current_hand()\n",
    "\n",
    "        elif action == 1:  # Hit\n",
    "            hand.append(draw_card(self.decks))\n",
    "            if is_bust(hand):\n",
    "                self.hand_results.append(-1)\n",
    "                self._advance_hand()\n",
    "\n",
    "        elif action == 2:  # Double Down\n",
    "            if not can_double_down(hand, self.actionstaken):\n",
    "                raise ValueError(\"Invalid double down attempt.\")\n",
    "            hand.append(draw_card(self.decks))\n",
    "            if is_bust(hand):\n",
    "                self.hand_results.append(-2)\n",
    "            else:\n",
    "                self._finalize_current_hand(double=True)\n",
    "\n",
    "        elif action == 3:  # Split\n",
    "            if len(hand) != 2 or hand[0] != hand[1]:\n",
    "                raise ValueError(\"Invalid split attempt.\")\n",
    "            card = hand[0]\n",
    "            self.hands[self.current_hand] = [card, draw_card(self.decks)]\n",
    "            self.hands.insert(self.current_hand + 1, [card, draw_card(self.decks)])\n",
    "\n",
    "        self.actionstaken += 1\n",
    "\n",
    "        if self.current_hand >= len(self.hands):\n",
    "            while sum_hand(self.dealer) < 17:\n",
    "                self.dealer.append(draw_card(self.decks))\n",
    "\n",
    "            if len(self.hand_results) < len(self.hands):\n",
    "                self._finalize_current_hand()\n",
    "\n",
    "            reward = sum(self.hand_results)\n",
    "            done = True\n",
    "        \n",
    "        return self._get_obs(), reward, done, {}\n",
    "\n",
    "    def _finalize_current_hand(self, double=False):\n",
    "        hand = self.hands[self.current_hand]\n",
    "        player_score = score(hand)\n",
    "        dealer_score = score(self.dealer)\n",
    "        result = float(player_score > dealer_score) - float(player_score < dealer_score)\n",
    "        if is_natural(hand) and result == 1 and self.natural:\n",
    "            result = 1.5\n",
    "        self.hand_results.append(result * (2 if double else 1))\n",
    "        self._advance_hand()\n",
    "\n",
    "    def _advance_hand(self):\n",
    "        self.current_hand += 1\n",
    "        self.actionstaken = 0\n",
    "\n",
    "    def _get_obs(self):\n",
    "        if self.current_hand >= len(self.hands):\n",
    "            return ((0, 0), self.dealer[0], 0, 0)\n",
    "\n",
    "        hand = self.hands[self.current_hand]\n",
    "        padded = hand[:2] + [0] * (2 - len(hand))\n",
    "        return (\n",
    "            tuple(card_value(c) if c != 0 else 0 for c in padded[:2]),\n",
    "            card_value(self.dealer[0]),\n",
    "            usable_ace(hand),\n",
    "            can_double_down(hand, self.actionstaken)\n",
    "        )\n",
    "\n",
    "    def _deck_is_out(self):\n",
    "        return len(self.decks) < self.numdecks * len(CARDS) * 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5901ee",
   "metadata": {},
   "source": [
    "# Set the Simple DQN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf99e25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import copy\n",
    "import os\n",
    "\n",
    "# === Q-Network ===\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# === Preprocess BlackjackEnv State ===\n",
    "def preprocess_state(state):\n",
    "    player_cards, dealer_card, usable_ace, can_double = state\n",
    "\n",
    "    def card_value(card):\n",
    "        return 10 if card in ['10', 'J', 'Q', 'K'] else card\n",
    "\n",
    "    dealer_value = card_value(dealer_card)\n",
    "    player_sum = sum(card_value(c) for c in player_cards if c != 0)\n",
    "\n",
    "    return np.array([player_sum, dealer_value, int(usable_ace), int(can_double)], dtype=np.float32)\n",
    "\n",
    "# === Epsilon-Greedy with Action Masking (Device-aware) ===\n",
    "def select_action(state, q_network, epsilon, env, device):\n",
    "    player_cards, dealer_card, usable_ace, can_double = state\n",
    "    valid_actions = [0, 1]  # Stick, Hit\n",
    "\n",
    "    def card_value(card):\n",
    "        return 10 if card in ['10', 'J', 'Q', 'K'] else card\n",
    "\n",
    "    if can_double:\n",
    "        valid_actions.append(2)\n",
    "\n",
    "    # Check if split is allowed\n",
    "    if env.current_hand < len(env.hands):\n",
    "        current_hand = env.hands[env.current_hand]\n",
    "        if len(current_hand) == 2 and card_value(current_hand[0]) == card_value(current_hand[1]):\n",
    "            valid_actions.append(3)\n",
    "\n",
    "    # Epsilon-greedy strategy\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice(valid_actions)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(preprocess_state(state)).unsqueeze(0).to(device)\n",
    "            q_values = q_network(state_tensor)\n",
    "\n",
    "            for action in range(env.action_space.n):\n",
    "                if action not in valid_actions:\n",
    "                    q_values[0, action] = float('-inf')\n",
    "\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "# === DQN Training Loop (Device-aware) ===\n",
    "def train_dqn(env, n_episodes=5000, gamma=0.99, lr=1e-3, batch_size=64,\n",
    "              epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995,\n",
    "              model_save_path='best_blackjack_dqn.pth'):\n",
    "\n",
    "    input_dim = 4  # [player_sum, dealer_value, usable_ace, can_double]\n",
    "    output_dim = env.action_space.n\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    q_network = QNetwork(input_dim, output_dim).to(device)\n",
    "    target_network = copy.deepcopy(q_network).to(device)\n",
    "    target_network.eval()\n",
    "\n",
    "    optimizer = optim.Adam(q_network.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    replay_buffer = deque(maxlen=10000)\n",
    "    epsilon = epsilon_start\n",
    "    steps_done = 0\n",
    "    target_update_freq = 1000\n",
    "\n",
    "    losses = []\n",
    "    reward_window = deque(maxlen=100)\n",
    "    best_avg_reward = float('-inf')\n",
    "    best_model = None\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = select_action(state, q_network, epsilon, env, device)\n",
    "\n",
    "            try:\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "            except ValueError:\n",
    "                # If invalid action, fallback to stick\n",
    "                action = 0\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            episode_reward += reward\n",
    "            replay_buffer.append((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            steps_done += 1\n",
    "\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                batch = random.sample(replay_buffer, batch_size)\n",
    "                states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "                states_tensor = torch.FloatTensor(np.array([preprocess_state(s) for s in states])).to(device)\n",
    "                next_states_tensor = torch.FloatTensor(np.array([preprocess_state(s) for s in next_states])).to(device)\n",
    "                actions_tensor = torch.LongTensor(actions).unsqueeze(1).to(device)\n",
    "                rewards_tensor = torch.FloatTensor(rewards).unsqueeze(1).to(device)\n",
    "                dones_tensor = torch.BoolTensor(dones).unsqueeze(1).to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    next_q_values = target_network(next_states_tensor).max(1, keepdim=True)[0]\n",
    "                    targets = rewards_tensor + gamma * next_q_values * (~dones_tensor)\n",
    "\n",
    "                q_values = q_network(states_tensor).gather(1, actions_tensor)\n",
    "\n",
    "                loss = loss_fn(q_values, targets)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                losses.append(loss.item())\n",
    "\n",
    "            if steps_done % target_update_freq == 0:\n",
    "                target_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "        reward_window.append(episode_reward)\n",
    "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "\n",
    "        if len(reward_window) == reward_window.maxlen:\n",
    "            avg_reward = np.mean(reward_window)\n",
    "            if avg_reward > best_avg_reward:\n",
    "                best_avg_reward = avg_reward\n",
    "                best_model = copy.deepcopy(q_network)\n",
    "                torch.save({\n",
    "                    'model_state_dict': best_model.state_dict(),\n",
    "                    'avg_reward': best_avg_reward,\n",
    "                    'episode': episode + 1\n",
    "                }, model_save_path)\n",
    "\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            print(f\"ğŸ“˜ Episode {episode+1} | Epsilon: {epsilon:.4f} | Avg Reward: {np.mean(reward_window):.2f}\")\n",
    "\n",
    "    if os.path.exists(model_save_path):\n",
    "        checkpoint = torch.load(model_save_path, map_location=device)\n",
    "        q_network.load_state_dict(checkpoint['model_state_dict'])\n",
    "        q_network.eval()\n",
    "        print(f\"âœ… Loaded best model from episode {checkpoint.get('episode', 'N/A')} | Avg Reward: {checkpoint.get('avg_reward', 'N/A'):.2f}\")\n",
    "    else:\n",
    "        print(\"âš ï¸ No saved model found.\")\n",
    "\n",
    "    return best_model if best_model else q_network, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b18a3743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training model for 1 deck(s) ===\n",
      "ğŸ“˜ Episode 100 | Epsilon: 0.6058 | Avg Reward: -0.26\n",
      "ğŸ“˜ Episode 200 | Epsilon: 0.3670 | Avg Reward: -0.61\n",
      "ğŸ“˜ Episode 300 | Epsilon: 0.2223 | Avg Reward: -0.43\n",
      "ğŸ“˜ Episode 400 | Epsilon: 0.1347 | Avg Reward: -0.65\n",
      "ğŸ“˜ Episode 500 | Epsilon: 0.0816 | Avg Reward: -0.61\n",
      "ğŸ“˜ Episode 600 | Epsilon: 0.0494 | Avg Reward: -0.54\n",
      "ğŸ“˜ Episode 700 | Epsilon: 0.0299 | Avg Reward: -0.76\n",
      "ğŸ“˜ Episode 800 | Epsilon: 0.0181 | Avg Reward: -0.19\n",
      "ğŸ“˜ Episode 900 | Epsilon: 0.0110 | Avg Reward: -0.50\n",
      "ğŸ“˜ Episode 1000 | Epsilon: 0.0100 | Avg Reward: -0.73\n",
      "ğŸ“˜ Episode 1100 | Epsilon: 0.0100 | Avg Reward: -0.44\n",
      "ğŸ“˜ Episode 1200 | Epsilon: 0.0100 | Avg Reward: -0.51\n",
      "ğŸ“˜ Episode 1300 | Epsilon: 0.0100 | Avg Reward: -0.62\n",
      "ğŸ“˜ Episode 1400 | Epsilon: 0.0100 | Avg Reward: -0.54\n",
      "ğŸ“˜ Episode 1500 | Epsilon: 0.0100 | Avg Reward: -0.43\n",
      "ğŸ“˜ Episode 1600 | Epsilon: 0.0100 | Avg Reward: -0.27\n",
      "ğŸ“˜ Episode 1700 | Epsilon: 0.0100 | Avg Reward: -0.50\n",
      "ğŸ“˜ Episode 1800 | Epsilon: 0.0100 | Avg Reward: -0.45\n",
      "ğŸ“˜ Episode 1900 | Epsilon: 0.0100 | Avg Reward: -0.49\n",
      "ğŸ“˜ Episode 2000 | Epsilon: 0.0100 | Avg Reward: -0.13\n",
      "ğŸ“˜ Episode 2100 | Epsilon: 0.0100 | Avg Reward: -0.70\n",
      "ğŸ“˜ Episode 2200 | Epsilon: 0.0100 | Avg Reward: -0.51\n",
      "ğŸ“˜ Episode 2300 | Epsilon: 0.0100 | Avg Reward: -0.49\n",
      "ğŸ“˜ Episode 2400 | Epsilon: 0.0100 | Avg Reward: -0.56\n",
      "ğŸ“˜ Episode 2500 | Epsilon: 0.0100 | Avg Reward: 0.07\n",
      "ğŸ“˜ Episode 2600 | Epsilon: 0.0100 | Avg Reward: -0.22\n",
      "ğŸ“˜ Episode 2700 | Epsilon: 0.0100 | Avg Reward: -0.32\n",
      "ğŸ“˜ Episode 2800 | Epsilon: 0.0100 | Avg Reward: -0.27\n",
      "ğŸ“˜ Episode 2900 | Epsilon: 0.0100 | Avg Reward: 0.02\n",
      "ğŸ“˜ Episode 3000 | Epsilon: 0.0100 | Avg Reward: -0.49\n",
      "ğŸ“˜ Episode 3100 | Epsilon: 0.0100 | Avg Reward: -0.37\n",
      "ğŸ“˜ Episode 3200 | Epsilon: 0.0100 | Avg Reward: -0.20\n",
      "ğŸ“˜ Episode 3300 | Epsilon: 0.0100 | Avg Reward: -0.34\n",
      "ğŸ“˜ Episode 3400 | Epsilon: 0.0100 | Avg Reward: -0.08\n",
      "ğŸ“˜ Episode 3500 | Epsilon: 0.0100 | Avg Reward: -0.35\n",
      "ğŸ“˜ Episode 3600 | Epsilon: 0.0100 | Avg Reward: -0.07\n",
      "ğŸ“˜ Episode 3700 | Epsilon: 0.0100 | Avg Reward: -0.58\n",
      "ğŸ“˜ Episode 3800 | Epsilon: 0.0100 | Avg Reward: -0.08\n",
      "ğŸ“˜ Episode 3900 | Epsilon: 0.0100 | Avg Reward: 0.07\n",
      "ğŸ“˜ Episode 4000 | Epsilon: 0.0100 | Avg Reward: -0.22\n",
      "ğŸ“˜ Episode 4100 | Epsilon: 0.0100 | Avg Reward: -0.01\n",
      "ğŸ“˜ Episode 4200 | Epsilon: 0.0100 | Avg Reward: 0.03\n",
      "ğŸ“˜ Episode 4300 | Epsilon: 0.0100 | Avg Reward: -0.19\n",
      "ğŸ“˜ Episode 4400 | Epsilon: 0.0100 | Avg Reward: -0.22\n",
      "ğŸ“˜ Episode 4500 | Epsilon: 0.0100 | Avg Reward: -0.07\n",
      "ğŸ“˜ Episode 4600 | Epsilon: 0.0100 | Avg Reward: -0.37\n",
      "ğŸ“˜ Episode 4700 | Epsilon: 0.0100 | Avg Reward: -0.03\n",
      "ğŸ“˜ Episode 4800 | Epsilon: 0.0100 | Avg Reward: 0.12\n",
      "ğŸ“˜ Episode 4900 | Epsilon: 0.0100 | Avg Reward: -0.34\n",
      "ğŸ“˜ Episode 5000 | Epsilon: 0.0100 | Avg Reward: -0.47\n",
      "ğŸ“˜ Episode 5100 | Epsilon: 0.0100 | Avg Reward: -0.20\n",
      "ğŸ“˜ Episode 5200 | Epsilon: 0.0100 | Avg Reward: -0.28\n",
      "ğŸ“˜ Episode 5300 | Epsilon: 0.0100 | Avg Reward: -0.32\n",
      "ğŸ“˜ Episode 5400 | Epsilon: 0.0100 | Avg Reward: -0.37\n",
      "ğŸ“˜ Episode 5500 | Epsilon: 0.0100 | Avg Reward: -0.15\n",
      "ğŸ“˜ Episode 5600 | Epsilon: 0.0100 | Avg Reward: 0.10\n",
      "ğŸ“˜ Episode 5700 | Epsilon: 0.0100 | Avg Reward: -0.01\n",
      "ğŸ“˜ Episode 5800 | Epsilon: 0.0100 | Avg Reward: -0.14\n",
      "ğŸ“˜ Episode 5900 | Epsilon: 0.0100 | Avg Reward: -0.04\n",
      "ğŸ“˜ Episode 6000 | Epsilon: 0.0100 | Avg Reward: -0.29\n",
      "ğŸ“˜ Episode 6100 | Epsilon: 0.0100 | Avg Reward: -0.21\n",
      "ğŸ“˜ Episode 6200 | Epsilon: 0.0100 | Avg Reward: -0.36\n",
      "ğŸ“˜ Episode 6300 | Epsilon: 0.0100 | Avg Reward: -0.38\n",
      "ğŸ“˜ Episode 6400 | Epsilon: 0.0100 | Avg Reward: -0.30\n",
      "ğŸ“˜ Episode 6500 | Epsilon: 0.0100 | Avg Reward: -0.33\n",
      "ğŸ“˜ Episode 6600 | Epsilon: 0.0100 | Avg Reward: -0.18\n",
      "ğŸ“˜ Episode 6700 | Epsilon: 0.0100 | Avg Reward: -0.44\n",
      "ğŸ“˜ Episode 6800 | Epsilon: 0.0100 | Avg Reward: -0.28\n",
      "ğŸ“˜ Episode 6900 | Epsilon: 0.0100 | Avg Reward: -0.35\n",
      "ğŸ“˜ Episode 7000 | Epsilon: 0.0100 | Avg Reward: -0.26\n",
      "ğŸ“˜ Episode 7100 | Epsilon: 0.0100 | Avg Reward: -0.06\n",
      "ğŸ“˜ Episode 7200 | Epsilon: 0.0100 | Avg Reward: -0.03\n",
      "ğŸ“˜ Episode 7300 | Epsilon: 0.0100 | Avg Reward: -0.17\n",
      "ğŸ“˜ Episode 7400 | Epsilon: 0.0100 | Avg Reward: -0.51\n",
      "ğŸ“˜ Episode 7500 | Epsilon: 0.0100 | Avg Reward: -0.48\n",
      "ğŸ“˜ Episode 7600 | Epsilon: 0.0100 | Avg Reward: -0.25\n",
      "ğŸ“˜ Episode 7700 | Epsilon: 0.0100 | Avg Reward: -0.02\n",
      "ğŸ“˜ Episode 7800 | Epsilon: 0.0100 | Avg Reward: -0.41\n",
      "ğŸ“˜ Episode 7900 | Epsilon: 0.0100 | Avg Reward: -0.13\n",
      "ğŸ“˜ Episode 8000 | Epsilon: 0.0100 | Avg Reward: 0.01\n",
      "ğŸ“˜ Episode 8100 | Epsilon: 0.0100 | Avg Reward: 0.08\n",
      "ğŸ“˜ Episode 8200 | Epsilon: 0.0100 | Avg Reward: -0.40\n",
      "ğŸ“˜ Episode 8300 | Epsilon: 0.0100 | Avg Reward: -0.47\n",
      "ğŸ“˜ Episode 8400 | Epsilon: 0.0100 | Avg Reward: 0.03\n",
      "ğŸ“˜ Episode 8500 | Epsilon: 0.0100 | Avg Reward: -0.26\n",
      "ğŸ“˜ Episode 8600 | Epsilon: 0.0100 | Avg Reward: -0.58\n",
      "ğŸ“˜ Episode 8700 | Epsilon: 0.0100 | Avg Reward: -0.29\n",
      "ğŸ“˜ Episode 8800 | Epsilon: 0.0100 | Avg Reward: 0.15\n",
      "ğŸ“˜ Episode 8900 | Epsilon: 0.0100 | Avg Reward: -0.23\n",
      "ğŸ“˜ Episode 9000 | Epsilon: 0.0100 | Avg Reward: -0.31\n",
      "ğŸ“˜ Episode 9100 | Epsilon: 0.0100 | Avg Reward: -0.07\n",
      "ğŸ“˜ Episode 9200 | Epsilon: 0.0100 | Avg Reward: 0.07\n",
      "ğŸ“˜ Episode 9300 | Epsilon: 0.0100 | Avg Reward: -0.07\n",
      "ğŸ“˜ Episode 9400 | Epsilon: 0.0100 | Avg Reward: -0.33\n",
      "ğŸ“˜ Episode 9500 | Epsilon: 0.0100 | Avg Reward: 0.12\n",
      "ğŸ“˜ Episode 9600 | Epsilon: 0.0100 | Avg Reward: -0.28\n",
      "ğŸ“˜ Episode 9700 | Epsilon: 0.0100 | Avg Reward: -0.01\n",
      "ğŸ“˜ Episode 9800 | Epsilon: 0.0100 | Avg Reward: -0.09\n",
      "ğŸ“˜ Episode 9900 | Epsilon: 0.0100 | Avg Reward: 0.13\n",
      "ğŸ“˜ Episode 10000 | Epsilon: 0.0100 | Avg Reward: -0.22\n",
      "âœ… Loaded best model from episode 5640 | Avg Reward: 0.42\n",
      "Completed training for 1 deck(s)\n",
      "\n",
      "=== Training model for 2 deck(s) ===\n",
      "ğŸ“˜ Episode 100 | Epsilon: 0.6058 | Avg Reward: -0.23\n",
      "ğŸ“˜ Episode 200 | Epsilon: 0.3670 | Avg Reward: -0.67\n",
      "ğŸ“˜ Episode 300 | Epsilon: 0.2223 | Avg Reward: -0.94\n",
      "ğŸ“˜ Episode 400 | Epsilon: 0.1347 | Avg Reward: -0.79\n",
      "ğŸ“˜ Episode 500 | Epsilon: 0.0816 | Avg Reward: -0.98\n",
      "ğŸ“˜ Episode 600 | Epsilon: 0.0494 | Avg Reward: -0.81\n",
      "ğŸ“˜ Episode 700 | Epsilon: 0.0299 | Avg Reward: -0.74\n",
      "ğŸ“˜ Episode 800 | Epsilon: 0.0181 | Avg Reward: -0.66\n",
      "ğŸ“˜ Episode 900 | Epsilon: 0.0110 | Avg Reward: -0.85\n",
      "ğŸ“˜ Episode 1000 | Epsilon: 0.0100 | Avg Reward: -0.69\n",
      "ğŸ“˜ Episode 1100 | Epsilon: 0.0100 | Avg Reward: -0.95\n",
      "ğŸ“˜ Episode 1200 | Epsilon: 0.0100 | Avg Reward: -1.22\n",
      "ğŸ“˜ Episode 1300 | Epsilon: 0.0100 | Avg Reward: -1.09\n",
      "ğŸ“˜ Episode 1400 | Epsilon: 0.0100 | Avg Reward: -0.37\n",
      "ğŸ“˜ Episode 1500 | Epsilon: 0.0100 | Avg Reward: -0.47\n",
      "ğŸ“˜ Episode 1600 | Epsilon: 0.0100 | Avg Reward: -0.80\n",
      "ğŸ“˜ Episode 1700 | Epsilon: 0.0100 | Avg Reward: -0.48\n",
      "ğŸ“˜ Episode 1800 | Epsilon: 0.0100 | Avg Reward: -0.69\n",
      "ğŸ“˜ Episode 1900 | Epsilon: 0.0100 | Avg Reward: -0.72\n",
      "ğŸ“˜ Episode 2000 | Epsilon: 0.0100 | Avg Reward: -0.63\n",
      "ğŸ“˜ Episode 2100 | Epsilon: 0.0100 | Avg Reward: -0.82\n",
      "ğŸ“˜ Episode 2200 | Epsilon: 0.0100 | Avg Reward: -1.07\n",
      "ğŸ“˜ Episode 2300 | Epsilon: 0.0100 | Avg Reward: -0.73\n",
      "ğŸ“˜ Episode 2400 | Epsilon: 0.0100 | Avg Reward: -0.90\n",
      "ğŸ“˜ Episode 2500 | Epsilon: 0.0100 | Avg Reward: -0.78\n",
      "ğŸ“˜ Episode 2600 | Epsilon: 0.0100 | Avg Reward: -1.01\n",
      "ğŸ“˜ Episode 2700 | Epsilon: 0.0100 | Avg Reward: -0.81\n",
      "ğŸ“˜ Episode 2800 | Epsilon: 0.0100 | Avg Reward: -0.50\n",
      "ğŸ“˜ Episode 2900 | Epsilon: 0.0100 | Avg Reward: -0.69\n",
      "ğŸ“˜ Episode 3000 | Epsilon: 0.0100 | Avg Reward: -0.81\n",
      "ğŸ“˜ Episode 3100 | Epsilon: 0.0100 | Avg Reward: -0.84\n",
      "ğŸ“˜ Episode 3200 | Epsilon: 0.0100 | Avg Reward: -0.72\n",
      "ğŸ“˜ Episode 3300 | Epsilon: 0.0100 | Avg Reward: -0.48\n",
      "ğŸ“˜ Episode 3400 | Epsilon: 0.0100 | Avg Reward: -0.94\n",
      "ğŸ“˜ Episode 3500 | Epsilon: 0.0100 | Avg Reward: -0.82\n",
      "ğŸ“˜ Episode 3600 | Epsilon: 0.0100 | Avg Reward: -1.14\n",
      "ğŸ“˜ Episode 3700 | Epsilon: 0.0100 | Avg Reward: -0.88\n",
      "ğŸ“˜ Episode 3800 | Epsilon: 0.0100 | Avg Reward: -1.01\n",
      "ğŸ“˜ Episode 3900 | Epsilon: 0.0100 | Avg Reward: -0.99\n",
      "ğŸ“˜ Episode 4000 | Epsilon: 0.0100 | Avg Reward: -0.97\n",
      "ğŸ“˜ Episode 4100 | Epsilon: 0.0100 | Avg Reward: -0.94\n",
      "ğŸ“˜ Episode 4200 | Epsilon: 0.0100 | Avg Reward: -1.02\n",
      "ğŸ“˜ Episode 4300 | Epsilon: 0.0100 | Avg Reward: -0.51\n",
      "ğŸ“˜ Episode 4400 | Epsilon: 0.0100 | Avg Reward: -0.71\n",
      "ğŸ“˜ Episode 4500 | Epsilon: 0.0100 | Avg Reward: -0.93\n",
      "ğŸ“˜ Episode 4600 | Epsilon: 0.0100 | Avg Reward: -0.87\n",
      "ğŸ“˜ Episode 4700 | Epsilon: 0.0100 | Avg Reward: -0.98\n",
      "ğŸ“˜ Episode 4800 | Epsilon: 0.0100 | Avg Reward: -0.97\n",
      "ğŸ“˜ Episode 4900 | Epsilon: 0.0100 | Avg Reward: -0.96\n",
      "ğŸ“˜ Episode 5000 | Epsilon: 0.0100 | Avg Reward: -1.28\n",
      "ğŸ“˜ Episode 5100 | Epsilon: 0.0100 | Avg Reward: -0.83\n",
      "ğŸ“˜ Episode 5200 | Epsilon: 0.0100 | Avg Reward: -0.99\n",
      "ğŸ“˜ Episode 5300 | Epsilon: 0.0100 | Avg Reward: -0.72\n",
      "ğŸ“˜ Episode 5400 | Epsilon: 0.0100 | Avg Reward: -1.27\n",
      "ğŸ“˜ Episode 5500 | Epsilon: 0.0100 | Avg Reward: -0.93\n",
      "ğŸ“˜ Episode 5600 | Epsilon: 0.0100 | Avg Reward: -1.26\n",
      "ğŸ“˜ Episode 5700 | Epsilon: 0.0100 | Avg Reward: -1.20\n",
      "ğŸ“˜ Episode 5800 | Epsilon: 0.0100 | Avg Reward: -0.80\n",
      "ğŸ“˜ Episode 5900 | Epsilon: 0.0100 | Avg Reward: -1.59\n",
      "ğŸ“˜ Episode 6000 | Epsilon: 0.0100 | Avg Reward: -1.25\n",
      "ğŸ“˜ Episode 6100 | Epsilon: 0.0100 | Avg Reward: -1.23\n",
      "ğŸ“˜ Episode 6200 | Epsilon: 0.0100 | Avg Reward: -1.41\n",
      "ğŸ“˜ Episode 6300 | Epsilon: 0.0100 | Avg Reward: -1.20\n",
      "ğŸ“˜ Episode 6400 | Epsilon: 0.0100 | Avg Reward: -1.37\n",
      "ğŸ“˜ Episode 6500 | Epsilon: 0.0100 | Avg Reward: -1.15\n",
      "ğŸ“˜ Episode 6600 | Epsilon: 0.0100 | Avg Reward: -1.30\n",
      "ğŸ“˜ Episode 6700 | Epsilon: 0.0100 | Avg Reward: -1.07\n",
      "ğŸ“˜ Episode 6800 | Epsilon: 0.0100 | Avg Reward: -1.33\n",
      "ğŸ“˜ Episode 6900 | Epsilon: 0.0100 | Avg Reward: -1.37\n",
      "ğŸ“˜ Episode 7000 | Epsilon: 0.0100 | Avg Reward: -1.51\n",
      "ğŸ“˜ Episode 7100 | Epsilon: 0.0100 | Avg Reward: -1.18\n",
      "ğŸ“˜ Episode 7200 | Epsilon: 0.0100 | Avg Reward: -1.28\n",
      "ğŸ“˜ Episode 7300 | Epsilon: 0.0100 | Avg Reward: -1.65\n",
      "ğŸ“˜ Episode 7400 | Epsilon: 0.0100 | Avg Reward: -1.43\n",
      "ğŸ“˜ Episode 7500 | Epsilon: 0.0100 | Avg Reward: -1.39\n",
      "ğŸ“˜ Episode 7600 | Epsilon: 0.0100 | Avg Reward: -1.17\n",
      "ğŸ“˜ Episode 7700 | Epsilon: 0.0100 | Avg Reward: -1.27\n",
      "ğŸ“˜ Episode 7800 | Epsilon: 0.0100 | Avg Reward: -1.62\n",
      "ğŸ“˜ Episode 7900 | Epsilon: 0.0100 | Avg Reward: -1.26\n",
      "ğŸ“˜ Episode 8000 | Epsilon: 0.0100 | Avg Reward: -1.19\n",
      "ğŸ“˜ Episode 8100 | Epsilon: 0.0100 | Avg Reward: -1.41\n",
      "ğŸ“˜ Episode 8200 | Epsilon: 0.0100 | Avg Reward: -1.44\n",
      "ğŸ“˜ Episode 8300 | Epsilon: 0.0100 | Avg Reward: -1.35\n",
      "ğŸ“˜ Episode 8400 | Epsilon: 0.0100 | Avg Reward: -1.21\n",
      "ğŸ“˜ Episode 8500 | Epsilon: 0.0100 | Avg Reward: -1.45\n",
      "ğŸ“˜ Episode 8600 | Epsilon: 0.0100 | Avg Reward: -1.44\n",
      "ğŸ“˜ Episode 8700 | Epsilon: 0.0100 | Avg Reward: -1.39\n",
      "ğŸ“˜ Episode 8800 | Epsilon: 0.0100 | Avg Reward: -1.45\n",
      "ğŸ“˜ Episode 8900 | Epsilon: 0.0100 | Avg Reward: -1.27\n",
      "ğŸ“˜ Episode 9000 | Epsilon: 0.0100 | Avg Reward: -1.40\n",
      "ğŸ“˜ Episode 9100 | Epsilon: 0.0100 | Avg Reward: -1.32\n",
      "ğŸ“˜ Episode 9200 | Epsilon: 0.0100 | Avg Reward: -1.35\n",
      "ğŸ“˜ Episode 9300 | Epsilon: 0.0100 | Avg Reward: -1.54\n",
      "ğŸ“˜ Episode 9400 | Epsilon: 0.0100 | Avg Reward: -1.52\n",
      "ğŸ“˜ Episode 9500 | Epsilon: 0.0100 | Avg Reward: -1.21\n",
      "ğŸ“˜ Episode 9600 | Epsilon: 0.0100 | Avg Reward: -1.43\n",
      "ğŸ“˜ Episode 9700 | Epsilon: 0.0100 | Avg Reward: -1.50\n",
      "ğŸ“˜ Episode 9800 | Epsilon: 0.0100 | Avg Reward: -1.39\n",
      "ğŸ“˜ Episode 9900 | Epsilon: 0.0100 | Avg Reward: -1.28\n",
      "ğŸ“˜ Episode 10000 | Epsilon: 0.0100 | Avg Reward: -1.27\n",
      "âœ… Loaded best model from episode 119 | Avg Reward: -0.13\n",
      "Completed training for 2 deck(s)\n",
      "\n",
      "=== Training model for 3 deck(s) ===\n",
      "ğŸ“˜ Episode 100 | Epsilon: 0.6058 | Avg Reward: -0.29\n",
      "ğŸ“˜ Episode 200 | Epsilon: 0.3670 | Avg Reward: -0.53\n",
      "ğŸ“˜ Episode 300 | Epsilon: 0.2223 | Avg Reward: -0.61\n",
      "ğŸ“˜ Episode 400 | Epsilon: 0.1347 | Avg Reward: -0.55\n",
      "ğŸ“˜ Episode 500 | Epsilon: 0.0816 | Avg Reward: -0.74\n",
      "ğŸ“˜ Episode 600 | Epsilon: 0.0494 | Avg Reward: -0.83\n",
      "ğŸ“˜ Episode 700 | Epsilon: 0.0299 | Avg Reward: -0.75\n",
      "ğŸ“˜ Episode 800 | Epsilon: 0.0181 | Avg Reward: -0.62\n",
      "ğŸ“˜ Episode 900 | Epsilon: 0.0110 | Avg Reward: -0.44\n",
      "ğŸ“˜ Episode 1000 | Epsilon: 0.0100 | Avg Reward: -0.60\n",
      "ğŸ“˜ Episode 1100 | Epsilon: 0.0100 | Avg Reward: -1.02\n",
      "ğŸ“˜ Episode 1200 | Epsilon: 0.0100 | Avg Reward: -0.90\n",
      "ğŸ“˜ Episode 1300 | Epsilon: 0.0100 | Avg Reward: -0.88\n",
      "ğŸ“˜ Episode 1400 | Epsilon: 0.0100 | Avg Reward: -1.12\n",
      "ğŸ“˜ Episode 1500 | Epsilon: 0.0100 | Avg Reward: -0.63\n",
      "ğŸ“˜ Episode 1600 | Epsilon: 0.0100 | Avg Reward: -0.58\n",
      "ğŸ“˜ Episode 1700 | Epsilon: 0.0100 | Avg Reward: -0.66\n",
      "ğŸ“˜ Episode 1800 | Epsilon: 0.0100 | Avg Reward: -0.65\n",
      "ğŸ“˜ Episode 1900 | Epsilon: 0.0100 | Avg Reward: -0.44\n",
      "ğŸ“˜ Episode 2000 | Epsilon: 0.0100 | Avg Reward: -0.97\n",
      "ğŸ“˜ Episode 2100 | Epsilon: 0.0100 | Avg Reward: -0.56\n",
      "ğŸ“˜ Episode 2200 | Epsilon: 0.0100 | Avg Reward: -0.63\n",
      "ğŸ“˜ Episode 2300 | Epsilon: 0.0100 | Avg Reward: -0.68\n",
      "ğŸ“˜ Episode 2400 | Epsilon: 0.0100 | Avg Reward: -0.76\n",
      "ğŸ“˜ Episode 2500 | Epsilon: 0.0100 | Avg Reward: -0.69\n",
      "ğŸ“˜ Episode 2600 | Epsilon: 0.0100 | Avg Reward: -0.49\n",
      "ğŸ“˜ Episode 2700 | Epsilon: 0.0100 | Avg Reward: -0.64\n",
      "ğŸ“˜ Episode 2800 | Epsilon: 0.0100 | Avg Reward: -0.36\n",
      "ğŸ“˜ Episode 2900 | Epsilon: 0.0100 | Avg Reward: -0.65\n",
      "ğŸ“˜ Episode 3000 | Epsilon: 0.0100 | Avg Reward: -0.40\n",
      "ğŸ“˜ Episode 3100 | Epsilon: 0.0100 | Avg Reward: -1.09\n",
      "ğŸ“˜ Episode 3200 | Epsilon: 0.0100 | Avg Reward: -0.83\n",
      "ğŸ“˜ Episode 3300 | Epsilon: 0.0100 | Avg Reward: -0.78\n",
      "ğŸ“˜ Episode 3400 | Epsilon: 0.0100 | Avg Reward: -0.79\n",
      "ğŸ“˜ Episode 3500 | Epsilon: 0.0100 | Avg Reward: -0.82\n",
      "ğŸ“˜ Episode 3600 | Epsilon: 0.0100 | Avg Reward: -0.50\n",
      "ğŸ“˜ Episode 3700 | Epsilon: 0.0100 | Avg Reward: -0.81\n",
      "ğŸ“˜ Episode 3800 | Epsilon: 0.0100 | Avg Reward: -0.73\n",
      "ğŸ“˜ Episode 3900 | Epsilon: 0.0100 | Avg Reward: -0.54\n",
      "ğŸ“˜ Episode 4000 | Epsilon: 0.0100 | Avg Reward: -0.86\n",
      "ğŸ“˜ Episode 4100 | Epsilon: 0.0100 | Avg Reward: -0.72\n",
      "ğŸ“˜ Episode 4200 | Epsilon: 0.0100 | Avg Reward: -0.62\n",
      "ğŸ“˜ Episode 4300 | Epsilon: 0.0100 | Avg Reward: -0.75\n",
      "ğŸ“˜ Episode 4400 | Epsilon: 0.0100 | Avg Reward: -0.61\n",
      "ğŸ“˜ Episode 4500 | Epsilon: 0.0100 | Avg Reward: -0.63\n",
      "ğŸ“˜ Episode 4600 | Epsilon: 0.0100 | Avg Reward: -0.94\n",
      "ğŸ“˜ Episode 4700 | Epsilon: 0.0100 | Avg Reward: -0.69\n",
      "ğŸ“˜ Episode 4800 | Epsilon: 0.0100 | Avg Reward: -0.55\n",
      "ğŸ“˜ Episode 4900 | Epsilon: 0.0100 | Avg Reward: -0.67\n",
      "ğŸ“˜ Episode 5000 | Epsilon: 0.0100 | Avg Reward: -0.53\n",
      "ğŸ“˜ Episode 5100 | Epsilon: 0.0100 | Avg Reward: -1.06\n",
      "ğŸ“˜ Episode 5200 | Epsilon: 0.0100 | Avg Reward: -0.51\n",
      "ğŸ“˜ Episode 5300 | Epsilon: 0.0100 | Avg Reward: -0.84\n",
      "ğŸ“˜ Episode 5400 | Epsilon: 0.0100 | Avg Reward: -0.50\n",
      "ğŸ“˜ Episode 5500 | Epsilon: 0.0100 | Avg Reward: -0.86\n",
      "ğŸ“˜ Episode 5600 | Epsilon: 0.0100 | Avg Reward: -0.77\n",
      "ğŸ“˜ Episode 5700 | Epsilon: 0.0100 | Avg Reward: -0.83\n",
      "ğŸ“˜ Episode 5800 | Epsilon: 0.0100 | Avg Reward: -0.87\n",
      "ğŸ“˜ Episode 5900 | Epsilon: 0.0100 | Avg Reward: -1.14\n",
      "ğŸ“˜ Episode 6000 | Epsilon: 0.0100 | Avg Reward: -0.57\n",
      "ğŸ“˜ Episode 6100 | Epsilon: 0.0100 | Avg Reward: -0.92\n",
      "ğŸ“˜ Episode 6200 | Epsilon: 0.0100 | Avg Reward: -1.03\n",
      "ğŸ“˜ Episode 6300 | Epsilon: 0.0100 | Avg Reward: -1.05\n",
      "ğŸ“˜ Episode 6400 | Epsilon: 0.0100 | Avg Reward: -0.83\n",
      "ğŸ“˜ Episode 6500 | Epsilon: 0.0100 | Avg Reward: -0.84\n",
      "ğŸ“˜ Episode 6600 | Epsilon: 0.0100 | Avg Reward: -0.63\n",
      "ğŸ“˜ Episode 6700 | Epsilon: 0.0100 | Avg Reward: -1.01\n",
      "ğŸ“˜ Episode 6800 | Epsilon: 0.0100 | Avg Reward: -0.87\n",
      "ğŸ“˜ Episode 6900 | Epsilon: 0.0100 | Avg Reward: -0.75\n",
      "ğŸ“˜ Episode 7000 | Epsilon: 0.0100 | Avg Reward: -0.99\n",
      "ğŸ“˜ Episode 7100 | Epsilon: 0.0100 | Avg Reward: -0.85\n",
      "ğŸ“˜ Episode 7200 | Epsilon: 0.0100 | Avg Reward: -0.97\n",
      "ğŸ“˜ Episode 7300 | Epsilon: 0.0100 | Avg Reward: -1.05\n",
      "ğŸ“˜ Episode 7400 | Epsilon: 0.0100 | Avg Reward: -0.88\n",
      "ğŸ“˜ Episode 7500 | Epsilon: 0.0100 | Avg Reward: -0.91\n",
      "ğŸ“˜ Episode 7600 | Epsilon: 0.0100 | Avg Reward: -0.89\n",
      "ğŸ“˜ Episode 7700 | Epsilon: 0.0100 | Avg Reward: -0.92\n",
      "ğŸ“˜ Episode 7800 | Epsilon: 0.0100 | Avg Reward: -1.11\n",
      "ğŸ“˜ Episode 7900 | Epsilon: 0.0100 | Avg Reward: -0.98\n",
      "ğŸ“˜ Episode 8000 | Epsilon: 0.0100 | Avg Reward: -1.10\n",
      "ğŸ“˜ Episode 8100 | Epsilon: 0.0100 | Avg Reward: -1.04\n",
      "ğŸ“˜ Episode 8200 | Epsilon: 0.0100 | Avg Reward: -1.21\n",
      "ğŸ“˜ Episode 8300 | Epsilon: 0.0100 | Avg Reward: -1.31\n",
      "ğŸ“˜ Episode 8400 | Epsilon: 0.0100 | Avg Reward: -0.82\n",
      "ğŸ“˜ Episode 8500 | Epsilon: 0.0100 | Avg Reward: -1.24\n",
      "ğŸ“˜ Episode 8600 | Epsilon: 0.0100 | Avg Reward: -1.19\n",
      "ğŸ“˜ Episode 8700 | Epsilon: 0.0100 | Avg Reward: -1.20\n",
      "ğŸ“˜ Episode 8800 | Epsilon: 0.0100 | Avg Reward: -1.17\n",
      "ğŸ“˜ Episode 8900 | Epsilon: 0.0100 | Avg Reward: -1.13\n",
      "ğŸ“˜ Episode 9000 | Epsilon: 0.0100 | Avg Reward: -1.14\n",
      "ğŸ“˜ Episode 9100 | Epsilon: 0.0100 | Avg Reward: -1.39\n",
      "ğŸ“˜ Episode 9200 | Epsilon: 0.0100 | Avg Reward: -1.24\n",
      "ğŸ“˜ Episode 9300 | Epsilon: 0.0100 | Avg Reward: -1.25\n",
      "ğŸ“˜ Episode 9400 | Epsilon: 0.0100 | Avg Reward: -1.28\n",
      "ğŸ“˜ Episode 9500 | Epsilon: 0.0100 | Avg Reward: -1.29\n",
      "ğŸ“˜ Episode 9600 | Epsilon: 0.0100 | Avg Reward: -1.40\n",
      "ğŸ“˜ Episode 9700 | Epsilon: 0.0100 | Avg Reward: -1.40\n",
      "ğŸ“˜ Episode 9800 | Epsilon: 0.0100 | Avg Reward: -1.16\n",
      "ğŸ“˜ Episode 9900 | Epsilon: 0.0100 | Avg Reward: -1.48\n",
      "ğŸ“˜ Episode 10000 | Epsilon: 0.0100 | Avg Reward: -1.40\n",
      "âœ… Loaded best model from episode 4438 | Avg Reward: -0.21\n",
      "Completed training for 3 deck(s)\n",
      "\n",
      "=== Training model for 4 deck(s) ===\n",
      "ğŸ“˜ Episode 100 | Epsilon: 0.6058 | Avg Reward: -0.28\n",
      "ğŸ“˜ Episode 200 | Epsilon: 0.3670 | Avg Reward: -0.76\n",
      "ğŸ“˜ Episode 300 | Epsilon: 0.2223 | Avg Reward: -1.10\n",
      "ğŸ“˜ Episode 400 | Epsilon: 0.1347 | Avg Reward: -0.76\n",
      "ğŸ“˜ Episode 500 | Epsilon: 0.0816 | Avg Reward: -0.60\n",
      "ğŸ“˜ Episode 600 | Epsilon: 0.0494 | Avg Reward: -0.76\n",
      "ğŸ“˜ Episode 700 | Epsilon: 0.0299 | Avg Reward: -0.64\n",
      "ğŸ“˜ Episode 800 | Epsilon: 0.0181 | Avg Reward: -0.88\n",
      "ğŸ“˜ Episode 900 | Epsilon: 0.0110 | Avg Reward: -0.73\n",
      "ğŸ“˜ Episode 1000 | Epsilon: 0.0100 | Avg Reward: -0.73\n",
      "ğŸ“˜ Episode 1100 | Epsilon: 0.0100 | Avg Reward: -0.74\n",
      "ğŸ“˜ Episode 1200 | Epsilon: 0.0100 | Avg Reward: -0.68\n",
      "ğŸ“˜ Episode 1300 | Epsilon: 0.0100 | Avg Reward: -0.61\n",
      "ğŸ“˜ Episode 1400 | Epsilon: 0.0100 | Avg Reward: -0.59\n",
      "ğŸ“˜ Episode 1500 | Epsilon: 0.0100 | Avg Reward: -1.09\n",
      "ğŸ“˜ Episode 1600 | Epsilon: 0.0100 | Avg Reward: -0.47\n",
      "ğŸ“˜ Episode 1700 | Epsilon: 0.0100 | Avg Reward: -0.56\n",
      "ğŸ“˜ Episode 1800 | Epsilon: 0.0100 | Avg Reward: -0.82\n",
      "ğŸ“˜ Episode 1900 | Epsilon: 0.0100 | Avg Reward: -0.55\n",
      "ğŸ“˜ Episode 2000 | Epsilon: 0.0100 | Avg Reward: -0.99\n",
      "ğŸ“˜ Episode 2100 | Epsilon: 0.0100 | Avg Reward: -0.61\n",
      "ğŸ“˜ Episode 2200 | Epsilon: 0.0100 | Avg Reward: -1.07\n",
      "ğŸ“˜ Episode 2300 | Epsilon: 0.0100 | Avg Reward: -0.68\n",
      "ğŸ“˜ Episode 2400 | Epsilon: 0.0100 | Avg Reward: -0.84\n",
      "ğŸ“˜ Episode 2500 | Epsilon: 0.0100 | Avg Reward: -0.96\n",
      "ğŸ“˜ Episode 2600 | Epsilon: 0.0100 | Avg Reward: -0.51\n",
      "ğŸ“˜ Episode 2700 | Epsilon: 0.0100 | Avg Reward: -0.91\n",
      "ğŸ“˜ Episode 2800 | Epsilon: 0.0100 | Avg Reward: -0.52\n",
      "ğŸ“˜ Episode 2900 | Epsilon: 0.0100 | Avg Reward: -0.56\n",
      "ğŸ“˜ Episode 3000 | Epsilon: 0.0100 | Avg Reward: -1.20\n",
      "ğŸ“˜ Episode 3100 | Epsilon: 0.0100 | Avg Reward: -0.59\n",
      "ğŸ“˜ Episode 3200 | Epsilon: 0.0100 | Avg Reward: -0.63\n",
      "ğŸ“˜ Episode 3300 | Epsilon: 0.0100 | Avg Reward: -0.88\n",
      "ğŸ“˜ Episode 3400 | Epsilon: 0.0100 | Avg Reward: -0.80\n",
      "ğŸ“˜ Episode 3500 | Epsilon: 0.0100 | Avg Reward: -0.69\n",
      "ğŸ“˜ Episode 3600 | Epsilon: 0.0100 | Avg Reward: -0.72\n",
      "ğŸ“˜ Episode 3700 | Epsilon: 0.0100 | Avg Reward: -0.99\n",
      "ğŸ“˜ Episode 3800 | Epsilon: 0.0100 | Avg Reward: -1.01\n",
      "ğŸ“˜ Episode 3900 | Epsilon: 0.0100 | Avg Reward: -0.86\n",
      "ğŸ“˜ Episode 4000 | Epsilon: 0.0100 | Avg Reward: -0.60\n",
      "ğŸ“˜ Episode 4100 | Epsilon: 0.0100 | Avg Reward: -0.83\n",
      "ğŸ“˜ Episode 4200 | Epsilon: 0.0100 | Avg Reward: -0.76\n",
      "ğŸ“˜ Episode 4300 | Epsilon: 0.0100 | Avg Reward: -0.85\n",
      "ğŸ“˜ Episode 4400 | Epsilon: 0.0100 | Avg Reward: -1.34\n",
      "ğŸ“˜ Episode 4500 | Epsilon: 0.0100 | Avg Reward: -0.79\n",
      "ğŸ“˜ Episode 4600 | Epsilon: 0.0100 | Avg Reward: -0.90\n",
      "ğŸ“˜ Episode 4700 | Epsilon: 0.0100 | Avg Reward: -0.93\n",
      "ğŸ“˜ Episode 4800 | Epsilon: 0.0100 | Avg Reward: -1.01\n",
      "ğŸ“˜ Episode 4900 | Epsilon: 0.0100 | Avg Reward: -0.76\n",
      "ğŸ“˜ Episode 5000 | Epsilon: 0.0100 | Avg Reward: -0.40\n",
      "ğŸ“˜ Episode 5100 | Epsilon: 0.0100 | Avg Reward: -0.99\n",
      "ğŸ“˜ Episode 5200 | Epsilon: 0.0100 | Avg Reward: -1.17\n",
      "ğŸ“˜ Episode 5300 | Epsilon: 0.0100 | Avg Reward: -0.99\n",
      "ğŸ“˜ Episode 5400 | Epsilon: 0.0100 | Avg Reward: -0.73\n",
      "ğŸ“˜ Episode 5500 | Epsilon: 0.0100 | Avg Reward: -0.89\n",
      "ğŸ“˜ Episode 5600 | Epsilon: 0.0100 | Avg Reward: -1.31\n",
      "ğŸ“˜ Episode 5700 | Epsilon: 0.0100 | Avg Reward: -1.24\n",
      "ğŸ“˜ Episode 5800 | Epsilon: 0.0100 | Avg Reward: -1.13\n",
      "ğŸ“˜ Episode 5900 | Epsilon: 0.0100 | Avg Reward: -0.98\n",
      "ğŸ“˜ Episode 6000 | Epsilon: 0.0100 | Avg Reward: -1.05\n",
      "ğŸ“˜ Episode 6100 | Epsilon: 0.0100 | Avg Reward: -1.06\n",
      "ğŸ“˜ Episode 6200 | Epsilon: 0.0100 | Avg Reward: -0.81\n",
      "ğŸ“˜ Episode 6300 | Epsilon: 0.0100 | Avg Reward: -1.10\n",
      "ğŸ“˜ Episode 6400 | Epsilon: 0.0100 | Avg Reward: -0.98\n",
      "ğŸ“˜ Episode 6500 | Epsilon: 0.0100 | Avg Reward: -0.88\n",
      "ğŸ“˜ Episode 6600 | Epsilon: 0.0100 | Avg Reward: -1.04\n",
      "ğŸ“˜ Episode 6700 | Epsilon: 0.0100 | Avg Reward: -1.03\n",
      "ğŸ“˜ Episode 6800 | Epsilon: 0.0100 | Avg Reward: -1.17\n",
      "ğŸ“˜ Episode 6900 | Epsilon: 0.0100 | Avg Reward: -1.34\n",
      "ğŸ“˜ Episode 7000 | Epsilon: 0.0100 | Avg Reward: -0.82\n",
      "ğŸ“˜ Episode 7100 | Epsilon: 0.0100 | Avg Reward: -1.11\n",
      "ğŸ“˜ Episode 7200 | Epsilon: 0.0100 | Avg Reward: -1.15\n",
      "ğŸ“˜ Episode 7300 | Epsilon: 0.0100 | Avg Reward: -1.29\n",
      "ğŸ“˜ Episode 7400 | Epsilon: 0.0100 | Avg Reward: -1.05\n",
      "ğŸ“˜ Episode 7500 | Epsilon: 0.0100 | Avg Reward: -0.83\n",
      "ğŸ“˜ Episode 7600 | Epsilon: 0.0100 | Avg Reward: -1.04\n",
      "ğŸ“˜ Episode 7700 | Epsilon: 0.0100 | Avg Reward: -0.97\n",
      "ğŸ“˜ Episode 7800 | Epsilon: 0.0100 | Avg Reward: -0.84\n",
      "ğŸ“˜ Episode 7900 | Epsilon: 0.0100 | Avg Reward: -0.95\n",
      "ğŸ“˜ Episode 8000 | Epsilon: 0.0100 | Avg Reward: -0.84\n",
      "ğŸ“˜ Episode 8100 | Epsilon: 0.0100 | Avg Reward: -1.13\n",
      "ğŸ“˜ Episode 8200 | Epsilon: 0.0100 | Avg Reward: -0.63\n",
      "ğŸ“˜ Episode 8300 | Epsilon: 0.0100 | Avg Reward: -1.06\n",
      "ğŸ“˜ Episode 8400 | Epsilon: 0.0100 | Avg Reward: -1.10\n",
      "ğŸ“˜ Episode 8500 | Epsilon: 0.0100 | Avg Reward: -1.09\n",
      "ğŸ“˜ Episode 8600 | Epsilon: 0.0100 | Avg Reward: -1.03\n",
      "ğŸ“˜ Episode 8700 | Epsilon: 0.0100 | Avg Reward: -1.03\n",
      "ğŸ“˜ Episode 8800 | Epsilon: 0.0100 | Avg Reward: -0.80\n",
      "ğŸ“˜ Episode 8900 | Epsilon: 0.0100 | Avg Reward: -1.14\n",
      "ğŸ“˜ Episode 9000 | Epsilon: 0.0100 | Avg Reward: -1.13\n",
      "ğŸ“˜ Episode 9100 | Epsilon: 0.0100 | Avg Reward: -1.41\n",
      "ğŸ“˜ Episode 9200 | Epsilon: 0.0100 | Avg Reward: -0.79\n",
      "ğŸ“˜ Episode 9300 | Epsilon: 0.0100 | Avg Reward: -1.23\n",
      "ğŸ“˜ Episode 9400 | Epsilon: 0.0100 | Avg Reward: -1.18\n",
      "ğŸ“˜ Episode 9500 | Epsilon: 0.0100 | Avg Reward: -1.12\n",
      "ğŸ“˜ Episode 9600 | Epsilon: 0.0100 | Avg Reward: -1.00\n",
      "ğŸ“˜ Episode 9700 | Epsilon: 0.0100 | Avg Reward: -1.03\n",
      "ğŸ“˜ Episode 9800 | Epsilon: 0.0100 | Avg Reward: -1.07\n",
      "ğŸ“˜ Episode 9900 | Epsilon: 0.0100 | Avg Reward: -1.26\n",
      "ğŸ“˜ Episode 10000 | Epsilon: 0.0100 | Avg Reward: -1.30\n",
      "âœ… Loaded best model from episode 1131 | Avg Reward: -0.24\n",
      "Completed training for 4 deck(s)\n",
      "\n",
      "=== Training model for 5 deck(s) ===\n",
      "ğŸ“˜ Episode 100 | Epsilon: 0.6058 | Avg Reward: -0.45\n",
      "ğŸ“˜ Episode 200 | Epsilon: 0.3670 | Avg Reward: -0.51\n",
      "ğŸ“˜ Episode 300 | Epsilon: 0.2223 | Avg Reward: -0.44\n",
      "ğŸ“˜ Episode 400 | Epsilon: 0.1347 | Avg Reward: -0.65\n",
      "ğŸ“˜ Episode 500 | Epsilon: 0.0816 | Avg Reward: -0.56\n",
      "ğŸ“˜ Episode 600 | Epsilon: 0.0494 | Avg Reward: -0.58\n",
      "ğŸ“˜ Episode 700 | Epsilon: 0.0299 | Avg Reward: -0.72\n",
      "ğŸ“˜ Episode 800 | Epsilon: 0.0181 | Avg Reward: -0.54\n",
      "ğŸ“˜ Episode 900 | Epsilon: 0.0110 | Avg Reward: -0.73\n",
      "ğŸ“˜ Episode 1000 | Epsilon: 0.0100 | Avg Reward: -0.81\n",
      "ğŸ“˜ Episode 1100 | Epsilon: 0.0100 | Avg Reward: -0.64\n",
      "ğŸ“˜ Episode 1200 | Epsilon: 0.0100 | Avg Reward: -0.65\n",
      "ğŸ“˜ Episode 1300 | Epsilon: 0.0100 | Avg Reward: -0.70\n",
      "ğŸ“˜ Episode 1400 | Epsilon: 0.0100 | Avg Reward: -0.32\n",
      "ğŸ“˜ Episode 1500 | Epsilon: 0.0100 | Avg Reward: 0.07\n",
      "ğŸ“˜ Episode 1600 | Epsilon: 0.0100 | Avg Reward: 0.04\n",
      "ğŸ“˜ Episode 1700 | Epsilon: 0.0100 | Avg Reward: 0.18\n",
      "ğŸ“˜ Episode 1800 | Epsilon: 0.0100 | Avg Reward: 0.09\n",
      "ğŸ“˜ Episode 1900 | Epsilon: 0.0100 | Avg Reward: 0.06\n",
      "ğŸ“˜ Episode 2000 | Epsilon: 0.0100 | Avg Reward: 0.32\n",
      "ğŸ“˜ Episode 2100 | Epsilon: 0.0100 | Avg Reward: 0.10\n",
      "ğŸ“˜ Episode 2200 | Epsilon: 0.0100 | Avg Reward: -0.40\n",
      "ğŸ“˜ Episode 2300 | Epsilon: 0.0100 | Avg Reward: -0.07\n",
      "ğŸ“˜ Episode 2400 | Epsilon: 0.0100 | Avg Reward: -0.07\n",
      "ğŸ“˜ Episode 2500 | Epsilon: 0.0100 | Avg Reward: -0.20\n",
      "ğŸ“˜ Episode 2600 | Epsilon: 0.0100 | Avg Reward: -0.10\n",
      "ğŸ“˜ Episode 2700 | Epsilon: 0.0100 | Avg Reward: 0.30\n",
      "ğŸ“˜ Episode 2800 | Epsilon: 0.0100 | Avg Reward: -0.42\n",
      "ğŸ“˜ Episode 2900 | Epsilon: 0.0100 | Avg Reward: 0.18\n",
      "ğŸ“˜ Episode 3000 | Epsilon: 0.0100 | Avg Reward: 0.06\n",
      "ğŸ“˜ Episode 3100 | Epsilon: 0.0100 | Avg Reward: 0.07\n",
      "ğŸ“˜ Episode 3200 | Epsilon: 0.0100 | Avg Reward: 0.08\n",
      "ğŸ“˜ Episode 3300 | Epsilon: 0.0100 | Avg Reward: -0.48\n",
      "ğŸ“˜ Episode 3400 | Epsilon: 0.0100 | Avg Reward: -0.13\n",
      "ğŸ“˜ Episode 3500 | Epsilon: 0.0100 | Avg Reward: 0.15\n",
      "ğŸ“˜ Episode 3600 | Epsilon: 0.0100 | Avg Reward: 0.02\n",
      "ğŸ“˜ Episode 3700 | Epsilon: 0.0100 | Avg Reward: 0.23\n",
      "ğŸ“˜ Episode 3800 | Epsilon: 0.0100 | Avg Reward: 0.02\n",
      "ğŸ“˜ Episode 3900 | Epsilon: 0.0100 | Avg Reward: -0.07\n",
      "ğŸ“˜ Episode 4000 | Epsilon: 0.0100 | Avg Reward: 0.11\n",
      "ğŸ“˜ Episode 4100 | Epsilon: 0.0100 | Avg Reward: -0.03\n",
      "ğŸ“˜ Episode 4200 | Epsilon: 0.0100 | Avg Reward: 0.26\n",
      "ğŸ“˜ Episode 4300 | Epsilon: 0.0100 | Avg Reward: -0.27\n",
      "ğŸ“˜ Episode 4400 | Epsilon: 0.0100 | Avg Reward: -0.28\n",
      "ğŸ“˜ Episode 4500 | Epsilon: 0.0100 | Avg Reward: -0.11\n",
      "ğŸ“˜ Episode 4600 | Epsilon: 0.0100 | Avg Reward: 0.20\n",
      "ğŸ“˜ Episode 4700 | Epsilon: 0.0100 | Avg Reward: 0.04\n",
      "ğŸ“˜ Episode 4800 | Epsilon: 0.0100 | Avg Reward: 0.28\n",
      "ğŸ“˜ Episode 4900 | Epsilon: 0.0100 | Avg Reward: -0.13\n",
      "ğŸ“˜ Episode 5000 | Epsilon: 0.0100 | Avg Reward: -0.18\n",
      "ğŸ“˜ Episode 5100 | Epsilon: 0.0100 | Avg Reward: -0.08\n",
      "ğŸ“˜ Episode 5200 | Epsilon: 0.0100 | Avg Reward: 0.06\n",
      "ğŸ“˜ Episode 5300 | Epsilon: 0.0100 | Avg Reward: 0.02\n",
      "ğŸ“˜ Episode 5400 | Epsilon: 0.0100 | Avg Reward: -0.03\n",
      "ğŸ“˜ Episode 5500 | Epsilon: 0.0100 | Avg Reward: -0.07\n",
      "ğŸ“˜ Episode 5600 | Epsilon: 0.0100 | Avg Reward: -0.11\n",
      "ğŸ“˜ Episode 5700 | Epsilon: 0.0100 | Avg Reward: 0.27\n",
      "ğŸ“˜ Episode 5800 | Epsilon: 0.0100 | Avg Reward: 0.17\n",
      "ğŸ“˜ Episode 5900 | Epsilon: 0.0100 | Avg Reward: 0.27\n",
      "ğŸ“˜ Episode 6000 | Epsilon: 0.0100 | Avg Reward: -0.21\n",
      "ğŸ“˜ Episode 6100 | Epsilon: 0.0100 | Avg Reward: 0.03\n",
      "ğŸ“˜ Episode 6200 | Epsilon: 0.0100 | Avg Reward: 0.04\n",
      "ğŸ“˜ Episode 6300 | Epsilon: 0.0100 | Avg Reward: 0.10\n",
      "ğŸ“˜ Episode 6400 | Epsilon: 0.0100 | Avg Reward: 0.26\n",
      "ğŸ“˜ Episode 6500 | Epsilon: 0.0100 | Avg Reward: 0.03\n",
      "ğŸ“˜ Episode 6600 | Epsilon: 0.0100 | Avg Reward: 0.02\n",
      "ğŸ“˜ Episode 6700 | Epsilon: 0.0100 | Avg Reward: -0.01\n",
      "ğŸ“˜ Episode 6800 | Epsilon: 0.0100 | Avg Reward: -0.14\n",
      "ğŸ“˜ Episode 6900 | Epsilon: 0.0100 | Avg Reward: -0.07\n",
      "ğŸ“˜ Episode 7000 | Epsilon: 0.0100 | Avg Reward: -0.13\n",
      "ğŸ“˜ Episode 7100 | Epsilon: 0.0100 | Avg Reward: -0.26\n",
      "ğŸ“˜ Episode 7200 | Epsilon: 0.0100 | Avg Reward: 0.12\n",
      "ğŸ“˜ Episode 7300 | Epsilon: 0.0100 | Avg Reward: 0.28\n",
      "ğŸ“˜ Episode 7400 | Epsilon: 0.0100 | Avg Reward: -0.40\n",
      "ğŸ“˜ Episode 7500 | Epsilon: 0.0100 | Avg Reward: 0.02\n",
      "ğŸ“˜ Episode 7600 | Epsilon: 0.0100 | Avg Reward: -0.20\n",
      "ğŸ“˜ Episode 7700 | Epsilon: 0.0100 | Avg Reward: -0.01\n",
      "ğŸ“˜ Episode 7800 | Epsilon: 0.0100 | Avg Reward: 0.16\n",
      "ğŸ“˜ Episode 7900 | Epsilon: 0.0100 | Avg Reward: -0.30\n",
      "ğŸ“˜ Episode 8000 | Epsilon: 0.0100 | Avg Reward: -0.14\n",
      "ğŸ“˜ Episode 8100 | Epsilon: 0.0100 | Avg Reward: 0.37\n",
      "ğŸ“˜ Episode 8200 | Epsilon: 0.0100 | Avg Reward: 0.00\n",
      "ğŸ“˜ Episode 8300 | Epsilon: 0.0100 | Avg Reward: -0.32\n",
      "ğŸ“˜ Episode 8400 | Epsilon: 0.0100 | Avg Reward: 0.10\n",
      "ğŸ“˜ Episode 8500 | Epsilon: 0.0100 | Avg Reward: -0.09\n",
      "ğŸ“˜ Episode 8600 | Epsilon: 0.0100 | Avg Reward: -0.09\n",
      "ğŸ“˜ Episode 8700 | Epsilon: 0.0100 | Avg Reward: 0.10\n",
      "ğŸ“˜ Episode 8800 | Epsilon: 0.0100 | Avg Reward: -0.30\n",
      "ğŸ“˜ Episode 8900 | Epsilon: 0.0100 | Avg Reward: -0.05\n",
      "ğŸ“˜ Episode 9000 | Epsilon: 0.0100 | Avg Reward: 0.37\n",
      "ğŸ“˜ Episode 9100 | Epsilon: 0.0100 | Avg Reward: 0.36\n",
      "ğŸ“˜ Episode 9200 | Epsilon: 0.0100 | Avg Reward: -0.24\n",
      "ğŸ“˜ Episode 9300 | Epsilon: 0.0100 | Avg Reward: 0.09\n",
      "ğŸ“˜ Episode 9400 | Epsilon: 0.0100 | Avg Reward: 0.12\n",
      "ğŸ“˜ Episode 9500 | Epsilon: 0.0100 | Avg Reward: -0.15\n",
      "ğŸ“˜ Episode 9600 | Epsilon: 0.0100 | Avg Reward: 0.02\n",
      "ğŸ“˜ Episode 9700 | Epsilon: 0.0100 | Avg Reward: 0.01\n",
      "ğŸ“˜ Episode 9800 | Epsilon: 0.0100 | Avg Reward: -0.32\n",
      "ğŸ“˜ Episode 9900 | Epsilon: 0.0100 | Avg Reward: 0.17\n",
      "ğŸ“˜ Episode 10000 | Epsilon: 0.0100 | Avg Reward: 0.27\n",
      "âœ… Loaded best model from episode 8091 | Avg Reward: 0.47\n",
      "Completed training for 5 deck(s)\n",
      "\n",
      "=== Training model for 6 deck(s) ===\n",
      "ğŸ“˜ Episode 100 | Epsilon: 0.6058 | Avg Reward: -0.20\n",
      "ğŸ“˜ Episode 200 | Epsilon: 0.3670 | Avg Reward: -0.49\n",
      "ğŸ“˜ Episode 300 | Epsilon: 0.2223 | Avg Reward: -0.38\n",
      "ğŸ“˜ Episode 400 | Epsilon: 0.1347 | Avg Reward: -0.03\n",
      "ğŸ“˜ Episode 500 | Epsilon: 0.0816 | Avg Reward: -0.56\n",
      "ğŸ“˜ Episode 600 | Epsilon: 0.0494 | Avg Reward: -0.22\n",
      "ğŸ“˜ Episode 700 | Epsilon: 0.0299 | Avg Reward: -0.50\n",
      "ğŸ“˜ Episode 800 | Epsilon: 0.0181 | Avg Reward: -0.06\n",
      "ğŸ“˜ Episode 900 | Epsilon: 0.0110 | Avg Reward: -0.33\n",
      "ğŸ“˜ Episode 1000 | Epsilon: 0.0100 | Avg Reward: 0.07\n",
      "ğŸ“˜ Episode 1100 | Epsilon: 0.0100 | Avg Reward: 0.05\n",
      "ğŸ“˜ Episode 1200 | Epsilon: 0.0100 | Avg Reward: -0.39\n",
      "ğŸ“˜ Episode 1300 | Epsilon: 0.0100 | Avg Reward: -0.22\n",
      "ğŸ“˜ Episode 1400 | Epsilon: 0.0100 | Avg Reward: -0.31\n",
      "ğŸ“˜ Episode 1500 | Epsilon: 0.0100 | Avg Reward: 0.23\n",
      "ğŸ“˜ Episode 1600 | Epsilon: 0.0100 | Avg Reward: -0.43\n",
      "ğŸ“˜ Episode 1700 | Epsilon: 0.0100 | Avg Reward: -0.08\n",
      "ğŸ“˜ Episode 1800 | Epsilon: 0.0100 | Avg Reward: -0.03\n",
      "ğŸ“˜ Episode 1900 | Epsilon: 0.0100 | Avg Reward: -0.40\n",
      "ğŸ“˜ Episode 2000 | Epsilon: 0.0100 | Avg Reward: 0.27\n",
      "ğŸ“˜ Episode 2100 | Epsilon: 0.0100 | Avg Reward: -0.14\n",
      "ğŸ“˜ Episode 2200 | Epsilon: 0.0100 | Avg Reward: -0.22\n",
      "ğŸ“˜ Episode 2300 | Epsilon: 0.0100 | Avg Reward: -0.07\n",
      "ğŸ“˜ Episode 2400 | Epsilon: 0.0100 | Avg Reward: -0.16\n",
      "ğŸ“˜ Episode 2500 | Epsilon: 0.0100 | Avg Reward: -0.53\n",
      "ğŸ“˜ Episode 2600 | Epsilon: 0.0100 | Avg Reward: -0.38\n",
      "ğŸ“˜ Episode 2700 | Epsilon: 0.0100 | Avg Reward: -0.46\n",
      "ğŸ“˜ Episode 2800 | Epsilon: 0.0100 | Avg Reward: -0.06\n",
      "ğŸ“˜ Episode 2900 | Epsilon: 0.0100 | Avg Reward: -0.17\n",
      "ğŸ“˜ Episode 3000 | Epsilon: 0.0100 | Avg Reward: 0.07\n",
      "ğŸ“˜ Episode 3100 | Epsilon: 0.0100 | Avg Reward: -0.10\n",
      "ğŸ“˜ Episode 3200 | Epsilon: 0.0100 | Avg Reward: -0.27\n",
      "ğŸ“˜ Episode 3300 | Epsilon: 0.0100 | Avg Reward: 0.06\n",
      "ğŸ“˜ Episode 3400 | Epsilon: 0.0100 | Avg Reward: -0.16\n",
      "ğŸ“˜ Episode 3500 | Epsilon: 0.0100 | Avg Reward: 0.08\n",
      "ğŸ“˜ Episode 3600 | Epsilon: 0.0100 | Avg Reward: -0.08\n",
      "ğŸ“˜ Episode 3700 | Epsilon: 0.0100 | Avg Reward: -0.08\n",
      "ğŸ“˜ Episode 3800 | Epsilon: 0.0100 | Avg Reward: 0.16\n",
      "ğŸ“˜ Episode 3900 | Epsilon: 0.0100 | Avg Reward: -0.18\n",
      "ğŸ“˜ Episode 4000 | Epsilon: 0.0100 | Avg Reward: -0.26\n",
      "ğŸ“˜ Episode 4100 | Epsilon: 0.0100 | Avg Reward: 0.19\n",
      "ğŸ“˜ Episode 4200 | Epsilon: 0.0100 | Avg Reward: -0.07\n",
      "ğŸ“˜ Episode 4300 | Epsilon: 0.0100 | Avg Reward: -0.11\n",
      "ğŸ“˜ Episode 4400 | Epsilon: 0.0100 | Avg Reward: -0.14\n",
      "ğŸ“˜ Episode 4500 | Epsilon: 0.0100 | Avg Reward: -0.26\n",
      "ğŸ“˜ Episode 4600 | Epsilon: 0.0100 | Avg Reward: 0.11\n",
      "ğŸ“˜ Episode 4700 | Epsilon: 0.0100 | Avg Reward: -0.40\n",
      "ğŸ“˜ Episode 4800 | Epsilon: 0.0100 | Avg Reward: -0.06\n",
      "ğŸ“˜ Episode 4900 | Epsilon: 0.0100 | Avg Reward: -0.53\n",
      "ğŸ“˜ Episode 5000 | Epsilon: 0.0100 | Avg Reward: -0.20\n",
      "ğŸ“˜ Episode 5100 | Epsilon: 0.0100 | Avg Reward: -0.07\n",
      "ğŸ“˜ Episode 5200 | Epsilon: 0.0100 | Avg Reward: -0.15\n",
      "ğŸ“˜ Episode 5300 | Epsilon: 0.0100 | Avg Reward: -0.02\n",
      "ğŸ“˜ Episode 5400 | Epsilon: 0.0100 | Avg Reward: -0.38\n",
      "ğŸ“˜ Episode 5500 | Epsilon: 0.0100 | Avg Reward: -0.63\n",
      "ğŸ“˜ Episode 5600 | Epsilon: 0.0100 | Avg Reward: -0.13\n",
      "ğŸ“˜ Episode 5700 | Epsilon: 0.0100 | Avg Reward: -0.30\n",
      "ğŸ“˜ Episode 5800 | Epsilon: 0.0100 | Avg Reward: -0.43\n",
      "ğŸ“˜ Episode 5900 | Epsilon: 0.0100 | Avg Reward: -0.34\n",
      "ğŸ“˜ Episode 6000 | Epsilon: 0.0100 | Avg Reward: -0.55\n",
      "ğŸ“˜ Episode 6100 | Epsilon: 0.0100 | Avg Reward: -0.37\n",
      "ğŸ“˜ Episode 6200 | Epsilon: 0.0100 | Avg Reward: -0.29\n",
      "ğŸ“˜ Episode 6300 | Epsilon: 0.0100 | Avg Reward: -0.29\n",
      "ğŸ“˜ Episode 6400 | Epsilon: 0.0100 | Avg Reward: 0.13\n",
      "ğŸ“˜ Episode 6500 | Epsilon: 0.0100 | Avg Reward: -0.75\n",
      "ğŸ“˜ Episode 6600 | Epsilon: 0.0100 | Avg Reward: -0.66\n",
      "ğŸ“˜ Episode 6700 | Epsilon: 0.0100 | Avg Reward: -0.52\n",
      "ğŸ“˜ Episode 6800 | Epsilon: 0.0100 | Avg Reward: -0.35\n",
      "ğŸ“˜ Episode 6900 | Epsilon: 0.0100 | Avg Reward: -0.36\n",
      "ğŸ“˜ Episode 7000 | Epsilon: 0.0100 | Avg Reward: -0.34\n",
      "ğŸ“˜ Episode 7100 | Epsilon: 0.0100 | Avg Reward: -0.38\n",
      "ğŸ“˜ Episode 7200 | Epsilon: 0.0100 | Avg Reward: -0.45\n",
      "ğŸ“˜ Episode 7300 | Epsilon: 0.0100 | Avg Reward: -0.04\n",
      "ğŸ“˜ Episode 7400 | Epsilon: 0.0100 | Avg Reward: -0.73\n",
      "ğŸ“˜ Episode 7500 | Epsilon: 0.0100 | Avg Reward: -0.41\n",
      "ğŸ“˜ Episode 7600 | Epsilon: 0.0100 | Avg Reward: -0.54\n",
      "ğŸ“˜ Episode 7700 | Epsilon: 0.0100 | Avg Reward: -0.37\n",
      "ğŸ“˜ Episode 7800 | Epsilon: 0.0100 | Avg Reward: -0.93\n",
      "ğŸ“˜ Episode 7900 | Epsilon: 0.0100 | Avg Reward: -0.19\n",
      "ğŸ“˜ Episode 8000 | Epsilon: 0.0100 | Avg Reward: -0.23\n",
      "ğŸ“˜ Episode 8100 | Epsilon: 0.0100 | Avg Reward: -0.43\n",
      "ğŸ“˜ Episode 8200 | Epsilon: 0.0100 | Avg Reward: -0.66\n",
      "ğŸ“˜ Episode 8300 | Epsilon: 0.0100 | Avg Reward: -0.40\n",
      "ğŸ“˜ Episode 8400 | Epsilon: 0.0100 | Avg Reward: -0.47\n",
      "ğŸ“˜ Episode 8500 | Epsilon: 0.0100 | Avg Reward: -0.30\n",
      "ğŸ“˜ Episode 8600 | Epsilon: 0.0100 | Avg Reward: -0.24\n",
      "ğŸ“˜ Episode 8700 | Epsilon: 0.0100 | Avg Reward: -0.80\n",
      "ğŸ“˜ Episode 8800 | Epsilon: 0.0100 | Avg Reward: -0.07\n",
      "ğŸ“˜ Episode 8900 | Epsilon: 0.0100 | Avg Reward: -0.24\n",
      "ğŸ“˜ Episode 9000 | Epsilon: 0.0100 | Avg Reward: -0.69\n",
      "ğŸ“˜ Episode 9100 | Epsilon: 0.0100 | Avg Reward: -0.69\n",
      "ğŸ“˜ Episode 9200 | Epsilon: 0.0100 | Avg Reward: -0.37\n",
      "ğŸ“˜ Episode 9300 | Epsilon: 0.0100 | Avg Reward: -0.37\n",
      "ğŸ“˜ Episode 9400 | Epsilon: 0.0100 | Avg Reward: -0.61\n",
      "ğŸ“˜ Episode 9500 | Epsilon: 0.0100 | Avg Reward: -0.33\n",
      "ğŸ“˜ Episode 9600 | Epsilon: 0.0100 | Avg Reward: -0.53\n",
      "ğŸ“˜ Episode 9700 | Epsilon: 0.0100 | Avg Reward: -0.43\n",
      "ğŸ“˜ Episode 9800 | Epsilon: 0.0100 | Avg Reward: -0.27\n",
      "ğŸ“˜ Episode 9900 | Epsilon: 0.0100 | Avg Reward: -0.70\n",
      "ğŸ“˜ Episode 10000 | Epsilon: 0.0100 | Avg Reward: -0.52\n",
      "âœ… Loaded best model from episode 1487 | Avg Reward: 0.42\n",
      "Completed training for 6 deck(s)\n",
      "All models trained successfully!\n"
     ]
    }
   ],
   "source": [
    "# Train models for different deck counts\n",
    "models = {}\n",
    "\n",
    "for num_decks in range(1, 7):\n",
    "    print(f\"\\n=== Training model for {num_decks} deck(s) ===\")\n",
    "    env = BlackjackEnv(numdecks=num_decks, natural=False)\n",
    "    model_save_path = f\"blackjack_dqn_decks_{num_decks}.pth\"\n",
    "    model, _ = train_dqn(env, n_episodes=10000, model_save_path=model_save_path)\n",
    "    models[num_decks] = model\n",
    "    print(f\"Completed training for {num_decks} deck(s)\")\n",
    "\n",
    "print(\"All models trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b19d0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DQN model for 1 deck(s)...\n",
      "âœ… Successfully loaded model for 1 deck(s) from blackjack_dqn_decks_1.pth\n",
      "   Average reward during training: 0.4200\n",
      "   Saved at episode: 5640\n",
      "Loading DQN model for 2 deck(s)...\n",
      "âœ… Successfully loaded model for 2 deck(s) from blackjack_dqn_decks_2.pth\n",
      "   Average reward during training: -0.1300\n",
      "   Saved at episode: 119\n",
      "Loading DQN model for 3 deck(s)...\n",
      "âœ… Successfully loaded model for 3 deck(s) from blackjack_dqn_decks_3.pth\n",
      "   Average reward during training: -0.2100\n",
      "   Saved at episode: 4438\n",
      "Loading DQN model for 4 deck(s)...\n",
      "âœ… Successfully loaded model for 4 deck(s) from blackjack_dqn_decks_4.pth\n",
      "   Average reward during training: -0.2400\n",
      "   Saved at episode: 1131\n",
      "Loading DQN model for 5 deck(s)...\n",
      "âœ… Successfully loaded model for 5 deck(s) from blackjack_dqn_decks_5.pth\n",
      "   Average reward during training: 0.4700\n",
      "   Saved at episode: 8091\n",
      "Loading DQN model for 6 deck(s)...\n",
      "âœ… Successfully loaded model for 6 deck(s) from blackjack_dqn_decks_6.pth\n",
      "   Average reward during training: 0.4200\n",
      "   Saved at episode: 1487\n",
      "\n",
      "All DQN models loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "from collections import deque\n",
    "\n",
    "# Define the Q-network\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Load the saved DQN models\n",
    "dqn_models = {}\n",
    "\n",
    "for num_decks in range(1, 7):\n",
    "    print(f\"Loading DQN model for {num_decks} deck(s)...\")\n",
    "    model_path = f\"blackjack_dqn_decks_{num_decks}.pth\"\n",
    "    \n",
    "    # Check if the model file exists\n",
    "    if os.path.exists(model_path):\n",
    "        # Create a new model instance with the correct architecture\n",
    "        input_dim = 4  # [player_sum, dealer_card, usable_ace, can_double]\n",
    "        output_dim = 4  # [stick, hit, double, split]\n",
    "        model = QNetwork(input_dim, output_dim)\n",
    "        \n",
    "        # Load the saved weights\n",
    "        checkpoint = torch.load(model_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        # Set to evaluation mode\n",
    "        model.eval()\n",
    "        \n",
    "        # Store in the models dictionary\n",
    "        dqn_models[num_decks] = model\n",
    "        \n",
    "        print(f\"âœ… Successfully loaded model for {num_decks} deck(s) from {model_path}\")\n",
    "        if 'avg_reward' in checkpoint:\n",
    "            print(f\"   Average reward during training: {checkpoint['avg_reward']:.4f}\")\n",
    "        if 'episode' in checkpoint:\n",
    "            print(f\"   Saved at episode: {checkpoint['episode']}\")\n",
    "    else:\n",
    "        print(f\"âŒ Model file not found: {model_path}\")\n",
    "        print(f\"   Creating a new untrained model for {num_decks} deck(s)\")\n",
    "        \n",
    "        # Create an untrained model as fallback\n",
    "        input_dim = 4\n",
    "        output_dim = 4\n",
    "        model = QNetwork(input_dim, output_dim)\n",
    "        dqn_models[num_decks] = model\n",
    "\n",
    "print(\"\\nAll DQN models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d956f654",
   "metadata": {},
   "source": [
    "## Evaluation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c758a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Decks</th>\n",
       "      <th>Games</th>\n",
       "      <th>Wins</th>\n",
       "      <th>Draws</th>\n",
       "      <th>Losses</th>\n",
       "      <th>Total Reward</th>\n",
       "      <th>Win Rate (%)</th>\n",
       "      <th>Loss Rate (%)</th>\n",
       "      <th>Draw Rate (%)</th>\n",
       "      <th>Average Reward</th>\n",
       "      <th>Natural Player</th>\n",
       "      <th>Natural Dealer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>4434</td>\n",
       "      <td>441</td>\n",
       "      <td>5125</td>\n",
       "      <td>-4716.0</td>\n",
       "      <td>44.34</td>\n",
       "      <td>51.25</td>\n",
       "      <td>4.41</td>\n",
       "      <td>-0.4716</td>\n",
       "      <td>348</td>\n",
       "      <td>424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>10000</td>\n",
       "      <td>3419</td>\n",
       "      <td>270</td>\n",
       "      <td>6311</td>\n",
       "      <td>-7844.0</td>\n",
       "      <td>34.19</td>\n",
       "      <td>63.11</td>\n",
       "      <td>2.70</td>\n",
       "      <td>-0.7844</td>\n",
       "      <td>63</td>\n",
       "      <td>470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>10000</td>\n",
       "      <td>3854</td>\n",
       "      <td>452</td>\n",
       "      <td>5694</td>\n",
       "      <td>-7129.0</td>\n",
       "      <td>38.54</td>\n",
       "      <td>56.94</td>\n",
       "      <td>4.52</td>\n",
       "      <td>-0.7129</td>\n",
       "      <td>299</td>\n",
       "      <td>470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>10000</td>\n",
       "      <td>3965</td>\n",
       "      <td>406</td>\n",
       "      <td>5629</td>\n",
       "      <td>-6472.0</td>\n",
       "      <td>39.65</td>\n",
       "      <td>56.29</td>\n",
       "      <td>4.06</td>\n",
       "      <td>-0.6472</td>\n",
       "      <td>244</td>\n",
       "      <td>452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>10000</td>\n",
       "      <td>5643</td>\n",
       "      <td>607</td>\n",
       "      <td>3750</td>\n",
       "      <td>183.0</td>\n",
       "      <td>56.43</td>\n",
       "      <td>37.50</td>\n",
       "      <td>6.07</td>\n",
       "      <td>0.0183</td>\n",
       "      <td>304</td>\n",
       "      <td>495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>10000</td>\n",
       "      <td>4829</td>\n",
       "      <td>453</td>\n",
       "      <td>4718</td>\n",
       "      <td>-2680.0</td>\n",
       "      <td>48.29</td>\n",
       "      <td>47.18</td>\n",
       "      <td>4.53</td>\n",
       "      <td>-0.2680</td>\n",
       "      <td>318</td>\n",
       "      <td>414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Decks  Games  Wins  Draws  Losses  Total Reward  Win Rate (%)  \\\n",
       "0      1  10000  4434    441    5125       -4716.0         44.34   \n",
       "1      2  10000  3419    270    6311       -7844.0         34.19   \n",
       "2      3  10000  3854    452    5694       -7129.0         38.54   \n",
       "3      4  10000  3965    406    5629       -6472.0         39.65   \n",
       "4      5  10000  5643    607    3750         183.0         56.43   \n",
       "5      6  10000  4829    453    4718       -2680.0         48.29   \n",
       "\n",
       "   Loss Rate (%)  Draw Rate (%)  Average Reward  Natural Player  \\\n",
       "0          51.25           4.41         -0.4716             348   \n",
       "1          63.11           2.70         -0.7844              63   \n",
       "2          56.94           4.52         -0.7129             299   \n",
       "3          56.29           4.06         -0.6472             244   \n",
       "4          37.50           6.07          0.0183             304   \n",
       "5          47.18           4.53         -0.2680             318   \n",
       "\n",
       "   Natural Dealer  \n",
       "0             424  \n",
       "1             470  \n",
       "2             470  \n",
       "3             452  \n",
       "4             495  \n",
       "5             414  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Helper to preprocess state matching your training code\n",
    "def preprocess_state(state):\n",
    "    \"\"\"\n",
    "    Converts the BlackjackEnv state to a format usable by the neural network\n",
    "    State format: ((card1, card2), dealer_card, usable_ace, can_double)\n",
    "    \"\"\"\n",
    "    player_cards, dealer_card, usable_ace, can_double = state\n",
    "    \n",
    "    # Convert dealer_card using card_value function\n",
    "    dealer_value = card_value(dealer_card)\n",
    "    \n",
    "    # For player cards tuple, calculate sum using card_value\n",
    "    player_sum = 0\n",
    "    for card in player_cards:\n",
    "        if card != 0:  # Skip zero values (padding)\n",
    "            player_sum += card_value(card)\n",
    "    \n",
    "    return np.array([player_sum, dealer_value, usable_ace, can_double], dtype=np.float32)\n",
    "\n",
    "# === RL Evaluation Simulation ===\n",
    "def evaluate_dqn_on_deck_sizes(models, num_games=10000, max_decks=6):\n",
    "    results = []\n",
    "\n",
    "    for num_deck in range(1, max_decks + 1):\n",
    "        env = BlackjackEnv(numdecks=num_deck, natural=False)\n",
    "        q_network = models[num_deck]  # Get the specific model for this deck size\n",
    "\n",
    "        wins = 0\n",
    "        losses = 0\n",
    "        draws = 0\n",
    "        \n",
    "        total_reward = 0\n",
    "        count_natural_player = 0\n",
    "        count_natural_dealer = 0\n",
    "\n",
    "        for game in range(1, num_games+1):\n",
    "            obs = env.reset(seed=game)\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                # Process state to match training format\n",
    "                state = preprocess_state(obs)\n",
    "                \n",
    "                try:\n",
    "                    with torch.no_grad():\n",
    "                        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "                        q_values = q_network(state_tensor)\n",
    "                        \n",
    "                        # Get valid actions\n",
    "                        valid_actions = [0, 1]  # Default stick and hit\n",
    "                        player_cards, dealer_card, usable_ace, can_double = obs\n",
    "                        \n",
    "                        if can_double:\n",
    "                            valid_actions.append(2)\n",
    "                            \n",
    "                        if env.current_hand < len(env.hands):\n",
    "                            current_hand = env.hands[env.current_hand]\n",
    "                            if len(current_hand) == 2 and card_value(current_hand[0]) == card_value(current_hand[1]):\n",
    "                                valid_actions.append(3)\n",
    "                        \n",
    "                        # Mask invalid actions\n",
    "                        for action in range(env.action_space.n):\n",
    "                            if action not in valid_actions:\n",
    "                                q_values[0, action] = float('-inf')\n",
    "                                \n",
    "                        action = q_values.argmax().item()\n",
    "                        \n",
    "                    next_obs, reward, done, _ = env.step(action)\n",
    "                    episode_reward += reward\n",
    "                    obs = next_obs\n",
    "                    \n",
    "                    # Check for naturals when game ends\n",
    "                    if done:\n",
    "                        if hasattr(env, 'hands') and len(env.hands) > 0:\n",
    "                            if is_natural(env.hands[0]):\n",
    "                                count_natural_player += 1\n",
    "                        \n",
    "                        if hasattr(env, 'dealer'):\n",
    "                            if is_natural(env.dealer):\n",
    "                                count_natural_dealer += 1\n",
    "                    \n",
    "                except ValueError:\n",
    "                    # If error, try a fallback action\n",
    "                    action = 0  # Stick is usually safe\n",
    "                    next_obs, reward, done, _ = env.step(action)\n",
    "                    episode_reward += reward\n",
    "                    obs = next_obs\n",
    "\n",
    "            total_reward += episode_reward\n",
    "            if episode_reward > 0:\n",
    "                wins += 1\n",
    "            elif episode_reward < 0:\n",
    "                losses += 1\n",
    "            else:\n",
    "                draws += 1\n",
    "\n",
    "        # Store results\n",
    "        results.append({\n",
    "            \"Decks\": num_deck,\n",
    "            \"Games\": num_games,\n",
    "            \"Wins\": wins,\n",
    "            \"Draws\": draws,\n",
    "            \"Losses\": losses,\n",
    "            \"Total Reward\": round(total_reward, 4),\n",
    "            \"Win Rate (%)\": round((wins / num_games) * 100, 4),\n",
    "            \"Loss Rate (%)\": round((losses / num_games) * 100, 4),\n",
    "            \"Draw Rate (%)\": round((draws / num_games) * 100, 4),\n",
    "            \"Average Reward\": round(total_reward / num_games, 4),\n",
    "            \"Natural Player\": count_natural_player,\n",
    "            \"Natural Dealer\": count_natural_dealer,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Example usage:\n",
    "df_rl = evaluate_dqn_on_deck_sizes(dqn_models, num_games=10000, max_decks=6)\n",
    "df_rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6676dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running DQN bankroll experiment...\n",
      "DQN bankroll experiment completed!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Decks</th>\n",
       "      <th>Games</th>\n",
       "      <th>Wins</th>\n",
       "      <th>Draws</th>\n",
       "      <th>Losses</th>\n",
       "      <th>Total Reward</th>\n",
       "      <th>Win Rate (%)</th>\n",
       "      <th>Loss Rate (%)</th>\n",
       "      <th>Draw Rate (%)</th>\n",
       "      <th>Average Reward</th>\n",
       "      <th>Final Money</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>188</td>\n",
       "      <td>81</td>\n",
       "      <td>11</td>\n",
       "      <td>96</td>\n",
       "      <td>-105.0</td>\n",
       "      <td>43.0851</td>\n",
       "      <td>51.0638</td>\n",
       "      <td>5.8511</td>\n",
       "      <td>-0.5585</td>\n",
       "      <td>0</td>\n",
       "      <td>Bankrupt after 188 games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>259</td>\n",
       "      <td>95</td>\n",
       "      <td>9</td>\n",
       "      <td>155</td>\n",
       "      <td>-188.0</td>\n",
       "      <td>36.6795</td>\n",
       "      <td>59.8456</td>\n",
       "      <td>3.4749</td>\n",
       "      <td>-0.7259</td>\n",
       "      <td>0</td>\n",
       "      <td>Bankrupt after 259 games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>226</td>\n",
       "      <td>135</td>\n",
       "      <td>11</td>\n",
       "      <td>80</td>\n",
       "      <td>77.0</td>\n",
       "      <td>59.7345</td>\n",
       "      <td>35.3982</td>\n",
       "      <td>4.8673</td>\n",
       "      <td>0.3407</td>\n",
       "      <td>0</td>\n",
       "      <td>Bankrupt after 226 games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>162</td>\n",
       "      <td>90</td>\n",
       "      <td>11</td>\n",
       "      <td>61</td>\n",
       "      <td>30.0</td>\n",
       "      <td>55.5556</td>\n",
       "      <td>37.6543</td>\n",
       "      <td>6.7901</td>\n",
       "      <td>0.1852</td>\n",
       "      <td>0</td>\n",
       "      <td>Bankrupt after 162 games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>163</td>\n",
       "      <td>58</td>\n",
       "      <td>7</td>\n",
       "      <td>98</td>\n",
       "      <td>-152.0</td>\n",
       "      <td>35.5828</td>\n",
       "      <td>60.1227</td>\n",
       "      <td>4.2945</td>\n",
       "      <td>-0.9325</td>\n",
       "      <td>-1</td>\n",
       "      <td>Bankrupt after 163 games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>117</td>\n",
       "      <td>45</td>\n",
       "      <td>9</td>\n",
       "      <td>63</td>\n",
       "      <td>-83.0</td>\n",
       "      <td>38.4615</td>\n",
       "      <td>53.8462</td>\n",
       "      <td>7.6923</td>\n",
       "      <td>-0.7094</td>\n",
       "      <td>-1</td>\n",
       "      <td>Bankrupt after 117 games</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Decks  Games  Wins  Draws  Losses  Total Reward  Win Rate (%)  \\\n",
       "0      1    188    81     11      96        -105.0       43.0851   \n",
       "1      2    259    95      9     155        -188.0       36.6795   \n",
       "2      3    226   135     11      80          77.0       59.7345   \n",
       "3      4    162    90     11      61          30.0       55.5556   \n",
       "4      5    163    58      7      98        -152.0       35.5828   \n",
       "5      6    117    45      9      63         -83.0       38.4615   \n",
       "\n",
       "   Loss Rate (%)  Draw Rate (%)  Average Reward  Final Money  \\\n",
       "0        51.0638         5.8511         -0.5585            0   \n",
       "1        59.8456         3.4749         -0.7259            0   \n",
       "2        35.3982         4.8673          0.3407            0   \n",
       "3        37.6543         6.7901          0.1852            0   \n",
       "4        60.1227         4.2945         -0.9325           -1   \n",
       "5        53.8462         7.6923         -0.7094           -1   \n",
       "\n",
       "                     Status  \n",
       "0  Bankrupt after 188 games  \n",
       "1  Bankrupt after 259 games  \n",
       "2  Bankrupt after 226 games  \n",
       "3  Bankrupt after 162 games  \n",
       "4  Bankrupt after 163 games  \n",
       "5  Bankrupt after 117 games  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to select action with the DQN model (no exploration)\n",
    "def select_action_eval(state, q_network, env):\n",
    "    # Extract state components to check for valid actions\n",
    "    player_cards, dealer_card, usable_ace, can_double = state\n",
    "    \n",
    "    # Default valid actions (stick and hit)\n",
    "    valid_actions = [0, 1]  \n",
    "    \n",
    "    # Check if can double down\n",
    "    if can_double:\n",
    "        valid_actions.append(2)\n",
    "    \n",
    "    # Check if the current hand allows split\n",
    "    if hasattr(env, 'current_hand') and hasattr(env, 'hands'):\n",
    "        if env.current_hand < len(env.hands):\n",
    "            current_hand = env.hands[env.current_hand]\n",
    "            # Check if can split (same card value and exactly 2 cards)\n",
    "            if len(current_hand) == 2 and card_value(current_hand[0]) == card_value(current_hand[1]):\n",
    "                valid_actions.append(3)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        state_tensor = torch.FloatTensor(preprocess_state(state)).unsqueeze(0)\n",
    "        q_values = q_network(state_tensor)\n",
    "        \n",
    "        # Mask invalid actions by setting their Q-values to -inf\n",
    "        for action in range(4):  # Assume 4 possible actions\n",
    "            if action not in valid_actions:\n",
    "                q_values[0, action] = float('-inf')\n",
    "        \n",
    "        return q_values.argmax().item()\n",
    "\n",
    "def evaluate_dqn_bankroll(models, num_games=10000, max_decks=6, initial_money=100):\n",
    "    \"\"\"\n",
    "    Evaluate DQN models with a bankroll simulation across different deck sizes\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for num_deck in range(1, max_decks + 1):\n",
    "        env = BlackjackEnv(numdecks=num_deck, natural=False)\n",
    "        q_network = models[num_deck]  # Get the specific model for this deck size\n",
    "        q_network.eval()  # Set the model to evaluation mode\n",
    "\n",
    "        money = initial_money\n",
    "        wins = 0\n",
    "        losses = 0\n",
    "        draws = 0\n",
    "        total_reward = 0\n",
    "        count_natural_player = 0\n",
    "        count_natural_dealer = 0\n",
    "        \n",
    "        # For tracking bankruptcy\n",
    "        games_played = 0\n",
    "        went_bankrupt = False\n",
    "\n",
    "        for game in range(1, num_games+1):\n",
    "            if money <= 0:\n",
    "                went_bankrupt = True\n",
    "                games_played = game - 1\n",
    "                break\n",
    "                \n",
    "            games_played = game\n",
    "            obs = env.reset()\n",
    "            done = False\n",
    "            \n",
    "            # Bet $1\n",
    "            money -= 1\n",
    "            episode_reward = 0\n",
    "            doubled_down = False\n",
    "\n",
    "            while not done:\n",
    "                # Get action from the model\n",
    "                action = select_action_eval(obs, q_network, env)\n",
    "                \n",
    "                # If doubling down, subtract another dollar\n",
    "                if action == 2:  # Double down\n",
    "                    money -= 1\n",
    "                    doubled_down = True\n",
    "                \n",
    "                # Execute the action\n",
    "                try:\n",
    "                    next_obs, reward, done, _ = env.step(action)\n",
    "                    episode_reward += reward\n",
    "                    obs = next_obs\n",
    "                    \n",
    "                    # Check for naturals when game ends\n",
    "                    if done:\n",
    "                        if hasattr(env, 'hands') and len(env.hands) > 0:\n",
    "                            if is_natural(env.hands[0]):\n",
    "                                count_natural_player += 1\n",
    "                        \n",
    "                        if hasattr(env, 'dealer'):\n",
    "                            if is_natural(env.dealer):\n",
    "                                count_natural_dealer += 1\n",
    "                except ValueError:\n",
    "                    # Fallback if error\n",
    "                    action = 0  # Stick\n",
    "                    next_obs, reward, done, _ = env.step(action)\n",
    "                    episode_reward += reward\n",
    "                    obs = next_obs\n",
    "\n",
    "            # End of episode accounting\n",
    "            total_reward += episode_reward\n",
    "            \n",
    "            if episode_reward > 0:\n",
    "                wins += 1\n",
    "                # Calculate payout\n",
    "                if doubled_down:\n",
    "                    money += 4  # Get 2x the doubled bet\n",
    "                else:\n",
    "                    if episode_reward > 1:  # Blackjack\n",
    "                        money += 2  # Regular win\n",
    "            elif episode_reward < 0:\n",
    "                losses += 1\n",
    "                # Money already subtracted for bet\n",
    "            else:\n",
    "                draws += 1\n",
    "                if doubled_down:\n",
    "                    money += 2  # Get doubled bet back\n",
    "                else:\n",
    "                    money += 1  # Get original bet back\n",
    "\n",
    "        # Store results\n",
    "        bankruptcy_message = f\"Bankrupt after {games_played} games\" if went_bankrupt else \"Solvent\"\n",
    "        \n",
    "        results.append({\n",
    "            \"Decks\": num_deck,\n",
    "            \"Games\": games_played,\n",
    "            \"Wins\": wins,\n",
    "            \"Draws\": draws,\n",
    "            \"Losses\": losses,\n",
    "            \"Total Reward\": round(total_reward, 4),\n",
    "            \"Win Rate (%)\": round((wins / games_played) * 100, 4) if games_played > 0 else 0,\n",
    "            \"Loss Rate (%)\": round((losses / games_played) * 100, 4) if games_played > 0 else 0,\n",
    "            \"Draw Rate (%)\": round((draws / games_played) * 100, 4) if games_played > 0 else 0,\n",
    "            \"Average Reward\": round(total_reward / games_played, 4) if games_played > 0 else 0,\n",
    "            \"Final Money\": round(money, 2),\n",
    "            \"Status\": bankruptcy_message\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Evaluate DQN model performance using the bankroll experiment\n",
    "print(\"\\nRunning DQN bankroll experiment...\")\n",
    "df_dqn_bankroll = evaluate_dqn_bankroll(dqn_models, num_games=10000, max_decks=6, initial_money=100)\n",
    "print(\"DQN bankroll experiment completed!\")\n",
    "\n",
    "# Display the results\n",
    "df_dqn_bankroll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3347af83",
   "metadata": {},
   "source": [
    "# Set PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "513ec849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "from collections import deque\n",
    "\n",
    "# PPO Actor-Critic Network\n",
    "class PPOActorCritic(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(PPOActorCritic, self).__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.policy_head = nn.Linear(64, output_dim)\n",
    "        self.value_head = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared_out = self.shared(x)\n",
    "        logits = self.policy_head(shared_out)\n",
    "        value = self.value_head(shared_out)\n",
    "        return logits, value\n",
    "\n",
    "# Adjusted for BlackjackEnv\n",
    "def preprocess_state(state):\n",
    "    \"\"\"\n",
    "    Converts the BlackjackEnv state to a format usable by the neural network\n",
    "    State format: ((card1, card2), dealer_card, usable_ace, can_double)\n",
    "    \"\"\"\n",
    "    player_cards, dealer_card, usable_ace, can_double = state\n",
    "    \n",
    "    # Convert dealer_card using card_value function\n",
    "    dealer_value = card_value(dealer_card)\n",
    "    \n",
    "    # For player cards tuple, calculate sum using card_value\n",
    "    player_sum = 0\n",
    "    for card in player_cards:\n",
    "        if card != 0:  # Skip zero values (padding)\n",
    "            player_sum += card_value(card)\n",
    "    \n",
    "    return np.array([player_sum, dealer_value, usable_ace, can_double], dtype=np.float32)\n",
    "\n",
    "# Function to select valid actions\n",
    "def select_valid_action(logits, state, env):\n",
    "    player_cards, dealer_card, usable_ace, can_double = state\n",
    "    \n",
    "    # Default valid actions (stick and hit)\n",
    "    valid_actions = [0, 1]\n",
    "    \n",
    "    # Check if can double down\n",
    "    if can_double:\n",
    "        valid_actions.append(2)\n",
    "    \n",
    "    # Check if can split\n",
    "    if env.current_hand < len(env.hands):\n",
    "        current_hand = env.hands[env.current_hand]\n",
    "        if len(current_hand) == 2 and card_value(current_hand[0]) == card_value(current_hand[1]):\n",
    "            valid_actions.append(3)\n",
    "    \n",
    "    # Apply mask to logits\n",
    "    masked_logits = logits.clone()\n",
    "    for action in range(env.action_space.n):\n",
    "        if action not in valid_actions:\n",
    "            masked_logits[action] = float('-inf')\n",
    "    \n",
    "    return masked_logits\n",
    "\n",
    "# Compute GAE\n",
    "def compute_gae(rewards, values, dones, gamma=0.99, lam=0.95):\n",
    "    returns = []\n",
    "    advantages = []\n",
    "    gae = 0\n",
    "    next_value = 0\n",
    "\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * next_value * (1 - dones[step]) - values[step]\n",
    "        gae = delta + gamma * lam * (1 - dones[step]) * gae\n",
    "        advantages.insert(0, gae)\n",
    "        returns.insert(0, gae + values[step])\n",
    "        next_value = values[step]\n",
    "\n",
    "    return torch.FloatTensor(returns), torch.FloatTensor(advantages)\n",
    "\n",
    "# PPO Training Function\n",
    "def train_ppo(env, n_episodes=5000, gamma=0.99, lam=0.95, clip_eps=0.2,\n",
    "              lr=3e-4, epochs=4, batch_size=64, model_save_path='ppo_blackjack.pth'):\n",
    "\n",
    "    input_dim = 4  # [player_sum, dealer_card, usable_ace, can_double]\n",
    "    output_dim = env.action_space.n\n",
    "\n",
    "    policy_net = PPOActorCritic(input_dim, output_dim)\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "\n",
    "    memory = []\n",
    "    reward_window = deque(maxlen=100)\n",
    "\n",
    "    best_avg_reward = float('-inf')\n",
    "    best_model = None\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        episode_data = []\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            state = preprocess_state(obs)\n",
    "            state_tensor = torch.FloatTensor(state)\n",
    "            logits, value = policy_net(state_tensor)\n",
    "            \n",
    "            # Mask invalid actions\n",
    "            masked_logits = select_valid_action(logits, obs, env)\n",
    "            probs = torch.softmax(masked_logits, dim=-1)\n",
    "            dist = torch.distributions.Categorical(probs)\n",
    "            \n",
    "            try:\n",
    "                action = dist.sample()\n",
    "                log_prob = dist.log_prob(action)\n",
    "                \n",
    "                next_obs, reward, done, _ = env.step(action.item())\n",
    "                episode_data.append((state, action.item(), reward, log_prob.item(), value.item(), done))\n",
    "                episode_reward += reward\n",
    "                obs = next_obs\n",
    "                \n",
    "            except ValueError:\n",
    "                # Fallback to a safe action (stick)\n",
    "                action = 0  # stick\n",
    "                next_obs, reward, done, _ = env.step(action)\n",
    "                \n",
    "                # Re-compute log_prob for the fallback action\n",
    "                masked_logits = select_valid_action(logits, obs, env)\n",
    "                probs = torch.softmax(masked_logits, dim=-1)\n",
    "                dist = torch.distributions.Categorical(probs)\n",
    "                log_prob = dist.log_prob(torch.tensor(action))\n",
    "                \n",
    "                episode_data.append((state, action, reward, log_prob.item(), value.item(), done))\n",
    "                episode_reward += reward\n",
    "                obs = next_obs\n",
    "\n",
    "        memory.extend(episode_data)\n",
    "        reward_window.append(episode_reward)\n",
    "\n",
    "        # Train when we have enough data\n",
    "        if len(memory) >= batch_size:\n",
    "            states, actions, rewards, old_log_probs, values, dones = zip(*memory)\n",
    "\n",
    "            # Process states to tensors\n",
    "            states_tensor = torch.FloatTensor(np.array(states))\n",
    "            actions_tensor = torch.LongTensor(actions)\n",
    "            old_log_probs_tensor = torch.FloatTensor(old_log_probs)\n",
    "            \n",
    "            returns, advantages = compute_gae(rewards, values, dones, gamma, lam)\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "            for _ in range(epochs):\n",
    "                logits, value_preds = policy_net(states_tensor)\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                dist = torch.distributions.Categorical(probs)\n",
    "\n",
    "                new_log_probs = dist.log_prob(actions_tensor)\n",
    "                ratio = torch.exp(new_log_probs - old_log_probs_tensor)\n",
    "\n",
    "                policy_loss = -torch.min(\n",
    "                    ratio * advantages,\n",
    "                    torch.clamp(ratio, 1 - clip_eps, 1 + clip_eps) * advantages\n",
    "                ).mean()\n",
    "\n",
    "                value_loss = nn.MSELoss()(value_preds.squeeze(), returns)\n",
    "\n",
    "                loss = policy_loss + 0.5 * value_loss - 0.01 * dist.entropy().mean()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            memory = []\n",
    "\n",
    "        # Save model based on rolling average\n",
    "        if len(reward_window) == 100:\n",
    "            avg_reward = np.mean(reward_window)\n",
    "            if avg_reward > best_avg_reward:\n",
    "                best_avg_reward = avg_reward\n",
    "                best_model = policy_net\n",
    "                torch.save({\n",
    "                    'model_state_dict': best_model.state_dict(),\n",
    "                    'avg_reward': best_avg_reward,\n",
    "                    'episode': episode + 1\n",
    "                }, model_save_path)\n",
    "                print(f\"âœ… Best model saved at episode {episode+1} | Avg Reward: {best_avg_reward:.4f}\")\n",
    "\n",
    "        if (episode + 1) % 500 == 0:\n",
    "            print(f\"Episode {episode+1} | Avg Reward: {np.mean(list(reward_window)):.4f}\")\n",
    "\n",
    "    return best_model if best_model else policy_net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb8c4e4",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6827439f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training PPO model for 1 deck(s) ===\n",
      "âœ… Best model saved at episode 100 | Avg Reward: -0.7000\n",
      "âœ… Best model saved at episode 104 | Avg Reward: -0.6900\n",
      "âœ… Best model saved at episode 133 | Avg Reward: -0.6700\n",
      "âœ… Best model saved at episode 135 | Avg Reward: -0.6400\n",
      "âœ… Best model saved at episode 136 | Avg Reward: -0.6200\n",
      "âœ… Best model saved at episode 137 | Avg Reward: -0.5700\n",
      "âœ… Best model saved at episode 146 | Avg Reward: -0.5500\n",
      "âœ… Best model saved at episode 148 | Avg Reward: -0.5400\n",
      "âœ… Best model saved at episode 149 | Avg Reward: -0.5100\n",
      "âœ… Best model saved at episode 150 | Avg Reward: -0.4900\n",
      "âœ… Best model saved at episode 151 | Avg Reward: -0.4700\n",
      "âœ… Best model saved at episode 225 | Avg Reward: -0.4500\n",
      "âœ… Best model saved at episode 227 | Avg Reward: -0.4300\n",
      "âœ… Best model saved at episode 324 | Avg Reward: -0.4100\n",
      "âœ… Best model saved at episode 325 | Avg Reward: -0.4000\n",
      "âœ… Best model saved at episode 328 | Avg Reward: -0.3800\n",
      "âœ… Best model saved at episode 329 | Avg Reward: -0.3700\n",
      "âœ… Best model saved at episode 330 | Avg Reward: -0.3300\n",
      "âœ… Best model saved at episode 331 | Avg Reward: -0.3100\n",
      "âœ… Best model saved at episode 333 | Avg Reward: -0.2800\n",
      "âœ… Best model saved at episode 345 | Avg Reward: -0.2600\n",
      "âœ… Best model saved at episode 348 | Avg Reward: -0.2300\n",
      "âœ… Best model saved at episode 349 | Avg Reward: -0.2100\n",
      "âœ… Best model saved at episode 353 | Avg Reward: -0.2000\n",
      "âœ… Best model saved at episode 357 | Avg Reward: -0.1600\n",
      "âœ… Best model saved at episode 358 | Avg Reward: -0.1400\n",
      "âœ… Best model saved at episode 364 | Avg Reward: -0.1300\n",
      "âœ… Best model saved at episode 365 | Avg Reward: -0.1100\n",
      "Episode 500 | Avg Reward: -0.3300\n",
      "âœ… Best model saved at episode 577 | Avg Reward: -0.0900\n",
      "âœ… Best model saved at episode 578 | Avg Reward: -0.0700\n",
      "âœ… Best model saved at episode 586 | Avg Reward: -0.0300\n",
      "âœ… Best model saved at episode 595 | Avg Reward: -0.0100\n",
      "âœ… Best model saved at episode 605 | Avg Reward: 0.0000\n",
      "âœ… Best model saved at episode 606 | Avg Reward: 0.0300\n",
      "âœ… Best model saved at episode 609 | Avg Reward: 0.0400\n",
      "âœ… Best model saved at episode 616 | Avg Reward: 0.0500\n",
      "âœ… Best model saved at episode 618 | Avg Reward: 0.0600\n",
      "âœ… Best model saved at episode 626 | Avg Reward: 0.0700\n",
      "âœ… Best model saved at episode 627 | Avg Reward: 0.0900\n",
      "âœ… Best model saved at episode 634 | Avg Reward: 0.1000\n",
      "âœ… Best model saved at episode 921 | Avg Reward: 0.1100\n",
      "âœ… Best model saved at episode 922 | Avg Reward: 0.1200\n",
      "âœ… Best model saved at episode 953 | Avg Reward: 0.1300\n",
      "âœ… Best model saved at episode 954 | Avg Reward: 0.1400\n",
      "Episode 1000 | Avg Reward: 0.0200\n",
      "âœ… Best model saved at episode 1182 | Avg Reward: 0.1700\n",
      "âœ… Best model saved at episode 1225 | Avg Reward: 0.1900\n",
      "âœ… Best model saved at episode 1227 | Avg Reward: 0.2100\n",
      "âœ… Best model saved at episode 1228 | Avg Reward: 0.2400\n",
      "âœ… Best model saved at episode 1296 | Avg Reward: 0.2500\n",
      "âœ… Best model saved at episode 1300 | Avg Reward: 0.2600\n",
      "âœ… Best model saved at episode 1301 | Avg Reward: 0.2800\n",
      "âœ… Best model saved at episode 1305 | Avg Reward: 0.2900\n",
      "âœ… Best model saved at episode 1307 | Avg Reward: 0.3100\n",
      "Episode 1500 | Avg Reward: -0.1500\n",
      "Episode 2000 | Avg Reward: 0.1300\n",
      "Episode 2500 | Avg Reward: -0.1000\n",
      "Episode 3000 | Avg Reward: 0.1500\n",
      "âœ… Best model saved at episode 3135 | Avg Reward: 0.3200\n",
      "Episode 3500 | Avg Reward: 0.0100\n",
      "Episode 4000 | Avg Reward: 0.1900\n",
      "Episode 4500 | Avg Reward: 0.0700\n",
      "âœ… Best model saved at episode 4710 | Avg Reward: 0.3300\n",
      "âœ… Best model saved at episode 4711 | Avg Reward: 0.3500\n",
      "âœ… Best model saved at episode 4713 | Avg Reward: 0.3800\n",
      "âœ… Best model saved at episode 4721 | Avg Reward: 0.3900\n",
      "âœ… Best model saved at episode 4722 | Avg Reward: 0.4100\n",
      "Episode 5000 | Avg Reward: 0.1200\n",
      "Episode 5500 | Avg Reward: 0.0800\n",
      "Episode 6000 | Avg Reward: 0.2800\n",
      "Episode 6500 | Avg Reward: -0.0100\n",
      "Episode 7000 | Avg Reward: 0.0800\n",
      "Episode 7500 | Avg Reward: 0.1000\n",
      "Episode 8000 | Avg Reward: 0.1800\n",
      "Episode 8500 | Avg Reward: 0.0300\n",
      "Episode 9000 | Avg Reward: 0.0200\n",
      "Episode 9500 | Avg Reward: 0.1500\n",
      "Episode 10000 | Avg Reward: 0.0200\n",
      "Episode 10500 | Avg Reward: 0.1300\n",
      "Episode 11000 | Avg Reward: -0.0300\n",
      "Episode 11500 | Avg Reward: -0.0400\n",
      "Episode 12000 | Avg Reward: 0.1100\n",
      "Episode 12500 | Avg Reward: 0.0200\n",
      "Episode 13000 | Avg Reward: 0.0900\n",
      "Episode 13500 | Avg Reward: 0.1100\n",
      "Episode 14000 | Avg Reward: 0.0400\n",
      "Episode 14500 | Avg Reward: 0.1400\n",
      "Episode 15000 | Avg Reward: -0.1600\n",
      "Episode 15500 | Avg Reward: -0.0600\n",
      "Episode 16000 | Avg Reward: 0.0700\n",
      "Episode 16500 | Avg Reward: -0.0400\n",
      "Episode 17000 | Avg Reward: 0.0100\n",
      "Episode 17500 | Avg Reward: 0.1100\n",
      "Episode 18000 | Avg Reward: -0.0100\n",
      "Episode 18500 | Avg Reward: 0.1800\n",
      "âœ… Best model saved at episode 18852 | Avg Reward: 0.4300\n",
      "Episode 19000 | Avg Reward: 0.2200\n",
      "Episode 19500 | Avg Reward: 0.0700\n",
      "Episode 20000 | Avg Reward: 0.0400\n",
      "Episode 20500 | Avg Reward: 0.0000\n",
      "Episode 21000 | Avg Reward: 0.3400\n",
      "Episode 21500 | Avg Reward: 0.2300\n",
      "Episode 22000 | Avg Reward: -0.0200\n",
      "Episode 22500 | Avg Reward: 0.1800\n",
      "Episode 23000 | Avg Reward: 0.1300\n",
      "Episode 23500 | Avg Reward: 0.0400\n",
      "Episode 24000 | Avg Reward: -0.0100\n",
      "Episode 24500 | Avg Reward: 0.2900\n",
      "Episode 25000 | Avg Reward: 0.1100\n",
      "Episode 25500 | Avg Reward: 0.2300\n",
      "Episode 26000 | Avg Reward: 0.0000\n",
      "Episode 26500 | Avg Reward: 0.2700\n",
      "Episode 27000 | Avg Reward: 0.1600\n",
      "Episode 27500 | Avg Reward: -0.0100\n",
      "Episode 28000 | Avg Reward: 0.1000\n",
      "Episode 28500 | Avg Reward: 0.2400\n",
      "Episode 29000 | Avg Reward: 0.3300\n",
      "âœ… Best model saved at episode 29258 | Avg Reward: 0.4400\n",
      "âœ… Best model saved at episode 29272 | Avg Reward: 0.4500\n",
      "Episode 29500 | Avg Reward: -0.0500\n",
      "Episode 30000 | Avg Reward: 0.1800\n",
      "Episode 30500 | Avg Reward: 0.0400\n",
      "Episode 31000 | Avg Reward: 0.0900\n",
      "Episode 31500 | Avg Reward: -0.0700\n",
      "âœ… Best model saved at episode 31615 | Avg Reward: 0.4600\n",
      "âœ… Best model saved at episode 31619 | Avg Reward: 0.4700\n",
      "Episode 32000 | Avg Reward: 0.2300\n",
      "Episode 32500 | Avg Reward: 0.0000\n",
      "Episode 33000 | Avg Reward: 0.1400\n",
      "Episode 33500 | Avg Reward: 0.0200\n",
      "Episode 34000 | Avg Reward: 0.3500\n",
      "Episode 34500 | Avg Reward: 0.1900\n",
      "Episode 35000 | Avg Reward: 0.0200\n",
      "Episode 35500 | Avg Reward: 0.0900\n",
      "Episode 36000 | Avg Reward: 0.1500\n",
      "Episode 36500 | Avg Reward: 0.2700\n",
      "Episode 37000 | Avg Reward: 0.3600\n",
      "Episode 37500 | Avg Reward: 0.2900\n",
      "Episode 38000 | Avg Reward: 0.2100\n",
      "Episode 38500 | Avg Reward: 0.1100\n",
      "âœ… Best model saved at episode 38761 | Avg Reward: 0.4800\n",
      "âœ… Best model saved at episode 38762 | Avg Reward: 0.5000\n",
      "Episode 39000 | Avg Reward: 0.1900\n",
      "Episode 39500 | Avg Reward: 0.1300\n",
      "Episode 40000 | Avg Reward: 0.2900\n",
      "Episode 40500 | Avg Reward: 0.0400\n",
      "âœ… Best model saved at episode 40701 | Avg Reward: 0.5200\n",
      "âœ… Best model saved at episode 40702 | Avg Reward: 0.5400\n",
      "Episode 41000 | Avg Reward: 0.4100\n",
      "Episode 41500 | Avg Reward: -0.0900\n",
      "Episode 42000 | Avg Reward: 0.0000\n",
      "Episode 42500 | Avg Reward: 0.2900\n",
      "Episode 43000 | Avg Reward: 0.0700\n",
      "Episode 43500 | Avg Reward: 0.0900\n",
      "Episode 44000 | Avg Reward: 0.3000\n",
      "Episode 44500 | Avg Reward: 0.3100\n",
      "Episode 45000 | Avg Reward: 0.3300\n",
      "Episode 45500 | Avg Reward: 0.3000\n",
      "Episode 46000 | Avg Reward: 0.0800\n",
      "Episode 46500 | Avg Reward: 0.2200\n",
      "Episode 47000 | Avg Reward: 0.2900\n",
      "Episode 47500 | Avg Reward: 0.2400\n",
      "Episode 48000 | Avg Reward: -0.0200\n",
      "Episode 48500 | Avg Reward: -0.1700\n",
      "Episode 49000 | Avg Reward: 0.2300\n",
      "Episode 49500 | Avg Reward: 0.1300\n",
      "Episode 50000 | Avg Reward: 0.1900\n",
      "Completed PPO training for 1 deck(s)\n",
      "\n",
      "=== Training PPO model for 2 deck(s) ===\n",
      "âœ… Best model saved at episode 100 | Avg Reward: 0.1500\n",
      "âœ… Best model saved at episode 102 | Avg Reward: 0.1700\n",
      "âœ… Best model saved at episode 463 | Avg Reward: 0.1900\n",
      "âœ… Best model saved at episode 464 | Avg Reward: 0.2100\n",
      "Episode 500 | Avg Reward: 0.1200\n",
      "Episode 1000 | Avg Reward: 0.0600\n",
      "âœ… Best model saved at episode 1211 | Avg Reward: 0.2200\n",
      "âœ… Best model saved at episode 1222 | Avg Reward: 0.2400\n",
      "âœ… Best model saved at episode 1223 | Avg Reward: 0.2600\n",
      "Episode 1500 | Avg Reward: -0.0300\n",
      "Episode 2000 | Avg Reward: -0.1200\n",
      "Episode 2500 | Avg Reward: 0.0200\n",
      "Episode 3000 | Avg Reward: -0.0500\n",
      "Episode 3500 | Avg Reward: 0.0400\n",
      "Episode 4000 | Avg Reward: -0.0600\n",
      "Episode 4500 | Avg Reward: 0.1200\n",
      "Episode 5000 | Avg Reward: 0.0700\n",
      "Episode 5500 | Avg Reward: 0.0300\n",
      "Episode 6000 | Avg Reward: 0.0000\n",
      "Episode 6500 | Avg Reward: -0.0800\n",
      "Episode 7000 | Avg Reward: 0.0400\n",
      "Episode 7500 | Avg Reward: 0.1100\n",
      "âœ… Best model saved at episode 7560 | Avg Reward: 0.2700\n",
      "Episode 8000 | Avg Reward: 0.1400\n",
      "âœ… Best model saved at episode 8037 | Avg Reward: 0.2900\n",
      "Episode 8500 | Avg Reward: 0.0700\n",
      "âœ… Best model saved at episode 8752 | Avg Reward: 0.3000\n",
      "Episode 9000 | Avg Reward: 0.0500\n",
      "Episode 9500 | Avg Reward: 0.0700\n",
      "Episode 10000 | Avg Reward: 0.1400\n",
      "âœ… Best model saved at episode 10205 | Avg Reward: 0.3300\n",
      "Episode 10500 | Avg Reward: 0.1800\n",
      "Episode 11000 | Avg Reward: -0.0500\n",
      "Episode 11500 | Avg Reward: -0.0900\n",
      "Episode 12000 | Avg Reward: -0.0300\n",
      "âœ… Best model saved at episode 12077 | Avg Reward: 0.3500\n",
      "âœ… Best model saved at episode 12079 | Avg Reward: 0.3600\n",
      "âœ… Best model saved at episode 12103 | Avg Reward: 0.3800\n",
      "âœ… Best model saved at episode 12106 | Avg Reward: 0.3900\n",
      "âœ… Best model saved at episode 12107 | Avg Reward: 0.4000\n",
      "Episode 12500 | Avg Reward: 0.0600\n",
      "Episode 13000 | Avg Reward: 0.1600\n",
      "Episode 13500 | Avg Reward: 0.0700\n",
      "Episode 14000 | Avg Reward: 0.1300\n",
      "Episode 14500 | Avg Reward: 0.0500\n",
      "Episode 15000 | Avg Reward: 0.0800\n",
      "Episode 15500 | Avg Reward: 0.1800\n",
      "âœ… Best model saved at episode 15948 | Avg Reward: 0.4300\n",
      "âœ… Best model saved at episode 15987 | Avg Reward: 0.4400\n",
      "Episode 16000 | Avg Reward: 0.3200\n",
      "Episode 16500 | Avg Reward: 0.2100\n",
      "Episode 17000 | Avg Reward: -0.1100\n",
      "Episode 17500 | Avg Reward: -0.0900\n",
      "Episode 18000 | Avg Reward: 0.0800\n",
      "Episode 18500 | Avg Reward: -0.1200\n",
      "Episode 19000 | Avg Reward: 0.3200\n",
      "Episode 19500 | Avg Reward: 0.1400\n",
      "Episode 20000 | Avg Reward: 0.2100\n",
      "Episode 20500 | Avg Reward: 0.0600\n",
      "Episode 21000 | Avg Reward: 0.1400\n",
      "Episode 21500 | Avg Reward: 0.2200\n",
      "Episode 22000 | Avg Reward: 0.2400\n",
      "Episode 22500 | Avg Reward: 0.1500\n",
      "Episode 23000 | Avg Reward: 0.0500\n",
      "âœ… Best model saved at episode 23441 | Avg Reward: 0.4500\n",
      "âœ… Best model saved at episode 23447 | Avg Reward: 0.4600\n",
      "âœ… Best model saved at episode 23448 | Avg Reward: 0.4700\n",
      "âœ… Best model saved at episode 23449 | Avg Reward: 0.4900\n",
      "âœ… Best model saved at episode 23451 | Avg Reward: 0.5000\n",
      "Episode 23500 | Avg Reward: 0.3200\n",
      "Episode 24000 | Avg Reward: -0.0600\n",
      "Episode 24500 | Avg Reward: 0.2600\n",
      "Episode 25000 | Avg Reward: 0.3300\n",
      "Episode 25500 | Avg Reward: 0.0200\n",
      "Episode 26000 | Avg Reward: 0.3700\n",
      "Episode 26500 | Avg Reward: 0.1000\n",
      "Episode 27000 | Avg Reward: 0.1400\n",
      "Episode 27500 | Avg Reward: 0.1700\n",
      "Episode 28000 | Avg Reward: 0.1100\n",
      "Episode 28500 | Avg Reward: 0.1300\n",
      "Episode 29000 | Avg Reward: 0.2800\n",
      "Episode 29500 | Avg Reward: 0.0500\n",
      "Episode 30000 | Avg Reward: 0.1500\n",
      "Episode 30500 | Avg Reward: 0.1300\n",
      "Episode 31000 | Avg Reward: 0.0700\n",
      "Episode 31500 | Avg Reward: 0.1100\n",
      "âœ… Best model saved at episode 31606 | Avg Reward: 0.5100\n",
      "âœ… Best model saved at episode 31607 | Avg Reward: 0.5300\n",
      "âœ… Best model saved at episode 31619 | Avg Reward: 0.5400\n",
      "âœ… Best model saved at episode 31622 | Avg Reward: 0.5500\n",
      "âœ… Best model saved at episode 31623 | Avg Reward: 0.5700\n",
      "âœ… Best model saved at episode 31629 | Avg Reward: 0.5800\n",
      "âœ… Best model saved at episode 31633 | Avg Reward: 0.5900\n",
      "âœ… Best model saved at episode 31637 | Avg Reward: 0.6100\n",
      "âœ… Best model saved at episode 31640 | Avg Reward: 0.6200\n",
      "Episode 32000 | Avg Reward: 0.0200\n",
      "Episode 32500 | Avg Reward: 0.3100\n",
      "Episode 33000 | Avg Reward: 0.3400\n",
      "Episode 33500 | Avg Reward: 0.2300\n",
      "Episode 34000 | Avg Reward: 0.4100\n",
      "Episode 34500 | Avg Reward: 0.1400\n",
      "Episode 35000 | Avg Reward: -0.1500\n",
      "Episode 35500 | Avg Reward: 0.2500\n",
      "Episode 36000 | Avg Reward: 0.2800\n",
      "Episode 36500 | Avg Reward: 0.1300\n",
      "Episode 37000 | Avg Reward: 0.1700\n",
      "Episode 37500 | Avg Reward: 0.2600\n",
      "Episode 38000 | Avg Reward: 0.2600\n",
      "Episode 38500 | Avg Reward: 0.1400\n",
      "Episode 39000 | Avg Reward: 0.3000\n",
      "Episode 39500 | Avg Reward: 0.3600\n",
      "Episode 40000 | Avg Reward: 0.2000\n",
      "Episode 40500 | Avg Reward: 0.3500\n",
      "Episode 41000 | Avg Reward: 0.1600\n",
      "Episode 41500 | Avg Reward: 0.2800\n",
      "Episode 42000 | Avg Reward: 0.2200\n",
      "Episode 42500 | Avg Reward: 0.3200\n",
      "Episode 43000 | Avg Reward: 0.2900\n",
      "Episode 43500 | Avg Reward: 0.0200\n",
      "Episode 44000 | Avg Reward: 0.3400\n",
      "Episode 44500 | Avg Reward: 0.2500\n",
      "Episode 45000 | Avg Reward: 0.4000\n",
      "Episode 45500 | Avg Reward: 0.3900\n",
      "Episode 46000 | Avg Reward: 0.1900\n",
      "âœ… Best model saved at episode 46290 | Avg Reward: 0.6300\n",
      "âœ… Best model saved at episode 46291 | Avg Reward: 0.6400\n",
      "Episode 46500 | Avg Reward: 0.3000\n",
      "Episode 47000 | Avg Reward: 0.2700\n",
      "Episode 47500 | Avg Reward: 0.2600\n",
      "Episode 48000 | Avg Reward: 0.2900\n",
      "Episode 48500 | Avg Reward: 0.2700\n",
      "âœ… Best model saved at episode 48623 | Avg Reward: 0.6500\n",
      "âœ… Best model saved at episode 48624 | Avg Reward: 0.6700\n",
      "âœ… Best model saved at episode 48625 | Avg Reward: 0.6900\n",
      "Episode 49000 | Avg Reward: 0.2000\n",
      "Episode 49500 | Avg Reward: 0.4200\n",
      "Episode 50000 | Avg Reward: 0.0200\n",
      "Completed PPO training for 2 deck(s)\n",
      "\n",
      "=== Training PPO model for 3 deck(s) ===\n",
      "âœ… Best model saved at episode 100 | Avg Reward: -0.2700\n",
      "âœ… Best model saved at episode 101 | Avg Reward: -0.2300\n",
      "âœ… Best model saved at episode 102 | Avg Reward: -0.2000\n",
      "âœ… Best model saved at episode 107 | Avg Reward: -0.1900\n",
      "âœ… Best model saved at episode 109 | Avg Reward: -0.1800\n",
      "âœ… Best model saved at episode 110 | Avg Reward: -0.1600\n",
      "âœ… Best model saved at episode 128 | Avg Reward: -0.1500\n",
      "âœ… Best model saved at episode 129 | Avg Reward: -0.1300\n",
      "âœ… Best model saved at episode 131 | Avg Reward: -0.1000\n",
      "âœ… Best model saved at episode 132 | Avg Reward: -0.0900\n",
      "âœ… Best model saved at episode 138 | Avg Reward: -0.0800\n",
      "âœ… Best model saved at episode 139 | Avg Reward: -0.0600\n",
      "âœ… Best model saved at episode 143 | Avg Reward: -0.0200\n",
      "âœ… Best model saved at episode 154 | Avg Reward: -0.0100\n",
      "âœ… Best model saved at episode 156 | Avg Reward: 0.0200\n",
      "âœ… Best model saved at episode 157 | Avg Reward: 0.0400\n",
      "âœ… Best model saved at episode 159 | Avg Reward: 0.0500\n",
      "âœ… Best model saved at episode 225 | Avg Reward: 0.0600\n",
      "Episode 500 | Avg Reward: -0.2000\n",
      "âœ… Best model saved at episode 625 | Avg Reward: 0.0800\n",
      "âœ… Best model saved at episode 657 | Avg Reward: 0.0900\n",
      "âœ… Best model saved at episode 659 | Avg Reward: 0.1100\n",
      "âœ… Best model saved at episode 661 | Avg Reward: 0.1300\n",
      "âœ… Best model saved at episode 667 | Avg Reward: 0.1600\n",
      "âœ… Best model saved at episode 668 | Avg Reward: 0.1800\n",
      "âœ… Best model saved at episode 669 | Avg Reward: 0.1900\n",
      "âœ… Best model saved at episode 672 | Avg Reward: 0.2000\n",
      "âœ… Best model saved at episode 682 | Avg Reward: 0.2200\n",
      "âœ… Best model saved at episode 683 | Avg Reward: 0.2400\n",
      "âœ… Best model saved at episode 693 | Avg Reward: 0.2500\n",
      "Episode 1000 | Avg Reward: -0.0700\n",
      "Episode 1500 | Avg Reward: -0.0100\n",
      "Episode 2000 | Avg Reward: 0.0800\n",
      "Episode 2500 | Avg Reward: 0.2300\n",
      "âœ… Best model saved at episode 2509 | Avg Reward: 0.2700\n",
      "âœ… Best model saved at episode 2515 | Avg Reward: 0.2800\n",
      "Episode 3000 | Avg Reward: 0.0000\n",
      "Episode 3500 | Avg Reward: -0.0900\n",
      "Episode 4000 | Avg Reward: 0.1000\n",
      "Episode 4500 | Avg Reward: 0.2500\n",
      "Episode 5000 | Avg Reward: 0.1200\n",
      "Episode 5500 | Avg Reward: 0.1400\n",
      "âœ… Best model saved at episode 5624 | Avg Reward: 0.2900\n",
      "âœ… Best model saved at episode 5625 | Avg Reward: 0.3100\n",
      "âœ… Best model saved at episode 5635 | Avg Reward: 0.3200\n",
      "âœ… Best model saved at episode 5639 | Avg Reward: 0.3400\n",
      "Episode 6000 | Avg Reward: -0.1300\n",
      "Episode 6500 | Avg Reward: 0.1300\n",
      "Episode 7000 | Avg Reward: 0.0400\n",
      "Episode 7500 | Avg Reward: -0.0600\n",
      "Episode 8000 | Avg Reward: 0.1900\n",
      "Episode 8500 | Avg Reward: 0.0400\n",
      "Episode 9000 | Avg Reward: -0.0300\n",
      "Episode 9500 | Avg Reward: 0.1400\n",
      "âœ… Best model saved at episode 9762 | Avg Reward: 0.3600\n",
      "âœ… Best model saved at episode 9764 | Avg Reward: 0.3700\n",
      "âœ… Best model saved at episode 9766 | Avg Reward: 0.3800\n",
      "Episode 10000 | Avg Reward: 0.0100\n",
      "Episode 10500 | Avg Reward: 0.0800\n",
      "Episode 11000 | Avg Reward: 0.0400\n",
      "Episode 11500 | Avg Reward: 0.2000\n",
      "Episode 12000 | Avg Reward: 0.0300\n",
      "Episode 12500 | Avg Reward: 0.0000\n",
      "Episode 13000 | Avg Reward: 0.1000\n",
      "Episode 13500 | Avg Reward: 0.1200\n",
      "Episode 14000 | Avg Reward: 0.1200\n",
      "Episode 14500 | Avg Reward: 0.0000\n",
      "Episode 15000 | Avg Reward: -0.0200\n",
      "Episode 15500 | Avg Reward: 0.0400\n",
      "âœ… Best model saved at episode 15963 | Avg Reward: 0.4000\n",
      "âœ… Best model saved at episode 15983 | Avg Reward: 0.4100\n",
      "Episode 16000 | Avg Reward: 0.3100\n",
      "âœ… Best model saved at episode 16207 | Avg Reward: 0.4200\n",
      "Episode 16500 | Avg Reward: 0.0900\n",
      "Episode 17000 | Avg Reward: 0.1300\n",
      "Episode 17500 | Avg Reward: 0.1300\n",
      "Episode 18000 | Avg Reward: 0.0700\n",
      "Episode 18500 | Avg Reward: -0.0900\n",
      "Episode 19000 | Avg Reward: 0.2000\n",
      "Episode 19500 | Avg Reward: 0.1600\n",
      "Episode 20000 | Avg Reward: 0.0300\n",
      "Episode 20500 | Avg Reward: 0.1400\n",
      "Episode 21000 | Avg Reward: 0.1700\n",
      "Episode 21500 | Avg Reward: 0.2500\n",
      "Episode 22000 | Avg Reward: 0.1400\n",
      "Episode 22500 | Avg Reward: 0.0700\n",
      "Episode 23000 | Avg Reward: 0.3000\n",
      "Episode 23500 | Avg Reward: 0.1100\n",
      "âœ… Best model saved at episode 23860 | Avg Reward: 0.4300\n",
      "Episode 24000 | Avg Reward: 0.2200\n",
      "Episode 24500 | Avg Reward: 0.0300\n",
      "Episode 25000 | Avg Reward: 0.1300\n",
      "Episode 25500 | Avg Reward: 0.2000\n",
      "Episode 26000 | Avg Reward: 0.1800\n",
      "Episode 26500 | Avg Reward: 0.1700\n",
      "âœ… Best model saved at episode 26770 | Avg Reward: 0.4500\n",
      "âœ… Best model saved at episode 26771 | Avg Reward: 0.4700\n",
      "Episode 27000 | Avg Reward: 0.1800\n",
      "Episode 27500 | Avg Reward: 0.1400\n",
      "Episode 28000 | Avg Reward: 0.1900\n",
      "Episode 28500 | Avg Reward: 0.0000\n",
      "Episode 29000 | Avg Reward: 0.2400\n",
      "âœ… Best model saved at episode 29310 | Avg Reward: 0.5000\n",
      "âœ… Best model saved at episode 29311 | Avg Reward: 0.5100\n",
      "Episode 29500 | Avg Reward: 0.3800\n",
      "âœ… Best model saved at episode 29720 | Avg Reward: 0.5400\n",
      "Episode 30000 | Avg Reward: 0.2200\n",
      "Episode 30500 | Avg Reward: 0.2700\n",
      "Episode 31000 | Avg Reward: 0.0900\n",
      "âœ… Best model saved at episode 31422 | Avg Reward: 0.5500\n",
      "Episode 31500 | Avg Reward: 0.0900\n",
      "Episode 32000 | Avg Reward: 0.3000\n",
      "Episode 32500 | Avg Reward: 0.2500\n",
      "Episode 33000 | Avg Reward: 0.1700\n",
      "Episode 33500 | Avg Reward: 0.2500\n",
      "Episode 34000 | Avg Reward: 0.0700\n",
      "Episode 34500 | Avg Reward: 0.1300\n",
      "Episode 35000 | Avg Reward: 0.3400\n",
      "âœ… Best model saved at episode 35298 | Avg Reward: 0.5700\n",
      "âœ… Best model saved at episode 35300 | Avg Reward: 0.5800\n",
      "âœ… Best model saved at episode 35301 | Avg Reward: 0.6100\n",
      "âœ… Best model saved at episode 35307 | Avg Reward: 0.6200\n",
      "Episode 35500 | Avg Reward: 0.2900\n",
      "Episode 36000 | Avg Reward: 0.1300\n",
      "Episode 36500 | Avg Reward: 0.2900\n",
      "Episode 37000 | Avg Reward: 0.0900\n",
      "Episode 37500 | Avg Reward: 0.3300\n",
      "Episode 38000 | Avg Reward: 0.1900\n",
      "Episode 38500 | Avg Reward: 0.0900\n",
      "Episode 39000 | Avg Reward: 0.3100\n",
      "Episode 39500 | Avg Reward: 0.1600\n",
      "Episode 40000 | Avg Reward: 0.2100\n",
      "Episode 40500 | Avg Reward: 0.2100\n",
      "Episode 41000 | Avg Reward: 0.0700\n",
      "Episode 41500 | Avg Reward: 0.2500\n",
      "Episode 42000 | Avg Reward: 0.3100\n",
      "Episode 42500 | Avg Reward: 0.0200\n",
      "Episode 43000 | Avg Reward: 0.2400\n",
      "Episode 43500 | Avg Reward: 0.0500\n",
      "Episode 44000 | Avg Reward: 0.2500\n",
      "Episode 44500 | Avg Reward: -0.0900\n",
      "Episode 45000 | Avg Reward: 0.1700\n",
      "Episode 45500 | Avg Reward: 0.2000\n",
      "Episode 46000 | Avg Reward: 0.0900\n",
      "Episode 46500 | Avg Reward: 0.2600\n",
      "Episode 47000 | Avg Reward: 0.1000\n",
      "Episode 47500 | Avg Reward: 0.2200\n",
      "Episode 48000 | Avg Reward: 0.2100\n",
      "Episode 48500 | Avg Reward: -0.0200\n",
      "Episode 49000 | Avg Reward: 0.1400\n",
      "Episode 49500 | Avg Reward: 0.4600\n",
      "Episode 50000 | Avg Reward: 0.1500\n",
      "Completed PPO training for 3 deck(s)\n",
      "\n",
      "=== Training PPO model for 4 deck(s) ===\n",
      "âœ… Best model saved at episode 100 | Avg Reward: -0.5300\n",
      "âœ… Best model saved at episode 133 | Avg Reward: -0.5200\n",
      "âœ… Best model saved at episode 134 | Avg Reward: -0.4900\n",
      "âœ… Best model saved at episode 141 | Avg Reward: -0.4800\n",
      "âœ… Best model saved at episode 143 | Avg Reward: -0.4600\n",
      "âœ… Best model saved at episode 155 | Avg Reward: -0.4400\n",
      "âœ… Best model saved at episode 158 | Avg Reward: -0.4200\n",
      "âœ… Best model saved at episode 187 | Avg Reward: -0.3900\n",
      "âœ… Best model saved at episode 191 | Avg Reward: -0.3500\n",
      "âœ… Best model saved at episode 197 | Avg Reward: -0.3200\n",
      "âœ… Best model saved at episode 199 | Avg Reward: -0.2900\n",
      "âœ… Best model saved at episode 206 | Avg Reward: -0.2800\n",
      "âœ… Best model saved at episode 446 | Avg Reward: -0.2700\n",
      "âœ… Best model saved at episode 447 | Avg Reward: -0.2400\n",
      "âœ… Best model saved at episode 451 | Avg Reward: -0.2300\n",
      "âœ… Best model saved at episode 456 | Avg Reward: -0.2200\n",
      "âœ… Best model saved at episode 457 | Avg Reward: -0.1800\n",
      "âœ… Best model saved at episode 458 | Avg Reward: -0.1400\n",
      "âœ… Best model saved at episode 463 | Avg Reward: -0.1300\n",
      "âœ… Best model saved at episode 466 | Avg Reward: -0.1100\n",
      "âœ… Best model saved at episode 468 | Avg Reward: -0.1000\n",
      "âœ… Best model saved at episode 469 | Avg Reward: -0.0800\n",
      "âœ… Best model saved at episode 470 | Avg Reward: -0.0400\n",
      "âœ… Best model saved at episode 473 | Avg Reward: -0.0300\n",
      "Episode 500 | Avg Reward: -0.3300\n",
      "âœ… Best model saved at episode 593 | Avg Reward: -0.0200\n",
      "âœ… Best model saved at episode 598 | Avg Reward: -0.0100\n",
      "âœ… Best model saved at episode 599 | Avg Reward: 0.0100\n",
      "âœ… Best model saved at episode 601 | Avg Reward: 0.0400\n",
      "âœ… Best model saved at episode 602 | Avg Reward: 0.0600\n",
      "âœ… Best model saved at episode 671 | Avg Reward: 0.0800\n",
      "âœ… Best model saved at episode 672 | Avg Reward: 0.0900\n",
      "âœ… Best model saved at episode 674 | Avg Reward: 0.1100\n",
      "âœ… Best model saved at episode 675 | Avg Reward: 0.1200\n",
      "Episode 1000 | Avg Reward: -0.0700\n",
      "âœ… Best model saved at episode 1143 | Avg Reward: 0.1300\n",
      "âœ… Best model saved at episode 1144 | Avg Reward: 0.1500\n",
      "âœ… Best model saved at episode 1146 | Avg Reward: 0.1700\n",
      "âœ… Best model saved at episode 1147 | Avg Reward: 0.1800\n",
      "âœ… Best model saved at episode 1148 | Avg Reward: 0.2000\n",
      "Episode 1500 | Avg Reward: -0.0600\n",
      "âœ… Best model saved at episode 1644 | Avg Reward: 0.2100\n",
      "Episode 2000 | Avg Reward: 0.0100\n",
      "âœ… Best model saved at episode 2096 | Avg Reward: 0.2200\n",
      "âœ… Best model saved at episode 2100 | Avg Reward: 0.2300\n",
      "âœ… Best model saved at episode 2106 | Avg Reward: 0.2500\n",
      "âœ… Best model saved at episode 2107 | Avg Reward: 0.3000\n",
      "âœ… Best model saved at episode 2115 | Avg Reward: 0.3200\n",
      "Episode 2500 | Avg Reward: 0.0700\n",
      "Episode 3000 | Avg Reward: 0.2200\n",
      "Episode 3500 | Avg Reward: 0.1000\n",
      "Episode 4000 | Avg Reward: -0.0400\n",
      "Episode 4500 | Avg Reward: -0.0200\n",
      "Episode 5000 | Avg Reward: 0.1700\n",
      "Episode 5500 | Avg Reward: 0.0800\n",
      "Episode 6000 | Avg Reward: 0.1600\n",
      "Episode 6500 | Avg Reward: 0.1500\n",
      "âœ… Best model saved at episode 6654 | Avg Reward: 0.3400\n",
      "Episode 7000 | Avg Reward: -0.0200\n",
      "Episode 7500 | Avg Reward: 0.1800\n",
      "Episode 8000 | Avg Reward: 0.1200\n",
      "Episode 8500 | Avg Reward: 0.0100\n",
      "Episode 9000 | Avg Reward: 0.0900\n",
      "Episode 9500 | Avg Reward: 0.0400\n",
      "Episode 10000 | Avg Reward: 0.3200\n",
      "âœ… Best model saved at episode 10002 | Avg Reward: 0.3500\n",
      "âœ… Best model saved at episode 10225 | Avg Reward: 0.3600\n",
      "âœ… Best model saved at episode 10226 | Avg Reward: 0.3800\n",
      "âœ… Best model saved at episode 10227 | Avg Reward: 0.4000\n",
      "âœ… Best model saved at episode 10228 | Avg Reward: 0.4100\n",
      "Episode 10500 | Avg Reward: -0.0800\n",
      "Episode 11000 | Avg Reward: 0.0600\n",
      "Episode 11500 | Avg Reward: 0.1700\n",
      "Episode 12000 | Avg Reward: -0.2300\n",
      "Episode 12500 | Avg Reward: -0.0500\n",
      "Episode 13000 | Avg Reward: 0.1200\n",
      "Episode 13500 | Avg Reward: 0.0700\n",
      "Episode 14000 | Avg Reward: 0.1800\n",
      "Episode 14500 | Avg Reward: 0.0000\n",
      "Episode 15000 | Avg Reward: 0.1200\n",
      "Episode 15500 | Avg Reward: 0.1200\n",
      "Episode 16000 | Avg Reward: 0.1100\n",
      "Episode 16500 | Avg Reward: 0.1200\n",
      "Episode 17000 | Avg Reward: 0.1200\n",
      "âœ… Best model saved at episode 17430 | Avg Reward: 0.4200\n",
      "Episode 17500 | Avg Reward: 0.1600\n",
      "Episode 18000 | Avg Reward: 0.2100\n",
      "Episode 18500 | Avg Reward: 0.0900\n",
      "Episode 19000 | Avg Reward: 0.0400\n",
      "Episode 19500 | Avg Reward: 0.0700\n",
      "Episode 20000 | Avg Reward: 0.1800\n",
      "Episode 20500 | Avg Reward: 0.2000\n",
      "Episode 21000 | Avg Reward: 0.3000\n",
      "Episode 21500 | Avg Reward: 0.0500\n",
      "Episode 22000 | Avg Reward: -0.0600\n",
      "Episode 22500 | Avg Reward: 0.1500\n",
      "Episode 23000 | Avg Reward: -0.0300\n",
      "Episode 23500 | Avg Reward: 0.0900\n",
      "âœ… Best model saved at episode 23683 | Avg Reward: 0.4300\n",
      "âœ… Best model saved at episode 23699 | Avg Reward: 0.4500\n",
      "âœ… Best model saved at episode 23721 | Avg Reward: 0.4700\n",
      "Episode 24000 | Avg Reward: 0.1400\n",
      "âœ… Best model saved at episode 24477 | Avg Reward: 0.4800\n",
      "Episode 24500 | Avg Reward: 0.3600\n",
      "Episode 25000 | Avg Reward: 0.1900\n",
      "Episode 25500 | Avg Reward: 0.2200\n",
      "Episode 26000 | Avg Reward: 0.2300\n",
      "Episode 26500 | Avg Reward: 0.1600\n",
      "Episode 27000 | Avg Reward: 0.0500\n",
      "Episode 27500 | Avg Reward: 0.1000\n",
      "Episode 28000 | Avg Reward: -0.0300\n",
      "Episode 28500 | Avg Reward: 0.2400\n",
      "Episode 29000 | Avg Reward: -0.0600\n",
      "Episode 29500 | Avg Reward: 0.1300\n",
      "Episode 30000 | Avg Reward: 0.2800\n",
      "Episode 30500 | Avg Reward: 0.1600\n",
      "Episode 31000 | Avg Reward: 0.2200\n",
      "Episode 31500 | Avg Reward: 0.1700\n",
      "Episode 32000 | Avg Reward: 0.0600\n",
      "Episode 32500 | Avg Reward: 0.1100\n",
      "Episode 33000 | Avg Reward: 0.2500\n",
      "Episode 33500 | Avg Reward: 0.2100\n",
      "Episode 34000 | Avg Reward: 0.2500\n",
      "Episode 34500 | Avg Reward: 0.2200\n",
      "âœ… Best model saved at episode 34579 | Avg Reward: 0.5000\n",
      "Episode 35000 | Avg Reward: 0.2500\n",
      "Episode 35500 | Avg Reward: 0.1700\n",
      "Episode 36000 | Avg Reward: 0.1500\n",
      "Episode 36500 | Avg Reward: 0.3900\n",
      "Episode 37000 | Avg Reward: 0.3400\n",
      "Episode 37500 | Avg Reward: 0.2700\n",
      "Episode 38000 | Avg Reward: 0.2300\n",
      "Episode 38500 | Avg Reward: 0.3200\n",
      "Episode 39000 | Avg Reward: 0.2600\n",
      "Episode 39500 | Avg Reward: 0.3900\n",
      "Episode 40000 | Avg Reward: 0.3200\n",
      "Episode 40500 | Avg Reward: 0.2900\n",
      "Episode 41000 | Avg Reward: 0.3100\n",
      "Episode 41500 | Avg Reward: 0.3500\n",
      "Episode 42000 | Avg Reward: 0.3600\n",
      "âœ… Best model saved at episode 42210 | Avg Reward: 0.5100\n",
      "âœ… Best model saved at episode 42222 | Avg Reward: 0.5300\n",
      "âœ… Best model saved at episode 42223 | Avg Reward: 0.5500\n",
      "Episode 42500 | Avg Reward: 0.3300\n",
      "Episode 43000 | Avg Reward: 0.2700\n",
      "Episode 43500 | Avg Reward: 0.2100\n",
      "Episode 44000 | Avg Reward: 0.2200\n",
      "Episode 44500 | Avg Reward: 0.1500\n",
      "Episode 45000 | Avg Reward: 0.0400\n",
      "Episode 45500 | Avg Reward: 0.3500\n",
      "Episode 46000 | Avg Reward: 0.1200\n",
      "âœ… Best model saved at episode 46239 | Avg Reward: 0.5700\n",
      "âœ… Best model saved at episode 46240 | Avg Reward: 0.5900\n",
      "Episode 46500 | Avg Reward: 0.1900\n",
      "Episode 47000 | Avg Reward: 0.2400\n",
      "Episode 47500 | Avg Reward: 0.2800\n",
      "Episode 48000 | Avg Reward: 0.1700\n",
      "Episode 48500 | Avg Reward: 0.1800\n",
      "Episode 49000 | Avg Reward: 0.2800\n",
      "Episode 49500 | Avg Reward: 0.2900\n",
      "Episode 50000 | Avg Reward: 0.1900\n",
      "Completed PPO training for 4 deck(s)\n",
      "\n",
      "=== Training PPO model for 5 deck(s) ===\n",
      "âœ… Best model saved at episode 100 | Avg Reward: -0.6400\n",
      "âœ… Best model saved at episode 102 | Avg Reward: -0.6300\n",
      "âœ… Best model saved at episode 104 | Avg Reward: -0.5800\n",
      "âœ… Best model saved at episode 105 | Avg Reward: -0.5700\n",
      "âœ… Best model saved at episode 172 | Avg Reward: -0.5600\n",
      "âœ… Best model saved at episode 174 | Avg Reward: -0.5500\n",
      "âœ… Best model saved at episode 175 | Avg Reward: -0.5300\n",
      "âœ… Best model saved at episode 192 | Avg Reward: -0.5200\n",
      "âœ… Best model saved at episode 193 | Avg Reward: -0.5100\n",
      "âœ… Best model saved at episode 199 | Avg Reward: -0.4800\n",
      "âœ… Best model saved at episode 202 | Avg Reward: -0.4700\n",
      "âœ… Best model saved at episode 210 | Avg Reward: -0.4600\n",
      "âœ… Best model saved at episode 321 | Avg Reward: -0.4200\n",
      "âœ… Best model saved at episode 322 | Avg Reward: -0.4000\n",
      "âœ… Best model saved at episode 489 | Avg Reward: -0.3900\n",
      "âœ… Best model saved at episode 490 | Avg Reward: -0.3600\n",
      "âœ… Best model saved at episode 493 | Avg Reward: -0.3400\n",
      "âœ… Best model saved at episode 495 | Avg Reward: -0.3300\n",
      "âœ… Best model saved at episode 499 | Avg Reward: -0.3100\n",
      "Episode 500 | Avg Reward: -0.3400\n",
      "âœ… Best model saved at episode 511 | Avg Reward: -0.2900\n",
      "âœ… Best model saved at episode 517 | Avg Reward: -0.2700\n",
      "âœ… Best model saved at episode 519 | Avg Reward: -0.2500\n",
      "âœ… Best model saved at episode 523 | Avg Reward: -0.2400\n",
      "âœ… Best model saved at episode 524 | Avg Reward: -0.2200\n",
      "âœ… Best model saved at episode 530 | Avg Reward: -0.2000\n",
      "âœ… Best model saved at episode 556 | Avg Reward: -0.1700\n",
      "âœ… Best model saved at episode 557 | Avg Reward: -0.1600\n",
      "âœ… Best model saved at episode 566 | Avg Reward: -0.1400\n",
      "âœ… Best model saved at episode 567 | Avg Reward: -0.1200\n",
      "âœ… Best model saved at episode 568 | Avg Reward: -0.1000\n",
      "âœ… Best model saved at episode 570 | Avg Reward: -0.0900\n",
      "âœ… Best model saved at episode 571 | Avg Reward: -0.0700\n",
      "âœ… Best model saved at episode 704 | Avg Reward: -0.0600\n",
      "âœ… Best model saved at episode 705 | Avg Reward: -0.0200\n",
      "âœ… Best model saved at episode 711 | Avg Reward: 0.0000\n",
      "âœ… Best model saved at episode 737 | Avg Reward: 0.0100\n",
      "âœ… Best model saved at episode 738 | Avg Reward: 0.0300\n",
      "âœ… Best model saved at episode 745 | Avg Reward: 0.0400\n",
      "âœ… Best model saved at episode 759 | Avg Reward: 0.0500\n",
      "âœ… Best model saved at episode 769 | Avg Reward: 0.0600\n",
      "âœ… Best model saved at episode 770 | Avg Reward: 0.0900\n",
      "âœ… Best model saved at episode 772 | Avg Reward: 0.1000\n",
      "âœ… Best model saved at episode 773 | Avg Reward: 0.1300\n",
      "âœ… Best model saved at episode 777 | Avg Reward: 0.1700\n",
      "Episode 1000 | Avg Reward: -0.0300\n",
      "Episode 1500 | Avg Reward: -0.1100\n",
      "âœ… Best model saved at episode 1991 | Avg Reward: 0.1800\n",
      "âœ… Best model saved at episode 1992 | Avg Reward: 0.2000\n",
      "Episode 2000 | Avg Reward: 0.1800\n",
      "âœ… Best model saved at episode 2037 | Avg Reward: 0.2200\n",
      "âœ… Best model saved at episode 2040 | Avg Reward: 0.2300\n",
      "âœ… Best model saved at episode 2123 | Avg Reward: 0.2400\n",
      "âœ… Best model saved at episode 2177 | Avg Reward: 0.2500\n",
      "âœ… Best model saved at episode 2178 | Avg Reward: 0.2700\n",
      "Episode 2500 | Avg Reward: 0.0100\n",
      "Episode 3000 | Avg Reward: -0.1000\n",
      "Episode 3500 | Avg Reward: 0.0900\n",
      "Episode 4000 | Avg Reward: 0.0100\n",
      "Episode 4500 | Avg Reward: -0.0300\n",
      "âœ… Best model saved at episode 4952 | Avg Reward: 0.2800\n",
      "âœ… Best model saved at episode 4955 | Avg Reward: 0.3000\n",
      "Episode 5000 | Avg Reward: 0.1600\n",
      "âœ… Best model saved at episode 5150 | Avg Reward: 0.3200\n",
      "âœ… Best model saved at episode 5153 | Avg Reward: 0.3300\n",
      "âœ… Best model saved at episode 5154 | Avg Reward: 0.3500\n",
      "âœ… Best model saved at episode 5157 | Avg Reward: 0.3600\n",
      "âœ… Best model saved at episode 5163 | Avg Reward: 0.3800\n",
      "Episode 5500 | Avg Reward: -0.0100\n",
      "Episode 6000 | Avg Reward: 0.3100\n",
      "Episode 6500 | Avg Reward: 0.1300\n",
      "Episode 7000 | Avg Reward: 0.1300\n",
      "Episode 7500 | Avg Reward: 0.1100\n",
      "Episode 8000 | Avg Reward: 0.0600\n",
      "Episode 8500 | Avg Reward: 0.0100\n",
      "Episode 9000 | Avg Reward: -0.1100\n",
      "Episode 9500 | Avg Reward: 0.0400\n",
      "Episode 10000 | Avg Reward: 0.2600\n",
      "âœ… Best model saved at episode 10394 | Avg Reward: 0.3900\n",
      "âœ… Best model saved at episode 10396 | Avg Reward: 0.4000\n",
      "âœ… Best model saved at episode 10397 | Avg Reward: 0.4100\n",
      "âœ… Best model saved at episode 10403 | Avg Reward: 0.4300\n",
      "âœ… Best model saved at episode 10412 | Avg Reward: 0.4500\n",
      "âœ… Best model saved at episode 10414 | Avg Reward: 0.4700\n",
      "âœ… Best model saved at episode 10416 | Avg Reward: 0.4900\n",
      "âœ… Best model saved at episode 10417 | Avg Reward: 0.5000\n",
      "âœ… Best model saved at episode 10420 | Avg Reward: 0.5200\n",
      "âœ… Best model saved at episode 10421 | Avg Reward: 0.5300\n",
      "âœ… Best model saved at episode 10422 | Avg Reward: 0.5500\n",
      "âœ… Best model saved at episode 10424 | Avg Reward: 0.5600\n",
      "âœ… Best model saved at episode 10425 | Avg Reward: 0.5800\n",
      "Episode 10500 | Avg Reward: 0.3400\n",
      "Episode 11000 | Avg Reward: -0.0600\n",
      "Episode 11500 | Avg Reward: 0.3500\n",
      "Episode 12000 | Avg Reward: 0.0800\n",
      "Episode 12500 | Avg Reward: 0.2000\n",
      "Episode 13000 | Avg Reward: -0.1200\n",
      "Episode 13500 | Avg Reward: 0.2900\n",
      "Episode 14000 | Avg Reward: 0.1700\n",
      "Episode 14500 | Avg Reward: 0.1300\n",
      "Episode 15000 | Avg Reward: 0.0100\n",
      "Episode 15500 | Avg Reward: 0.1400\n",
      "Episode 16000 | Avg Reward: 0.0900\n",
      "Episode 16500 | Avg Reward: 0.0900\n",
      "Episode 17000 | Avg Reward: 0.0500\n",
      "Episode 17500 | Avg Reward: 0.2100\n",
      "Episode 18000 | Avg Reward: 0.1700\n",
      "Episode 18500 | Avg Reward: 0.1100\n",
      "Episode 19000 | Avg Reward: 0.1300\n",
      "Episode 19500 | Avg Reward: 0.1100\n",
      "Episode 20000 | Avg Reward: 0.1600\n",
      "Episode 20500 | Avg Reward: 0.1700\n",
      "Episode 21000 | Avg Reward: 0.1500\n",
      "Episode 21500 | Avg Reward: 0.2900\n",
      "Episode 22000 | Avg Reward: 0.2900\n",
      "Episode 22500 | Avg Reward: 0.1500\n",
      "Episode 23000 | Avg Reward: 0.2000\n",
      "Episode 23500 | Avg Reward: 0.3000\n",
      "Episode 24000 | Avg Reward: 0.2300\n",
      "Episode 24500 | Avg Reward: 0.1800\n",
      "Episode 25000 | Avg Reward: -0.0400\n",
      "Episode 25500 | Avg Reward: 0.2100\n",
      "Episode 26000 | Avg Reward: 0.2800\n",
      "Episode 26500 | Avg Reward: 0.1800\n",
      "Episode 27000 | Avg Reward: -0.0100\n",
      "Episode 27500 | Avg Reward: 0.1500\n",
      "Episode 28000 | Avg Reward: 0.1800\n",
      "Episode 28500 | Avg Reward: 0.1800\n",
      "Episode 29000 | Avg Reward: 0.2400\n",
      "Episode 29500 | Avg Reward: 0.0400\n",
      "Episode 30000 | Avg Reward: 0.2300\n",
      "Episode 30500 | Avg Reward: 0.3600\n",
      "Episode 31000 | Avg Reward: 0.0900\n",
      "Episode 31500 | Avg Reward: 0.1600\n",
      "Episode 32000 | Avg Reward: 0.2000\n",
      "Episode 32500 | Avg Reward: 0.1200\n",
      "Episode 33000 | Avg Reward: 0.0200\n",
      "Episode 33500 | Avg Reward: 0.2400\n",
      "Episode 34000 | Avg Reward: 0.2800\n",
      "Episode 34500 | Avg Reward: 0.1800\n",
      "Episode 35000 | Avg Reward: -0.0600\n",
      "Episode 35500 | Avg Reward: 0.1500\n",
      "Episode 36000 | Avg Reward: -0.0200\n",
      "Episode 36500 | Avg Reward: 0.1400\n",
      "Episode 37000 | Avg Reward: 0.2200\n",
      "Episode 37500 | Avg Reward: 0.3700\n",
      "Episode 38000 | Avg Reward: 0.0500\n",
      "Episode 38500 | Avg Reward: 0.3800\n",
      "Episode 39000 | Avg Reward: 0.2700\n",
      "Episode 39500 | Avg Reward: 0.1300\n",
      "Episode 40000 | Avg Reward: 0.1200\n",
      "Episode 40500 | Avg Reward: 0.0200\n",
      "Episode 41000 | Avg Reward: 0.2200\n",
      "Episode 41500 | Avg Reward: 0.3900\n",
      "Episode 42000 | Avg Reward: 0.2800\n",
      "Episode 42500 | Avg Reward: 0.1800\n",
      "Episode 43000 | Avg Reward: 0.2000\n",
      "Episode 43500 | Avg Reward: 0.4300\n",
      "Episode 44000 | Avg Reward: 0.2800\n",
      "Episode 44500 | Avg Reward: 0.3900\n",
      "Episode 45000 | Avg Reward: 0.1800\n",
      "Episode 45500 | Avg Reward: 0.2400\n",
      "Episode 46000 | Avg Reward: 0.3200\n",
      "Episode 46500 | Avg Reward: 0.3100\n",
      "Episode 47000 | Avg Reward: 0.2300\n",
      "Episode 47500 | Avg Reward: 0.2100\n",
      "âœ… Best model saved at episode 47681 | Avg Reward: 0.5900\n",
      "âœ… Best model saved at episode 47817 | Avg Reward: 0.6000\n",
      "âœ… Best model saved at episode 47819 | Avg Reward: 0.6300\n",
      "âœ… Best model saved at episode 47847 | Avg Reward: 0.6400\n",
      "âœ… Best model saved at episode 47848 | Avg Reward: 0.6800\n",
      "âœ… Best model saved at episode 47851 | Avg Reward: 0.7000\n",
      "âœ… Best model saved at episode 47855 | Avg Reward: 0.7100\n",
      "âœ… Best model saved at episode 47856 | Avg Reward: 0.7300\n",
      "âœ… Best model saved at episode 47857 | Avg Reward: 0.7500\n",
      "Episode 48000 | Avg Reward: 0.2900\n",
      "Episode 48500 | Avg Reward: 0.1500\n",
      "Episode 49000 | Avg Reward: 0.1000\n",
      "Episode 49500 | Avg Reward: 0.0800\n",
      "Episode 50000 | Avg Reward: 0.2300\n",
      "Completed PPO training for 5 deck(s)\n",
      "\n",
      "=== Training PPO model for 6 deck(s) ===\n",
      "âœ… Best model saved at episode 100 | Avg Reward: 0.0300\n",
      "âœ… Best model saved at episode 262 | Avg Reward: 0.0400\n",
      "âœ… Best model saved at episode 284 | Avg Reward: 0.0600\n",
      "âœ… Best model saved at episode 311 | Avg Reward: 0.0800\n",
      "âœ… Best model saved at episode 313 | Avg Reward: 0.0900\n",
      "âœ… Best model saved at episode 406 | Avg Reward: 0.1100\n",
      "âœ… Best model saved at episode 408 | Avg Reward: 0.1300\n",
      "âœ… Best model saved at episode 410 | Avg Reward: 0.1400\n",
      "âœ… Best model saved at episode 418 | Avg Reward: 0.1500\n",
      "Episode 500 | Avg Reward: -0.1300\n",
      "âœ… Best model saved at episode 695 | Avg Reward: 0.1600\n",
      "âœ… Best model saved at episode 696 | Avg Reward: 0.1800\n",
      "âœ… Best model saved at episode 697 | Avg Reward: 0.2000\n",
      "âœ… Best model saved at episode 698 | Avg Reward: 0.2100\n",
      "âœ… Best model saved at episode 705 | Avg Reward: 0.2300\n",
      "âœ… Best model saved at episode 709 | Avg Reward: 0.2400\n",
      "âœ… Best model saved at episode 712 | Avg Reward: 0.2600\n",
      "âœ… Best model saved at episode 713 | Avg Reward: 0.3000\n",
      "âœ… Best model saved at episode 722 | Avg Reward: 0.3100\n",
      "âœ… Best model saved at episode 724 | Avg Reward: 0.3300\n",
      "Episode 1000 | Avg Reward: 0.0800\n",
      "Episode 1500 | Avg Reward: 0.1700\n",
      "Episode 2000 | Avg Reward: 0.0200\n",
      "Episode 2500 | Avg Reward: 0.0500\n",
      "Episode 3000 | Avg Reward: 0.0100\n",
      "Episode 3500 | Avg Reward: 0.0900\n",
      "Episode 4000 | Avg Reward: 0.1300\n",
      "Episode 4500 | Avg Reward: 0.0700\n",
      "Episode 5000 | Avg Reward: 0.0300\n",
      "âœ… Best model saved at episode 5266 | Avg Reward: 0.3400\n",
      "âœ… Best model saved at episode 5267 | Avg Reward: 0.3700\n",
      "âœ… Best model saved at episode 5270 | Avg Reward: 0.3900\n",
      "âœ… Best model saved at episode 5271 | Avg Reward: 0.4100\n",
      "âœ… Best model saved at episode 5274 | Avg Reward: 0.4200\n",
      "Episode 5500 | Avg Reward: -0.0100\n",
      "Episode 6000 | Avg Reward: 0.1200\n",
      "Episode 6500 | Avg Reward: -0.0700\n",
      "Episode 7000 | Avg Reward: -0.0900\n",
      "Episode 7500 | Avg Reward: -0.0700\n",
      "Episode 8000 | Avg Reward: 0.3500\n",
      "Episode 8500 | Avg Reward: 0.1100\n",
      "Episode 9000 | Avg Reward: 0.1500\n",
      "Episode 9500 | Avg Reward: 0.0100\n",
      "Episode 10000 | Avg Reward: 0.1500\n",
      "Episode 10500 | Avg Reward: 0.0600\n",
      "Episode 11000 | Avg Reward: 0.1100\n",
      "Episode 11500 | Avg Reward: -0.0300\n",
      "Episode 12000 | Avg Reward: -0.0300\n",
      "Episode 12500 | Avg Reward: 0.1700\n",
      "Episode 13000 | Avg Reward: 0.0900\n",
      "Episode 13500 | Avg Reward: -0.0500\n",
      "Episode 14000 | Avg Reward: 0.1800\n",
      "Episode 14500 | Avg Reward: 0.1100\n",
      "Episode 15000 | Avg Reward: 0.3100\n",
      "Episode 15500 | Avg Reward: 0.1000\n",
      "Episode 16000 | Avg Reward: 0.2300\n",
      "Episode 16500 | Avg Reward: 0.0800\n",
      "Episode 17000 | Avg Reward: 0.0300\n",
      "Episode 17500 | Avg Reward: -0.0700\n",
      "Episode 18000 | Avg Reward: 0.0400\n",
      "Episode 18500 | Avg Reward: -0.0800\n",
      "âœ… Best model saved at episode 18991 | Avg Reward: 0.4300\n",
      "âœ… Best model saved at episode 18997 | Avg Reward: 0.4400\n",
      "âœ… Best model saved at episode 18999 | Avg Reward: 0.4500\n",
      "Episode 19000 | Avg Reward: 0.4500\n",
      "âœ… Best model saved at episode 19018 | Avg Reward: 0.4600\n",
      "âœ… Best model saved at episode 19062 | Avg Reward: 0.4700\n",
      "Episode 19500 | Avg Reward: 0.0700\n",
      "Episode 20000 | Avg Reward: 0.2800\n",
      "Episode 20500 | Avg Reward: 0.0200\n",
      "Episode 21000 | Avg Reward: 0.1400\n",
      "Episode 21500 | Avg Reward: 0.0000\n",
      "Episode 22000 | Avg Reward: 0.2900\n",
      "Episode 22500 | Avg Reward: 0.0200\n",
      "Episode 23000 | Avg Reward: -0.1400\n",
      "Episode 23500 | Avg Reward: 0.0300\n",
      "Episode 24000 | Avg Reward: 0.2200\n",
      "Episode 24500 | Avg Reward: 0.0600\n",
      "Episode 25000 | Avg Reward: 0.1700\n",
      "Episode 25500 | Avg Reward: 0.1300\n",
      "Episode 26000 | Avg Reward: -0.0100\n",
      "Episode 26500 | Avg Reward: 0.2600\n",
      "Episode 27000 | Avg Reward: 0.2300\n",
      "Episode 27500 | Avg Reward: 0.0200\n",
      "Episode 28000 | Avg Reward: 0.1200\n",
      "Episode 28500 | Avg Reward: 0.0900\n",
      "Episode 29000 | Avg Reward: 0.1100\n",
      "Episode 29500 | Avg Reward: 0.0900\n",
      "âœ… Best model saved at episode 29780 | Avg Reward: 0.4900\n",
      "âœ… Best model saved at episode 29781 | Avg Reward: 0.5200\n",
      "âœ… Best model saved at episode 29783 | Avg Reward: 0.5500\n",
      "âœ… Best model saved at episode 29790 | Avg Reward: 0.5700\n",
      "âœ… Best model saved at episode 29792 | Avg Reward: 0.5900\n",
      "âœ… Best model saved at episode 29793 | Avg Reward: 0.6100\n",
      "âœ… Best model saved at episode 29801 | Avg Reward: 0.6300\n",
      "âœ… Best model saved at episode 29802 | Avg Reward: 0.6600\n",
      "Episode 30000 | Avg Reward: -0.0300\n",
      "Episode 30500 | Avg Reward: 0.2800\n",
      "Episode 31000 | Avg Reward: 0.0300\n",
      "Episode 31500 | Avg Reward: 0.1700\n",
      "Episode 32000 | Avg Reward: 0.1200\n",
      "Episode 32500 | Avg Reward: 0.1400\n",
      "Episode 33000 | Avg Reward: 0.1500\n",
      "Episode 33500 | Avg Reward: 0.1300\n",
      "Episode 34000 | Avg Reward: 0.2600\n",
      "Episode 34500 | Avg Reward: -0.0300\n",
      "Episode 35000 | Avg Reward: 0.3100\n",
      "Episode 35500 | Avg Reward: 0.3400\n",
      "Episode 36000 | Avg Reward: -0.0500\n",
      "Episode 36500 | Avg Reward: 0.2000\n",
      "Episode 37000 | Avg Reward: 0.2400\n",
      "Episode 37500 | Avg Reward: 0.1900\n",
      "Episode 38000 | Avg Reward: 0.2300\n",
      "Episode 38500 | Avg Reward: 0.3800\n",
      "Episode 39000 | Avg Reward: 0.2700\n",
      "Episode 39500 | Avg Reward: 0.1700\n",
      "Episode 40000 | Avg Reward: 0.2200\n",
      "Episode 40500 | Avg Reward: -0.0200\n",
      "Episode 41000 | Avg Reward: 0.2100\n",
      "Episode 41500 | Avg Reward: 0.3300\n",
      "Episode 42000 | Avg Reward: 0.2300\n",
      "Episode 42500 | Avg Reward: -0.0100\n",
      "Episode 43000 | Avg Reward: 0.0900\n",
      "Episode 43500 | Avg Reward: 0.3600\n",
      "Episode 44000 | Avg Reward: 0.0900\n",
      "Episode 44500 | Avg Reward: -0.1500\n",
      "Episode 45000 | Avg Reward: 0.2100\n",
      "Episode 45500 | Avg Reward: 0.2900\n",
      "Episode 46000 | Avg Reward: 0.1900\n",
      "Episode 46500 | Avg Reward: 0.1200\n",
      "Episode 47000 | Avg Reward: 0.1800\n",
      "Episode 47500 | Avg Reward: 0.0100\n",
      "Episode 48000 | Avg Reward: 0.1100\n",
      "Episode 48500 | Avg Reward: 0.3700\n",
      "Episode 49000 | Avg Reward: 0.3700\n",
      "Episode 49500 | Avg Reward: 0.2000\n",
      "Episode 50000 | Avg Reward: 0.1900\n",
      "Completed PPO training for 6 deck(s)\n",
      "All PPO models trained successfully!\n"
     ]
    }
   ],
   "source": [
    "# Train models for different deck counts\n",
    "ppo_models = {}\n",
    "\n",
    "for num_decks in range(1, 7):\n",
    "    print(f\"\\n=== Training PPO model for {num_decks} deck(s) ===\")\n",
    "    env = BlackjackEnv(numdecks=num_decks, natural=False)\n",
    "    model_save_path = f\"blackjack_ppo_decks_{num_decks}.pth\"\n",
    "    ppo_model = train_ppo(env, n_episodes=50000, model_save_path=model_save_path)\n",
    "    ppo_models[num_decks] = ppo_model\n",
    "    print(f\"Completed PPO training for {num_decks} deck(s)\")\n",
    "\n",
    "print(\"All PPO models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaaacf6",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66dc1c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# === PPO Evaluation on Deck Sizes ===\n",
    "def evaluate_ppo_on_deck_sizes(models, num_games=10000, max_decks=6):\n",
    "    results = []\n",
    "\n",
    "    for num_deck in range(1, max_decks + 1):\n",
    "        env = BlackjackEnv(numdecks=num_deck, natural=False)\n",
    "        policy_net = models[num_deck]  # Get the specific model for this deck size\n",
    "\n",
    "        wins = 0\n",
    "        losses = 0\n",
    "        draws = 0\n",
    "        total_reward = 0\n",
    "        count_natural_player = 0\n",
    "        count_natural_dealer = 0\n",
    "\n",
    "        for game in range(num_games):\n",
    "            obs = env.reset(seed=game)\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                # Process state to match training format\n",
    "                state = preprocess_state(obs)\n",
    "                \n",
    "                try:\n",
    "                    with torch.no_grad():\n",
    "                        state_tensor = torch.FloatTensor(state)\n",
    "                        logits, _ = policy_net(state_tensor)\n",
    "                        \n",
    "                        # Apply action masking\n",
    "                        masked_logits = select_valid_action(logits, obs, env)\n",
    "                        probs = torch.softmax(masked_logits, dim=-1)\n",
    "                        \n",
    "                        # For evaluation, choose the action with highest probability\n",
    "                        action = torch.argmax(probs).item()\n",
    "                    \n",
    "                    next_obs, reward, done, _ = env.step(action)\n",
    "                    episode_reward += reward\n",
    "                    obs = next_obs\n",
    "                    \n",
    "                    # Check for naturals when game ends\n",
    "                    if done:\n",
    "                        if hasattr(env, 'hands') and len(env.hands) > 0:\n",
    "                            if is_natural(env.hands[0]):\n",
    "                                count_natural_player += 1\n",
    "                        \n",
    "                        if hasattr(env, 'dealer'):\n",
    "                            if is_natural(env.dealer):\n",
    "                                count_natural_dealer += 1\n",
    "                    \n",
    "                except ValueError:\n",
    "                    # If error, try a fallback action\n",
    "                    action = 0  # Stick is usually safe\n",
    "                    next_obs, reward, done, _ = env.step(action)\n",
    "                    episode_reward += reward\n",
    "                    obs = next_obs\n",
    "\n",
    "            total_reward += episode_reward\n",
    "            if episode_reward > 0:\n",
    "                wins += 1\n",
    "            elif episode_reward < 0:\n",
    "                losses += 1\n",
    "            else:\n",
    "                draws += 1\n",
    "\n",
    "        # Store results\n",
    "        results.append({\n",
    "            \"Decks\": num_deck,\n",
    "            \"Games\": num_games,\n",
    "            \"Wins\": wins,\n",
    "            \"Draws\": draws,\n",
    "            \"Losses\": losses,\n",
    "            \"Total Reward\": round(total_reward, 4),\n",
    "            \"Win Rate (%)\": round((wins / num_games) * 100, 4),\n",
    "            \"Loss Rate (%)\": round((losses / num_games) * 100, 4),\n",
    "            \"Draw Rate (%)\": round((draws / num_games) * 100, 4),\n",
    "            \"Average Reward\": round(total_reward / num_games, 4),\n",
    "            \"Natural Player\": count_natural_player,\n",
    "            \"Natural Dealer\": count_natural_dealer\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def evaluate_ppo_bankroll(models, num_games=10000, max_decks=6, initial_money=100):\n",
    "    \"\"\"\n",
    "    Evaluate PPO models with a bankroll simulation across different deck sizes\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for num_deck in range(1, max_decks + 1):\n",
    "        env = BlackjackEnv(numdecks=num_deck, natural=False)\n",
    "        policy_net = models[num_deck]  # Get the specific model for this deck size\n",
    "        policy_net.eval()  # Set the model to evaluation mode\n",
    "\n",
    "        money = initial_money\n",
    "        wins = 0\n",
    "        losses = 0\n",
    "        draws = 0\n",
    "        total_reward = 0\n",
    "        count_natural_player = 0\n",
    "        count_natural_dealer = 0\n",
    "        \n",
    "        # For tracking bankruptcy\n",
    "        games_played = 0\n",
    "        went_bankrupt = False\n",
    "\n",
    "        for game in range(1, num_games+1):\n",
    "            if money <= 0:\n",
    "                went_bankrupt = True\n",
    "                games_played = game - 1\n",
    "                break\n",
    "                \n",
    "            games_played = game\n",
    "            obs = env.reset(seed=game)\n",
    "            done = False\n",
    "            \n",
    "            # Bet $1\n",
    "            money -= 1\n",
    "            episode_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                # Process state to match training format\n",
    "                state = preprocess_state(obs)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    state_tensor = torch.FloatTensor(state)\n",
    "                    logits, _ = policy_net(state_tensor)\n",
    "                    \n",
    "                    # Get valid actions for the current state\n",
    "                    player_cards, dealer_card, usable_ace, can_double = obs\n",
    "                    valid_actions = [0, 1]  # Stick, Hit are always valid\n",
    "                    \n",
    "                    if can_double:\n",
    "                        valid_actions.append(2)  # Double down if allowed\n",
    "                    \n",
    "                    # Check for split (same rank cards)\n",
    "                    if hasattr(env, 'current_hand') and hasattr(env, 'hands'):\n",
    "                        if env.current_hand < len(env.hands):\n",
    "                            current_hand = env.hands[env.current_hand]\n",
    "                            if len(current_hand) == 2 and card_value(current_hand[0]) == card_value(current_hand[1]):\n",
    "                                valid_actions.append(3)  # Split if allowed\n",
    "                    \n",
    "                    # Mask invalid actions\n",
    "                    masked_logits = logits.clone()\n",
    "                    for i in range(len(masked_logits)):\n",
    "                        if i not in valid_actions:\n",
    "                            masked_logits[i] = float('-inf')\n",
    "                    \n",
    "                    probs = torch.softmax(masked_logits, dim=-1)\n",
    "                    action = torch.argmax(probs).item()\n",
    "                    \n",
    "                    \n",
    "                \n",
    "                # Check if action is valid (safeguard)\n",
    "                if action not in valid_actions:\n",
    "                    action = 0  # Default to stick if somehow invalid\n",
    "                \n",
    "                # If doubling down, subtract another dollar\n",
    "                if action == 2:  # Double down\n",
    "                    money -= 1\n",
    "                \n",
    "                # Execute the action\n",
    "                try:\n",
    "                    next_obs, reward, done, _ = env.step(action)\n",
    "                    episode_reward += reward\n",
    "                    obs = next_obs\n",
    "                    \n",
    "                    # Check for naturals when game ends\n",
    "                    if done:\n",
    "                        if hasattr(env, 'hands') and len(env.hands) > 0:\n",
    "                            if is_natural(env.hands[0]):\n",
    "                                count_natural_player += 1\n",
    "                        \n",
    "                        if hasattr(env, 'dealer'):\n",
    "                            if is_natural(env.dealer):\n",
    "                                count_natural_dealer += 1\n",
    "                    \n",
    "                except ValueError:\n",
    "                    # Fallback if error\n",
    "                    action = 0  # Stick\n",
    "                    next_obs, reward, done, _ = env.step(action)\n",
    "                    episode_reward += reward\n",
    "                    obs = next_obs\n",
    "\n",
    "            # End of episode accounting\n",
    "            total_reward += episode_reward\n",
    "            \n",
    "            if episode_reward > 0:\n",
    "                wins += 1\n",
    "                # Calculate payout based on bet\n",
    "                if action == 2:  # Double down win\n",
    "                    money += 4  # Get 2x the doubled bet\n",
    "                else:\n",
    "                    money += 2  # Regular win\n",
    "            elif episode_reward < 0:\n",
    "                losses += 1\n",
    "                # Money already subtracted for bet\n",
    "            else:\n",
    "                draws += 1\n",
    "                if action == 2:  # Double down push\n",
    "                    money += 2  # Get doubled bet back\n",
    "                else:\n",
    "                    money += 1  # Get original bet back\n",
    "\n",
    "        # Store results\n",
    "        bankruptcy_message = f\"Bankrupt after {games_played} games\" if went_bankrupt else \"Solvent\"\n",
    "        \n",
    "        results.append({\n",
    "            \"Decks\": num_deck,\n",
    "            \"Games\": games_played,\n",
    "            \"Wins\": wins,\n",
    "            \"Draws\": draws,\n",
    "            \"Losses\": losses,\n",
    "            \"Total Reward\": round(total_reward, 4),\n",
    "            \"Win Rate (%)\": round((wins / games_played) * 100, 4) if games_played > 0 else 0,\n",
    "            \"Loss Rate (%)\": round((losses / games_played) * 100, 4) if games_played > 0 else 0,\n",
    "            \"Draw Rate (%)\": round((draws / games_played) * 100, 4) if games_played > 0 else 0,\n",
    "            \"Average Reward\": round(total_reward / games_played, 4) if games_played > 0 else 0,\n",
    "            \"Final Money\": money,\n",
    "            \"Natural Player\": count_natural_player,\n",
    "            \"Natural Dealer\": count_natural_dealer,\n",
    "            \"Status\": bankruptcy_message\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "630721a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PPO model for 1 deck(s)...\n",
      "âœ… Successfully loaded model for 1 deck(s) from blackjack_ppo_decks_1.pth\n",
      "   Average reward during training: 0.5400\n",
      "   Saved at episode: 40702\n",
      "Loading PPO model for 2 deck(s)...\n",
      "âœ… Successfully loaded model for 2 deck(s) from blackjack_ppo_decks_2.pth\n",
      "   Average reward during training: 0.6900\n",
      "   Saved at episode: 48625\n",
      "Loading PPO model for 3 deck(s)...\n",
      "âœ… Successfully loaded model for 3 deck(s) from blackjack_ppo_decks_3.pth\n",
      "   Average reward during training: 0.6200\n",
      "   Saved at episode: 35307\n",
      "Loading PPO model for 4 deck(s)...\n",
      "âœ… Successfully loaded model for 4 deck(s) from blackjack_ppo_decks_4.pth\n",
      "   Average reward during training: 0.5900\n",
      "   Saved at episode: 46240\n",
      "Loading PPO model for 5 deck(s)...\n",
      "âœ… Successfully loaded model for 5 deck(s) from blackjack_ppo_decks_5.pth\n",
      "   Average reward during training: 0.7500\n",
      "   Saved at episode: 47857\n",
      "Loading PPO model for 6 deck(s)...\n",
      "âœ… Successfully loaded model for 6 deck(s) from blackjack_ppo_decks_6.pth\n",
      "   Average reward during training: 0.6600\n",
      "   Saved at episode: 29802\n",
      "\n",
      "All models loaded. Running evaluation...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Decks</th>\n",
       "      <th>Games</th>\n",
       "      <th>Wins</th>\n",
       "      <th>Draws</th>\n",
       "      <th>Losses</th>\n",
       "      <th>Total Reward</th>\n",
       "      <th>Win Rate (%)</th>\n",
       "      <th>Loss Rate (%)</th>\n",
       "      <th>Draw Rate (%)</th>\n",
       "      <th>Average Reward</th>\n",
       "      <th>Natural Player</th>\n",
       "      <th>Natural Dealer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>6076</td>\n",
       "      <td>638</td>\n",
       "      <td>3286</td>\n",
       "      <td>3342.0</td>\n",
       "      <td>60.76</td>\n",
       "      <td>32.86</td>\n",
       "      <td>6.38</td>\n",
       "      <td>0.3342</td>\n",
       "      <td>495</td>\n",
       "      <td>461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>10000</td>\n",
       "      <td>5898</td>\n",
       "      <td>729</td>\n",
       "      <td>3373</td>\n",
       "      <td>3293.0</td>\n",
       "      <td>58.98</td>\n",
       "      <td>33.73</td>\n",
       "      <td>7.29</td>\n",
       "      <td>0.3293</td>\n",
       "      <td>349</td>\n",
       "      <td>454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>10000</td>\n",
       "      <td>5914</td>\n",
       "      <td>752</td>\n",
       "      <td>3334</td>\n",
       "      <td>2889.0</td>\n",
       "      <td>59.14</td>\n",
       "      <td>33.34</td>\n",
       "      <td>7.52</td>\n",
       "      <td>0.2889</td>\n",
       "      <td>459</td>\n",
       "      <td>509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>10000</td>\n",
       "      <td>5946</td>\n",
       "      <td>747</td>\n",
       "      <td>3307</td>\n",
       "      <td>3138.0</td>\n",
       "      <td>59.46</td>\n",
       "      <td>33.07</td>\n",
       "      <td>7.47</td>\n",
       "      <td>0.3138</td>\n",
       "      <td>471</td>\n",
       "      <td>447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>10000</td>\n",
       "      <td>5855</td>\n",
       "      <td>793</td>\n",
       "      <td>3352</td>\n",
       "      <td>3284.0</td>\n",
       "      <td>58.55</td>\n",
       "      <td>33.52</td>\n",
       "      <td>7.93</td>\n",
       "      <td>0.3284</td>\n",
       "      <td>234</td>\n",
       "      <td>482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>10000</td>\n",
       "      <td>5656</td>\n",
       "      <td>774</td>\n",
       "      <td>3570</td>\n",
       "      <td>2056.0</td>\n",
       "      <td>56.56</td>\n",
       "      <td>35.70</td>\n",
       "      <td>7.74</td>\n",
       "      <td>0.2056</td>\n",
       "      <td>504</td>\n",
       "      <td>473</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Decks  Games  Wins  Draws  Losses  Total Reward  Win Rate (%)  \\\n",
       "0      1  10000  6076    638    3286        3342.0         60.76   \n",
       "1      2  10000  5898    729    3373        3293.0         58.98   \n",
       "2      3  10000  5914    752    3334        2889.0         59.14   \n",
       "3      4  10000  5946    747    3307        3138.0         59.46   \n",
       "4      5  10000  5855    793    3352        3284.0         58.55   \n",
       "5      6  10000  5656    774    3570        2056.0         56.56   \n",
       "\n",
       "   Loss Rate (%)  Draw Rate (%)  Average Reward  Natural Player  \\\n",
       "0          32.86           6.38          0.3342             495   \n",
       "1          33.73           7.29          0.3293             349   \n",
       "2          33.34           7.52          0.2889             459   \n",
       "3          33.07           7.47          0.3138             471   \n",
       "4          33.52           7.93          0.3284             234   \n",
       "5          35.70           7.74          0.2056             504   \n",
       "\n",
       "   Natural Dealer  \n",
       "0             461  \n",
       "1             454  \n",
       "2             509  \n",
       "3             447  \n",
       "4             482  \n",
       "5             473  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# Load the saved PPO models\n",
    "ppo_models = {}\n",
    "\n",
    "for num_decks in range(1, 7):\n",
    "    print(f\"Loading PPO model for {num_decks} deck(s)...\")\n",
    "    model_path = f\"blackjack_ppo_decks_{num_decks}.pth\"\n",
    "    \n",
    "    # Check if the model file exists\n",
    "    if os.path.exists(model_path):\n",
    "        # Create a new model instance with the correct architecture\n",
    "        input_dim = 4  # [player_sum, dealer_card, usable_ace, can_double]\n",
    "        output_dim = 4  # [stick, hit, double, split]\n",
    "        model = PPOActorCritic(input_dim, output_dim)\n",
    "        \n",
    "        # Load the saved weights\n",
    "        checkpoint = torch.load(model_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        # Set to evaluation mode\n",
    "        model.eval()\n",
    "        \n",
    "        # Store in the models dictionary\n",
    "        ppo_models[num_decks] = model\n",
    "        \n",
    "        print(f\"âœ… Successfully loaded model for {num_decks} deck(s) from {model_path}\")\n",
    "        print(f\"   Average reward during training: {checkpoint['avg_reward']:.4f}\")\n",
    "        print(f\"   Saved at episode: {checkpoint['episode']}\")\n",
    "    else:\n",
    "        print(f\"âŒ Model file not found: {model_path}\")\n",
    "        print(f\"   Creating a new untrained model for {num_decks} deck(s)\")\n",
    "        \n",
    "        # Create an untrained model as fallback\n",
    "        input_dim = 4\n",
    "        output_dim = 4\n",
    "        model = PPOActorCritic(input_dim, output_dim)\n",
    "        ppo_models[num_decks] = model\n",
    "\n",
    "print(\"\\nAll models loaded. Running evaluation...\")\n",
    "\n",
    "# Run the evaluation\n",
    "df_ppo = evaluate_ppo_on_deck_sizes(ppo_models, num_games=10000, max_decks=6)\n",
    "\n",
    "# Display the results\n",
    "df_ppo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bd75ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Decks</th>\n",
       "      <th>Games</th>\n",
       "      <th>Wins</th>\n",
       "      <th>Draws</th>\n",
       "      <th>Losses</th>\n",
       "      <th>Total Reward</th>\n",
       "      <th>Win Rate (%)</th>\n",
       "      <th>Loss Rate (%)</th>\n",
       "      <th>Draw Rate (%)</th>\n",
       "      <th>Average Reward</th>\n",
       "      <th>Final Money</th>\n",
       "      <th>Natural Player</th>\n",
       "      <th>Natural Dealer</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>6077</td>\n",
       "      <td>641</td>\n",
       "      <td>3282</td>\n",
       "      <td>3349.0</td>\n",
       "      <td>60.77</td>\n",
       "      <td>32.82</td>\n",
       "      <td>6.41</td>\n",
       "      <td>0.3349</td>\n",
       "      <td>3640</td>\n",
       "      <td>495</td>\n",
       "      <td>460</td>\n",
       "      <td>Solvent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>10000</td>\n",
       "      <td>5877</td>\n",
       "      <td>737</td>\n",
       "      <td>3386</td>\n",
       "      <td>3261.0</td>\n",
       "      <td>58.77</td>\n",
       "      <td>33.86</td>\n",
       "      <td>7.37</td>\n",
       "      <td>0.3261</td>\n",
       "      <td>3471</td>\n",
       "      <td>342</td>\n",
       "      <td>453</td>\n",
       "      <td>Solvent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>10000</td>\n",
       "      <td>5917</td>\n",
       "      <td>754</td>\n",
       "      <td>3329</td>\n",
       "      <td>2897.0</td>\n",
       "      <td>59.17</td>\n",
       "      <td>33.29</td>\n",
       "      <td>7.54</td>\n",
       "      <td>0.2897</td>\n",
       "      <td>3039</td>\n",
       "      <td>459</td>\n",
       "      <td>506</td>\n",
       "      <td>Solvent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>10000</td>\n",
       "      <td>5933</td>\n",
       "      <td>749</td>\n",
       "      <td>3318</td>\n",
       "      <td>3123.0</td>\n",
       "      <td>59.33</td>\n",
       "      <td>33.18</td>\n",
       "      <td>7.49</td>\n",
       "      <td>0.3123</td>\n",
       "      <td>3267</td>\n",
       "      <td>466</td>\n",
       "      <td>446</td>\n",
       "      <td>Solvent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>10000</td>\n",
       "      <td>5966</td>\n",
       "      <td>711</td>\n",
       "      <td>3323</td>\n",
       "      <td>3375.0</td>\n",
       "      <td>59.66</td>\n",
       "      <td>33.23</td>\n",
       "      <td>7.11</td>\n",
       "      <td>0.3375</td>\n",
       "      <td>3715</td>\n",
       "      <td>234</td>\n",
       "      <td>498</td>\n",
       "      <td>Solvent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>10000</td>\n",
       "      <td>5666</td>\n",
       "      <td>769</td>\n",
       "      <td>3565</td>\n",
       "      <td>2082.0</td>\n",
       "      <td>56.66</td>\n",
       "      <td>35.65</td>\n",
       "      <td>7.69</td>\n",
       "      <td>0.2082</td>\n",
       "      <td>2446</td>\n",
       "      <td>509</td>\n",
       "      <td>473</td>\n",
       "      <td>Solvent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Decks  Games  Wins  Draws  Losses  Total Reward  Win Rate (%)  \\\n",
       "0      1  10000  6077    641    3282        3349.0         60.77   \n",
       "1      2  10000  5877    737    3386        3261.0         58.77   \n",
       "2      3  10000  5917    754    3329        2897.0         59.17   \n",
       "3      4  10000  5933    749    3318        3123.0         59.33   \n",
       "4      5  10000  5966    711    3323        3375.0         59.66   \n",
       "5      6  10000  5666    769    3565        2082.0         56.66   \n",
       "\n",
       "   Loss Rate (%)  Draw Rate (%)  Average Reward  Final Money  Natural Player  \\\n",
       "0          32.82           6.41          0.3349         3640             495   \n",
       "1          33.86           7.37          0.3261         3471             342   \n",
       "2          33.29           7.54          0.2897         3039             459   \n",
       "3          33.18           7.49          0.3123         3267             466   \n",
       "4          33.23           7.11          0.3375         3715             234   \n",
       "5          35.65           7.69          0.2082         2446             509   \n",
       "\n",
       "   Natural Dealer   Status  \n",
       "0             460  Solvent  \n",
       "1             453  Solvent  \n",
       "2             506  Solvent  \n",
       "3             446  Solvent  \n",
       "4             498  Solvent  \n",
       "5             473  Solvent  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ppo_bankroll = evaluate_ppo_bankroll(ppo_models, num_games=10000, max_decks=6, initial_money=100)\n",
    "df_ppo_bankroll"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study_venvs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
