{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "567f0958",
   "metadata": {},
   "source": [
    "# Set Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "071c5f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "import random\n",
    "\n",
    "# Full deck with distinct face cards\n",
    "CARDS = [1, 2, 3, 4, 5, 6, 7, 8, 9, '10', 'J', 'Q', 'K'] * 4\n",
    "\n",
    "def card_value(card):\n",
    "    return 10 if card in ['10', 'J', 'Q', 'K'] else card\n",
    "\n",
    "def draw_card(deck):\n",
    "    return deck.pop()\n",
    "\n",
    "def draw_hand(deck):\n",
    "    return [draw_card(deck), draw_card(deck)]\n",
    "\n",
    "def usable_ace(hand):\n",
    "    return 1 in hand and sum(card_value(c) for c in hand) + 10 <= 21\n",
    "\n",
    "def sum_hand(hand):\n",
    "    total = sum(card_value(c) for c in hand)\n",
    "    return total + 10 if usable_ace(hand) else total\n",
    "\n",
    "def is_bust(hand):\n",
    "    return sum_hand(hand) > 21\n",
    "\n",
    "def score(hand):\n",
    "    return 0 if is_bust(hand) else sum_hand(hand)\n",
    "\n",
    "def is_natural(hand):\n",
    "    return set(hand) == {1, '10'} or set(hand) == {1, 'J'} or set(hand) == {1, 'Q'} or set(hand) == {1, 'K'}\n",
    "\n",
    "def can_double_down(hand, actionstaken):\n",
    "    return len(hand) == 2 and actionstaken == 0\n",
    "\n",
    "class BlackjackEnv(gym.Env):\n",
    "    metadata = {\"render.modes\": [\"human\"]}\n",
    "\n",
    "    def __init__(self, numdecks=4, natural=True):\n",
    "        super().__init__()\n",
    "        self.action_space = spaces.Discrete(4)  # 0: Stick, 1: Hit, 2: Double Down, 3: Split\n",
    "        self.observation_space = spaces.Tuple((\n",
    "            spaces.Tuple((spaces.Discrete(32), spaces.Discrete(32))),  # Player hand (2 cards)\n",
    "            spaces.Discrete(11),  # Dealer's showing card\n",
    "            spaces.Discrete(2),   # Usable ace\n",
    "            spaces.Discrete(2)    # Can double down\n",
    "        ))\n",
    "\n",
    "        self.natural = natural\n",
    "        self.numdecks = numdecks\n",
    "        self.decks = CARDS * self.numdecks\n",
    "        random.shuffle(self.decks)\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        random.seed(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        if seed is not None:\n",
    "            self.seed(seed)\n",
    "\n",
    "        if self._deck_is_out():\n",
    "            self.decks = CARDS * self.numdecks\n",
    "            random.shuffle(self.decks)\n",
    "\n",
    "        self.dealer = draw_hand(self.decks)\n",
    "        first_hand = draw_hand(self.decks)\n",
    "        self.hands = [first_hand]\n",
    "        self.current_hand = 0\n",
    "        self.actionstaken = 0\n",
    "        self.hand_results = []\n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action), f\"Invalid action: {action}\"\n",
    "        if self._deck_is_out():\n",
    "            self.decks = CARDS * self.numdecks\n",
    "            random.shuffle(self.decks)\n",
    "\n",
    "        done = False\n",
    "        reward = 0\n",
    "        hand = self.hands[self.current_hand]\n",
    "\n",
    "        if action == 0:  # Stick\n",
    "            self._finalize_current_hand()\n",
    "\n",
    "        elif action == 1:  # Hit\n",
    "            hand.append(draw_card(self.decks))\n",
    "            if is_bust(hand):\n",
    "                self.hand_results.append(-1)\n",
    "                self._advance_hand()\n",
    "\n",
    "        elif action == 2:  # Double Down\n",
    "            if not can_double_down(hand, self.actionstaken):\n",
    "                raise ValueError(\"Invalid double down attempt.\")\n",
    "            hand.append(draw_card(self.decks))\n",
    "            if is_bust(hand):\n",
    "                self.hand_results.append(-2)\n",
    "            else:\n",
    "                self._finalize_current_hand(double=True)\n",
    "\n",
    "        elif action == 3:  # Split\n",
    "            if len(hand) != 2 or hand[0] != hand[1]:\n",
    "                raise ValueError(\"Invalid split attempt.\")\n",
    "            card = hand[0]\n",
    "            self.hands[self.current_hand] = [card, draw_card(self.decks)]\n",
    "            self.hands.insert(self.current_hand + 1, [card, draw_card(self.decks)])\n",
    "\n",
    "        self.actionstaken += 1\n",
    "\n",
    "        if self.current_hand >= len(self.hands):\n",
    "            while sum_hand(self.dealer) < 17:\n",
    "                self.dealer.append(draw_card(self.decks))\n",
    "\n",
    "            if len(self.hand_results) < len(self.hands):\n",
    "                self._finalize_current_hand()\n",
    "\n",
    "            reward = sum(self.hand_results)\n",
    "            done = True\n",
    "        \n",
    "        return self._get_obs(), reward, done, {}\n",
    "\n",
    "    def _finalize_current_hand(self, double=False):\n",
    "        hand = self.hands[self.current_hand]\n",
    "        player_score = score(hand)\n",
    "        dealer_score = score(self.dealer)\n",
    "        result = float(player_score > dealer_score) - float(player_score < dealer_score)\n",
    "        if is_natural(hand) and result == 1 and self.natural:\n",
    "            result = 1.5\n",
    "        self.hand_results.append(result * (2 if double else 1))\n",
    "        self._advance_hand()\n",
    "\n",
    "    def _advance_hand(self):\n",
    "        self.current_hand += 1\n",
    "        self.actionstaken = 0\n",
    "\n",
    "    def _get_obs(self):\n",
    "        if self.current_hand >= len(self.hands):\n",
    "            return ((0, 0), self.dealer[0], 0, 0)\n",
    "\n",
    "        hand = self.hands[self.current_hand]\n",
    "        padded = hand[:2] + [0] * (2 - len(hand))\n",
    "        return (\n",
    "            tuple(card_value(c) if c != 0 else 0 for c in padded[:2]),\n",
    "            card_value(self.dealer[0]),\n",
    "            usable_ace(hand),\n",
    "            can_double_down(hand, self.actionstaken)\n",
    "        )\n",
    "\n",
    "    def _deck_is_out(self):\n",
    "        return len(self.decks) < self.numdecks * len(CARDS) * 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5901ee",
   "metadata": {},
   "source": [
    "# Set the Simple DQN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf99e25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import copy\n",
    "import os\n",
    "\n",
    "# Define the Q-network\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "def preprocess_state(state):\n",
    "    \"\"\"\n",
    "    Converts the BlackjackEnv state to a format usable by the neural network\n",
    "    State format: ((card1, card2), dealer_card, usable_ace, can_double)\n",
    "    \"\"\"\n",
    "    player_cards, dealer_card, usable_ace, can_double = state\n",
    "    \n",
    "    # Convert dealer_card using card_value function\n",
    "    dealer_value = card_value(dealer_card)\n",
    "    \n",
    "    # For player cards tuple, calculate sum using card_value\n",
    "    player_sum = 0\n",
    "    for card in player_cards:\n",
    "        if card != 0:  # Skip zero values (padding)\n",
    "            player_sum += card_value(card)\n",
    "    \n",
    "    return np.array([player_sum, dealer_value, usable_ace, can_double], dtype=np.float32)\n",
    "\n",
    "# Epsilon-greedy action selection with valid action masking\n",
    "def select_action(state, q_network, epsilon, env):\n",
    "    # Extract state components to check for valid actions\n",
    "    player_cards, dealer_card, usable_ace, can_double = state\n",
    "    \n",
    "    # Default valid actions (stick and hit)\n",
    "    valid_actions = [0, 1]  \n",
    "    \n",
    "    # Check if can double down\n",
    "    if can_double:\n",
    "        valid_actions.append(2)\n",
    "    \n",
    "    # Check if the current hand allows split\n",
    "    if env.current_hand < len(env.hands):\n",
    "        current_hand = env.hands[env.current_hand]\n",
    "        # Check if can split (same card value and exactly 2 cards)\n",
    "        if len(current_hand) == 2 and card_value(current_hand[0]) == card_value(current_hand[1]):\n",
    "            valid_actions.append(3)\n",
    "    \n",
    "    # Epsilon-greedy action selection\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice(valid_actions)  # Choose randomly from valid actions\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(preprocess_state(state)).unsqueeze(0)\n",
    "            q_values = q_network(state_tensor)\n",
    "            \n",
    "            # Mask invalid actions by setting their Q-values to -inf\n",
    "            for action in range(env.action_space.n):\n",
    "                if action not in valid_actions:\n",
    "                    q_values[0, action] = float('-inf')\n",
    "            \n",
    "            return q_values.argmax().item()\n",
    "\n",
    "def train_dqn(env, n_episodes=5000, gamma=0.99, lr=1e-3, batch_size=64,\n",
    "              epsilon_start=1.0, epsilon_end=0.1, epsilon_decay=0.995,\n",
    "              model_save_path='best_blackjack_dqn.pth'):\n",
    "\n",
    "    input_dim = 4  # [player_sum, dealer_card, usable_ace, can_double]\n",
    "    output_dim = env.action_space.n\n",
    "\n",
    "    q_network = QNetwork(input_dim, output_dim)\n",
    "    optimizer = optim.Adam(q_network.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    replay_buffer = deque(maxlen=10000)\n",
    "    epsilon = epsilon_start\n",
    "    losses = []\n",
    "\n",
    "    best_model = None\n",
    "    best_avg_reward = float('-inf')\n",
    "    reward_window = deque(maxlen=100)\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = select_action(state, q_network, epsilon, env)\n",
    "            \n",
    "            try:\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                episode_reward += reward\n",
    "                \n",
    "                replay_buffer.append((state, action, reward, next_state, done))\n",
    "                state = next_state\n",
    "                \n",
    "            except ValueError:\n",
    "                # If invalid action is selected, try a different valid action\n",
    "                valid_actions = [0, 1]  # Default to stick or hit\n",
    "                action = random.choice(valid_actions)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                episode_reward += reward\n",
    "                \n",
    "                replay_buffer.append((state, action, reward, next_state, done))\n",
    "                state = next_state\n",
    "\n",
    "            # Only train if we have enough samples\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                batch = random.sample(replay_buffer, batch_size)\n",
    "                states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "                # Preprocess states and next_states\n",
    "                processed_states = [preprocess_state(s) for s in states]\n",
    "                processed_next_states = [preprocess_state(s) for s in next_states]\n",
    "\n",
    "                states_tensor = torch.FloatTensor(np.array(processed_states))\n",
    "                actions_tensor = torch.LongTensor(actions).unsqueeze(1)\n",
    "                rewards_tensor = torch.FloatTensor(rewards).unsqueeze(1)\n",
    "                next_states_tensor = torch.FloatTensor(np.array(processed_next_states))\n",
    "                dones_tensor = torch.BoolTensor(dones).unsqueeze(1)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    next_q_values = q_network(next_states_tensor).max(1, keepdim=True)[0]\n",
    "                    targets = rewards_tensor + gamma * next_q_values * (~dones_tensor)\n",
    "\n",
    "                q_values = q_network(states_tensor).gather(1, actions_tensor)\n",
    "\n",
    "                loss = loss_fn(q_values, targets)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                losses.append(loss.item())\n",
    "\n",
    "        # Track rewards for model saving\n",
    "        reward_window.append(episode_reward)\n",
    "        \n",
    "        # Decay epsilon\n",
    "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "\n",
    "        # Save best model based on average reward\n",
    "        if len(reward_window) == 100:\n",
    "            avg_reward = np.mean(reward_window)\n",
    "            if avg_reward > best_avg_reward:\n",
    "                best_avg_reward = avg_reward\n",
    "                best_model = copy.deepcopy(q_network)\n",
    "                torch.save({\n",
    "                    'model_state_dict': best_model.state_dict(),\n",
    "                    'avg_reward': best_avg_reward,\n",
    "                    'episode': episode + 1\n",
    "                }, model_save_path)\n",
    "                # print(f\"✅ Best model saved at episode {episode+1} | Avg Reward: {best_avg_reward:.4f}\")\n",
    "\n",
    "        if (episode + 1) % 10000 == 0:\n",
    "            print(f\"Episode {episode+1} | Epsilon: {epsilon:.4f} | Avg Reward: {np.mean(list(reward_window)):.4f}\")\n",
    "\n",
    "    return best_model if best_model else q_network, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b18a3743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training model for 1 deck(s) ===\n",
      "Episode 10000 | Epsilon: 0.1000 | Avg Reward: -0.1400\n",
      "Completed training for 1 deck(s)\n",
      "\n",
      "=== Training model for 2 deck(s) ===\n",
      "Episode 10000 | Epsilon: 0.1000 | Avg Reward: -0.8800\n",
      "Completed training for 2 deck(s)\n",
      "\n",
      "=== Training model for 3 deck(s) ===\n",
      "Episode 10000 | Epsilon: 0.1000 | Avg Reward: -1.1800\n",
      "Completed training for 3 deck(s)\n",
      "\n",
      "=== Training model for 4 deck(s) ===\n",
      "Episode 10000 | Epsilon: 0.1000 | Avg Reward: -1.0900\n",
      "Completed training for 4 deck(s)\n",
      "\n",
      "=== Training model for 5 deck(s) ===\n",
      "Episode 10000 | Epsilon: 0.1000 | Avg Reward: -0.9100\n",
      "Completed training for 5 deck(s)\n",
      "\n",
      "=== Training model for 6 deck(s) ===\n",
      "Episode 10000 | Epsilon: 0.1000 | Avg Reward: -1.2800\n",
      "Completed training for 6 deck(s)\n",
      "All models trained successfully!\n"
     ]
    }
   ],
   "source": [
    "# Train models for different deck counts\n",
    "models = {}\n",
    "\n",
    "for num_decks in range(1, 7):\n",
    "    print(f\"\\n=== Training model for {num_decks} deck(s) ===\")\n",
    "    env = BlackjackEnv(numdecks=num_decks, natural=True)\n",
    "    model_save_path = f\"blackjack_dqn_decks_{num_decks}.pth\"\n",
    "    model, _ = train_dqn(env, n_episodes=10000, model_save_path=model_save_path)\n",
    "    models[num_decks] = model\n",
    "    print(f\"Completed training for {num_decks} deck(s)\")\n",
    "\n",
    "print(\"All models trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b19d0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DQN model for 1 deck(s)...\n",
      "✅ Successfully loaded model for 1 deck(s) from blackjack_dqn_decks_1.pth\n",
      "   Average reward during training: 0.6250\n",
      "   Saved at episode: 1442\n",
      "Loading DQN model for 2 deck(s)...\n",
      "✅ Successfully loaded model for 2 deck(s) from blackjack_dqn_decks_2.pth\n",
      "   Average reward during training: 0.0050\n",
      "   Saved at episode: 373\n",
      "Loading DQN model for 3 deck(s)...\n",
      "✅ Successfully loaded model for 3 deck(s) from blackjack_dqn_decks_3.pth\n",
      "   Average reward during training: 0.3000\n",
      "   Saved at episode: 839\n",
      "Loading DQN model for 4 deck(s)...\n",
      "✅ Successfully loaded model for 4 deck(s) from blackjack_dqn_decks_4.pth\n",
      "   Average reward during training: -0.4700\n",
      "   Saved at episode: 212\n",
      "Loading DQN model for 5 deck(s)...\n",
      "✅ Successfully loaded model for 5 deck(s) from blackjack_dqn_decks_5.pth\n",
      "   Average reward during training: -0.2800\n",
      "   Saved at episode: 140\n",
      "Loading DQN model for 6 deck(s)...\n",
      "✅ Successfully loaded model for 6 deck(s) from blackjack_dqn_decks_6.pth\n",
      "   Average reward during training: -0.2850\n",
      "   Saved at episode: 101\n",
      "\n",
      "All DQN models loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "from collections import deque\n",
    "\n",
    "# Define the Q-network\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Load the saved DQN models\n",
    "dqn_models = {}\n",
    "\n",
    "for num_decks in range(1, 7):\n",
    "    print(f\"Loading DQN model for {num_decks} deck(s)...\")\n",
    "    model_path = f\"blackjack_dqn_decks_{num_decks}.pth\"\n",
    "    \n",
    "    # Check if the model file exists\n",
    "    if os.path.exists(model_path):\n",
    "        # Create a new model instance with the correct architecture\n",
    "        input_dim = 4  # [player_sum, dealer_card, usable_ace, can_double]\n",
    "        output_dim = 4  # [stick, hit, double, split]\n",
    "        model = QNetwork(input_dim, output_dim)\n",
    "        \n",
    "        # Load the saved weights\n",
    "        checkpoint = torch.load(model_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        # Set to evaluation mode\n",
    "        model.eval()\n",
    "        \n",
    "        # Store in the models dictionary\n",
    "        dqn_models[num_decks] = model\n",
    "        \n",
    "        print(f\"✅ Successfully loaded model for {num_decks} deck(s) from {model_path}\")\n",
    "        if 'avg_reward' in checkpoint:\n",
    "            print(f\"   Average reward during training: {checkpoint['avg_reward']:.4f}\")\n",
    "        if 'episode' in checkpoint:\n",
    "            print(f\"   Saved at episode: {checkpoint['episode']}\")\n",
    "    else:\n",
    "        print(f\"❌ Model file not found: {model_path}\")\n",
    "        print(f\"   Creating a new untrained model for {num_decks} deck(s)\")\n",
    "        \n",
    "        # Create an untrained model as fallback\n",
    "        input_dim = 4\n",
    "        output_dim = 4\n",
    "        model = QNetwork(input_dim, output_dim)\n",
    "        dqn_models[num_decks] = model\n",
    "\n",
    "print(\"\\nAll DQN models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d956f654",
   "metadata": {},
   "source": [
    "## Evaluation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c758a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Decks</th>\n",
       "      <th>Games</th>\n",
       "      <th>Wins</th>\n",
       "      <th>Draws</th>\n",
       "      <th>Losses</th>\n",
       "      <th>Total Reward</th>\n",
       "      <th>Win Rate (%)</th>\n",
       "      <th>Loss Rate (%)</th>\n",
       "      <th>Draw Rate (%)</th>\n",
       "      <th>Average Reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>5560</td>\n",
       "      <td>701</td>\n",
       "      <td>3739</td>\n",
       "      <td>2670.0</td>\n",
       "      <td>55.60</td>\n",
       "      <td>37.39</td>\n",
       "      <td>7.01</td>\n",
       "      <td>0.2670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>10000</td>\n",
       "      <td>3979</td>\n",
       "      <td>442</td>\n",
       "      <td>5579</td>\n",
       "      <td>-6759.0</td>\n",
       "      <td>39.79</td>\n",
       "      <td>55.79</td>\n",
       "      <td>4.42</td>\n",
       "      <td>-0.6759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>10000</td>\n",
       "      <td>5781</td>\n",
       "      <td>594</td>\n",
       "      <td>3625</td>\n",
       "      <td>789.0</td>\n",
       "      <td>57.81</td>\n",
       "      <td>36.25</td>\n",
       "      <td>5.94</td>\n",
       "      <td>0.0789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>10000</td>\n",
       "      <td>2465</td>\n",
       "      <td>282</td>\n",
       "      <td>7253</td>\n",
       "      <td>-10318.5</td>\n",
       "      <td>24.65</td>\n",
       "      <td>72.53</td>\n",
       "      <td>2.82</td>\n",
       "      <td>-1.0318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>10000</td>\n",
       "      <td>3960</td>\n",
       "      <td>479</td>\n",
       "      <td>5561</td>\n",
       "      <td>-7372.5</td>\n",
       "      <td>39.60</td>\n",
       "      <td>55.61</td>\n",
       "      <td>4.79</td>\n",
       "      <td>-0.7372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>10000</td>\n",
       "      <td>3644</td>\n",
       "      <td>424</td>\n",
       "      <td>5932</td>\n",
       "      <td>-5330.0</td>\n",
       "      <td>36.44</td>\n",
       "      <td>59.32</td>\n",
       "      <td>4.24</td>\n",
       "      <td>-0.5330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Decks  Games  Wins  Draws  Losses  Total Reward  Win Rate (%)  \\\n",
       "0      1  10000  5560    701    3739        2670.0         55.60   \n",
       "1      2  10000  3979    442    5579       -6759.0         39.79   \n",
       "2      3  10000  5781    594    3625         789.0         57.81   \n",
       "3      4  10000  2465    282    7253      -10318.5         24.65   \n",
       "4      5  10000  3960    479    5561       -7372.5         39.60   \n",
       "5      6  10000  3644    424    5932       -5330.0         36.44   \n",
       "\n",
       "   Loss Rate (%)  Draw Rate (%)  Average Reward  \n",
       "0          37.39           7.01          0.2670  \n",
       "1          55.79           4.42         -0.6759  \n",
       "2          36.25           5.94          0.0789  \n",
       "3          72.53           2.82         -1.0318  \n",
       "4          55.61           4.79         -0.7372  \n",
       "5          59.32           4.24         -0.5330  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Helper to preprocess state matching your training code\n",
    "def preprocess_state(state):\n",
    "    \"\"\"\n",
    "    Converts the BlackjackEnv state to a format usable by the neural network\n",
    "    State format: ((card1, card2), dealer_card, usable_ace, can_double)\n",
    "    \"\"\"\n",
    "    player_cards, dealer_card, usable_ace, can_double = state\n",
    "    \n",
    "    # Convert dealer_card using card_value function\n",
    "    dealer_value = card_value(dealer_card)\n",
    "    \n",
    "    # For player cards tuple, calculate sum using card_value\n",
    "    player_sum = 0\n",
    "    for card in player_cards:\n",
    "        if card != 0:  # Skip zero values (padding)\n",
    "            player_sum += card_value(card)\n",
    "    \n",
    "    return np.array([player_sum, dealer_value, usable_ace, can_double], dtype=np.float32)\n",
    "\n",
    "# === RL Evaluation Simulation ===\n",
    "def evaluate_dqn_on_deck_sizes(models, num_games=10000, max_decks=6):\n",
    "    results = []\n",
    "\n",
    "    for num_deck in range(1, max_decks + 1):\n",
    "        env = BlackjackEnv(numdecks=num_deck, natural=True)\n",
    "        q_network = models[num_deck]  # Get the specific model for this deck size\n",
    "\n",
    "        wins = 0\n",
    "        losses = 0\n",
    "        draws = 0\n",
    "        total_reward = 0\n",
    "\n",
    "        for game in range(num_games):\n",
    "            obs = env.reset(seed=42 + game)\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                # Process state to match training format\n",
    "                state = preprocess_state(obs)\n",
    "                \n",
    "                try:\n",
    "                    with torch.no_grad():\n",
    "                        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "                        q_values = q_network(state_tensor)\n",
    "                        \n",
    "                        # Get valid actions\n",
    "                        valid_actions = [0, 1]  # Default stick and hit\n",
    "                        player_cards, dealer_card, usable_ace, can_double = obs\n",
    "                        \n",
    "                        if can_double:\n",
    "                            valid_actions.append(2)\n",
    "                            \n",
    "                        if env.current_hand < len(env.hands):\n",
    "                            current_hand = env.hands[env.current_hand]\n",
    "                            if len(current_hand) == 2 and card_value(current_hand[0]) == card_value(current_hand[1]):\n",
    "                                valid_actions.append(3)\n",
    "                        \n",
    "                        # Mask invalid actions\n",
    "                        for action in range(env.action_space.n):\n",
    "                            if action not in valid_actions:\n",
    "                                q_values[0, action] = float('-inf')\n",
    "                                \n",
    "                        action = q_values.argmax().item()\n",
    "                        \n",
    "                    next_obs, reward, done, _ = env.step(action)\n",
    "                    episode_reward += reward\n",
    "                    obs = next_obs\n",
    "                    \n",
    "                except ValueError:\n",
    "                    # If error, try a fallback action\n",
    "                    action = 0  # Stick is usually safe\n",
    "                    next_obs, reward, done, _ = env.step(action)\n",
    "                    episode_reward += reward\n",
    "                    obs = next_obs\n",
    "\n",
    "            total_reward += episode_reward\n",
    "            if episode_reward > 0:\n",
    "                wins += 1\n",
    "            elif episode_reward < 0:\n",
    "                losses += 1\n",
    "            else:\n",
    "                draws += 1\n",
    "\n",
    "        # Store results\n",
    "        results.append({\n",
    "            \"Decks\": num_deck,\n",
    "            \"Games\": num_games,\n",
    "            \"Wins\": wins,\n",
    "            \"Draws\": draws,\n",
    "            \"Losses\": losses,\n",
    "            \"Total Reward\": round(total_reward, 4),\n",
    "            \"Win Rate (%)\": round((wins / num_games) * 100, 4),\n",
    "            \"Loss Rate (%)\": round((losses / num_games) * 100, 4),\n",
    "            \"Draw Rate (%)\": round((draws / num_games) * 100, 4),\n",
    "            \"Average Reward\": round(total_reward / num_games, 4)\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Example usage:\n",
    "df_rl = evaluate_dqn_on_deck_sizes(dqn_models, num_games=10000, max_decks=6)\n",
    "df_rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6676dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running DQN bankroll experiment...\n",
      "DQN bankroll experiment completed!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Decks</th>\n",
       "      <th>Games</th>\n",
       "      <th>Wins</th>\n",
       "      <th>Draws</th>\n",
       "      <th>Losses</th>\n",
       "      <th>Total Reward</th>\n",
       "      <th>Win Rate (%)</th>\n",
       "      <th>Loss Rate (%)</th>\n",
       "      <th>Draw Rate (%)</th>\n",
       "      <th>Average Reward</th>\n",
       "      <th>Final Money</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>5597</td>\n",
       "      <td>685</td>\n",
       "      <td>3718</td>\n",
       "      <td>2757.0</td>\n",
       "      <td>55.9700</td>\n",
       "      <td>37.1800</td>\n",
       "      <td>6.8500</td>\n",
       "      <td>0.2757</td>\n",
       "      <td>3050.0</td>\n",
       "      <td>Solvent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>401</td>\n",
       "      <td>167</td>\n",
       "      <td>14</td>\n",
       "      <td>220</td>\n",
       "      <td>-253.0</td>\n",
       "      <td>41.6459</td>\n",
       "      <td>54.8628</td>\n",
       "      <td>3.4913</td>\n",
       "      <td>-0.6309</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Bankrupt after 401 games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>10000</td>\n",
       "      <td>5694</td>\n",
       "      <td>601</td>\n",
       "      <td>3705</td>\n",
       "      <td>540.5</td>\n",
       "      <td>56.9400</td>\n",
       "      <td>37.0500</td>\n",
       "      <td>6.0100</td>\n",
       "      <td>0.0541</td>\n",
       "      <td>2358.5</td>\n",
       "      <td>Solvent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>127</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>96</td>\n",
       "      <td>-144.0</td>\n",
       "      <td>20.4724</td>\n",
       "      <td>75.5906</td>\n",
       "      <td>3.9370</td>\n",
       "      <td>-1.1339</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Bankrupt after 127 games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>166</td>\n",
       "      <td>54</td>\n",
       "      <td>7</td>\n",
       "      <td>105</td>\n",
       "      <td>-163.0</td>\n",
       "      <td>32.5301</td>\n",
       "      <td>63.2530</td>\n",
       "      <td>4.2169</td>\n",
       "      <td>-0.9819</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Bankrupt after 166 games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>391</td>\n",
       "      <td>142</td>\n",
       "      <td>17</td>\n",
       "      <td>232</td>\n",
       "      <td>-193.0</td>\n",
       "      <td>36.3171</td>\n",
       "      <td>59.3350</td>\n",
       "      <td>4.3478</td>\n",
       "      <td>-0.4936</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>Bankrupt after 391 games</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Decks  Games  Wins  Draws  Losses  Total Reward  Win Rate (%)  \\\n",
       "0      1  10000  5597    685    3718        2757.0       55.9700   \n",
       "1      2    401   167     14     220        -253.0       41.6459   \n",
       "2      3  10000  5694    601    3705         540.5       56.9400   \n",
       "3      4    127    26      5      96        -144.0       20.4724   \n",
       "4      5    166    54      7     105        -163.0       32.5301   \n",
       "5      6    391   142     17     232        -193.0       36.3171   \n",
       "\n",
       "   Loss Rate (%)  Draw Rate (%)  Average Reward  Final Money  \\\n",
       "0        37.1800         6.8500          0.2757       3050.0   \n",
       "1        54.8628         3.4913         -0.6309         -1.0   \n",
       "2        37.0500         6.0100          0.0541       2358.5   \n",
       "3        75.5906         3.9370         -1.1339          0.0   \n",
       "4        63.2530         4.2169         -0.9819         -1.0   \n",
       "5        59.3350         4.3478         -0.4936         -1.5   \n",
       "\n",
       "                     Status  \n",
       "0                   Solvent  \n",
       "1  Bankrupt after 401 games  \n",
       "2                   Solvent  \n",
       "3  Bankrupt after 127 games  \n",
       "4  Bankrupt after 166 games  \n",
       "5  Bankrupt after 391 games  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to select action with the DQN model (no exploration)\n",
    "def select_action_eval(state, q_network, env):\n",
    "    # Extract state components to check for valid actions\n",
    "    player_cards, dealer_card, usable_ace, can_double = state\n",
    "    \n",
    "    # Default valid actions (stick and hit)\n",
    "    valid_actions = [0, 1]  \n",
    "    \n",
    "    # Check if can double down\n",
    "    if can_double:\n",
    "        valid_actions.append(2)\n",
    "    \n",
    "    # Check if the current hand allows split\n",
    "    if hasattr(env, 'current_hand') and hasattr(env, 'hands'):\n",
    "        if env.current_hand < len(env.hands):\n",
    "            current_hand = env.hands[env.current_hand]\n",
    "            # Check if can split (same card value and exactly 2 cards)\n",
    "            if len(current_hand) == 2 and card_value(current_hand[0]) == card_value(current_hand[1]):\n",
    "                valid_actions.append(3)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        state_tensor = torch.FloatTensor(preprocess_state(state)).unsqueeze(0)\n",
    "        q_values = q_network(state_tensor)\n",
    "        \n",
    "        # Mask invalid actions by setting their Q-values to -inf\n",
    "        for action in range(4):  # Assume 4 possible actions\n",
    "            if action not in valid_actions:\n",
    "                q_values[0, action] = float('-inf')\n",
    "        \n",
    "        return q_values.argmax().item()\n",
    "\n",
    "def evaluate_dqn_bankroll(models, num_games=10000, max_decks=6, initial_money=100):\n",
    "    \"\"\"\n",
    "    Evaluate DQN models with a bankroll simulation across different deck sizes\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for num_deck in range(1, max_decks + 1):\n",
    "        random.seed(42)\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "        \n",
    "        env = BlackjackEnv(numdecks=num_deck, natural=True)\n",
    "        q_network = models[num_deck]  # Get the specific model for this deck size\n",
    "        q_network.eval()  # Set the model to evaluation mode\n",
    "\n",
    "        money = initial_money\n",
    "        wins = 0\n",
    "        losses = 0\n",
    "        draws = 0\n",
    "        total_reward = 0\n",
    "        \n",
    "        # For tracking bankruptcy\n",
    "        games_played = 0\n",
    "        went_bankrupt = False\n",
    "\n",
    "        for game in range(1, num_games+1):\n",
    "            if money <= 0:\n",
    "                went_bankrupt = True\n",
    "                games_played = game - 1\n",
    "                break\n",
    "                \n",
    "            games_played = game\n",
    "            obs = env.reset()\n",
    "            done = False\n",
    "            \n",
    "            # Bet $1\n",
    "            money -= 1\n",
    "            episode_reward = 0\n",
    "            doubled_down = False\n",
    "\n",
    "            while not done:\n",
    "                # Get action from the model\n",
    "                action = select_action_eval(obs, q_network, env)\n",
    "                \n",
    "                # If doubling down, subtract another dollar\n",
    "                if action == 2:  # Double down\n",
    "                    money -= 1\n",
    "                    doubled_down = True\n",
    "                \n",
    "                # Execute the action\n",
    "                try:\n",
    "                    next_obs, reward, done, _ = env.step(action)\n",
    "                    episode_reward += reward\n",
    "                    obs = next_obs\n",
    "                except ValueError:\n",
    "                    # Fallback if error\n",
    "                    action = 0  # Stick\n",
    "                    next_obs, reward, done, _ = env.step(action)\n",
    "                    episode_reward += reward\n",
    "                    obs = next_obs\n",
    "\n",
    "            # End of episode accounting\n",
    "            total_reward += episode_reward\n",
    "            \n",
    "            if episode_reward > 0:\n",
    "                wins += 1\n",
    "                # Calculate payout\n",
    "                if doubled_down:\n",
    "                    money += 4  # Get 2x the doubled bet\n",
    "                else:\n",
    "                    if episode_reward > 1:  # Blackjack\n",
    "                        money += 2.5  # Blackjack pays 3:2\n",
    "                    else:\n",
    "                        money += 2  # Regular win\n",
    "            elif episode_reward < 0:\n",
    "                losses += 1\n",
    "                # Money already subtracted for bet\n",
    "            else:\n",
    "                draws += 1\n",
    "                if doubled_down:\n",
    "                    money += 2  # Get doubled bet back\n",
    "                else:\n",
    "                    money += 1  # Get original bet back\n",
    "\n",
    "        # Store results\n",
    "        bankruptcy_message = f\"Bankrupt after {games_played} games\" if went_bankrupt else \"Solvent\"\n",
    "        \n",
    "        results.append({\n",
    "            \"Decks\": num_deck,\n",
    "            \"Games\": games_played,\n",
    "            \"Wins\": wins,\n",
    "            \"Draws\": draws,\n",
    "            \"Losses\": losses,\n",
    "            \"Total Reward\": round(total_reward, 4),\n",
    "            \"Win Rate (%)\": round((wins / games_played) * 100, 4) if games_played > 0 else 0,\n",
    "            \"Loss Rate (%)\": round((losses / games_played) * 100, 4) if games_played > 0 else 0,\n",
    "            \"Draw Rate (%)\": round((draws / games_played) * 100, 4) if games_played > 0 else 0,\n",
    "            \"Average Reward\": round(total_reward / games_played, 4) if games_played > 0 else 0,\n",
    "            \"Final Money\": round(money, 2),\n",
    "            \"Status\": bankruptcy_message\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Evaluate DQN model performance using the bankroll experiment\n",
    "print(\"\\nRunning DQN bankroll experiment...\")\n",
    "df_dqn_bankroll = evaluate_dqn_bankroll(dqn_models, num_games=10000, max_decks=6, initial_money=100)\n",
    "print(\"DQN bankroll experiment completed!\")\n",
    "\n",
    "# Display the results\n",
    "df_dqn_bankroll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3347af83",
   "metadata": {},
   "source": [
    "# Set PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513ec849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "from collections import deque\n",
    "\n",
    "# PPO Actor-Critic Network\n",
    "class PPOActorCritic(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(PPOActorCritic, self).__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.policy_head = nn.Linear(64, output_dim)\n",
    "        self.value_head = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared_out = self.shared(x)\n",
    "        logits = self.policy_head(shared_out)\n",
    "        value = self.value_head(shared_out)\n",
    "        return logits, value\n",
    "\n",
    "# Adjusted for BlackjackEnv\n",
    "def preprocess_state(state):\n",
    "    \"\"\"\n",
    "    Converts the BlackjackEnv state to a format usable by the neural network\n",
    "    State format: ((card1, card2), dealer_card, usable_ace, can_double)\n",
    "    \"\"\"\n",
    "    player_cards, dealer_card, usable_ace, can_double = state\n",
    "    \n",
    "    # Convert dealer_card using card_value function\n",
    "    dealer_value = card_value(dealer_card)\n",
    "    \n",
    "    # For player cards tuple, calculate sum using card_value\n",
    "    player_sum = 0\n",
    "    for card in player_cards:\n",
    "        if card != 0:  # Skip zero values (padding)\n",
    "            player_sum += card_value(card)\n",
    "    \n",
    "    return np.array([player_sum, dealer_value, usable_ace, can_double], dtype=np.float32)\n",
    "\n",
    "# Function to select valid actions\n",
    "def select_valid_action(logits, state, env):\n",
    "    player_cards, dealer_card, usable_ace, can_double = state\n",
    "    \n",
    "    # Default valid actions (stick and hit)\n",
    "    valid_actions = [0, 1]\n",
    "    \n",
    "    # Check if can double down\n",
    "    if can_double:\n",
    "        valid_actions.append(2)\n",
    "    \n",
    "    # Check if can split\n",
    "    if env.current_hand < len(env.hands):\n",
    "        current_hand = env.hands[env.current_hand]\n",
    "        if len(current_hand) == 2 and card_value(current_hand[0]) == card_value(current_hand[1]):\n",
    "            valid_actions.append(3)\n",
    "    \n",
    "    # Apply mask to logits\n",
    "    masked_logits = logits.clone()\n",
    "    for action in range(env.action_space.n):\n",
    "        if action not in valid_actions:\n",
    "            masked_logits[action] = float('-inf')\n",
    "    \n",
    "    return masked_logits\n",
    "\n",
    "# Compute GAE\n",
    "def compute_gae(rewards, values, dones, gamma=0.99, lam=0.95):\n",
    "    returns = []\n",
    "    advantages = []\n",
    "    gae = 0\n",
    "    next_value = 0\n",
    "\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * next_value * (1 - dones[step]) - values[step]\n",
    "        gae = delta + gamma * lam * (1 - dones[step]) * gae\n",
    "        advantages.insert(0, gae)\n",
    "        returns.insert(0, gae + values[step])\n",
    "        next_value = values[step]\n",
    "\n",
    "    return torch.FloatTensor(returns), torch.FloatTensor(advantages)\n",
    "\n",
    "# PPO Training Function\n",
    "def train_ppo(env, n_episodes=5000, gamma=0.99, lam=0.95, clip_eps=0.2,\n",
    "              lr=3e-4, epochs=4, batch_size=64, model_save_path='ppo_blackjack.pth'):\n",
    "\n",
    "    input_dim = 4  # [player_sum, dealer_card, usable_ace, can_double]\n",
    "    output_dim = env.action_space.n\n",
    "\n",
    "    policy_net = PPOActorCritic(input_dim, output_dim)\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "\n",
    "    memory = []\n",
    "    reward_window = deque(maxlen=100)\n",
    "\n",
    "    best_avg_reward = float('-inf')\n",
    "    best_model = None\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        episode_data = []\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            state = preprocess_state(obs)\n",
    "            state_tensor = torch.FloatTensor(state)\n",
    "            logits, value = policy_net(state_tensor)\n",
    "            \n",
    "            # Mask invalid actions\n",
    "            masked_logits = select_valid_action(logits, obs, env)\n",
    "            probs = torch.softmax(masked_logits, dim=-1)\n",
    "            dist = torch.distributions.Categorical(probs)\n",
    "            \n",
    "            try:\n",
    "                action = dist.sample()\n",
    "                log_prob = dist.log_prob(action)\n",
    "                \n",
    "                next_obs, reward, done, _ = env.step(action.item())\n",
    "                episode_data.append((state, action.item(), reward, log_prob.item(), value.item(), done))\n",
    "                episode_reward += reward\n",
    "                obs = next_obs\n",
    "                \n",
    "            except ValueError:\n",
    "                # Fallback to a safe action (stick)\n",
    "                action = 0  # stick\n",
    "                next_obs, reward, done, _ = env.step(action)\n",
    "                \n",
    "                # Re-compute log_prob for the fallback action\n",
    "                masked_logits = select_valid_action(logits, obs, env)\n",
    "                probs = torch.softmax(masked_logits, dim=-1)\n",
    "                dist = torch.distributions.Categorical(probs)\n",
    "                log_prob = dist.log_prob(torch.tensor(action))\n",
    "                \n",
    "                episode_data.append((state, action, reward, log_prob.item(), value.item(), done))\n",
    "                episode_reward += reward\n",
    "                obs = next_obs\n",
    "\n",
    "        memory.extend(episode_data)\n",
    "        reward_window.append(episode_reward)\n",
    "\n",
    "        # Train when we have enough data\n",
    "        if len(memory) >= batch_size:\n",
    "            states, actions, rewards, old_log_probs, values, dones = zip(*memory)\n",
    "\n",
    "            # Process states to tensors\n",
    "            states_tensor = torch.FloatTensor(np.array(states))\n",
    "            actions_tensor = torch.LongTensor(actions)\n",
    "            old_log_probs_tensor = torch.FloatTensor(old_log_probs)\n",
    "            \n",
    "            returns, advantages = compute_gae(rewards, values, dones, gamma, lam)\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "            for _ in range(epochs):\n",
    "                logits, value_preds = policy_net(states_tensor)\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                dist = torch.distributions.Categorical(probs)\n",
    "\n",
    "                new_log_probs = dist.log_prob(actions_tensor)\n",
    "                ratio = torch.exp(new_log_probs - old_log_probs_tensor)\n",
    "\n",
    "                policy_loss = -torch.min(\n",
    "                    ratio * advantages,\n",
    "                    torch.clamp(ratio, 1 - clip_eps, 1 + clip_eps) * advantages\n",
    "                ).mean()\n",
    "\n",
    "                value_loss = nn.MSELoss()(value_preds.squeeze(), returns)\n",
    "\n",
    "                loss = policy_loss + 0.5 * value_loss - 0.01 * dist.entropy().mean()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            memory = []\n",
    "\n",
    "        # Save model based on rolling average\n",
    "        if len(reward_window) == 100:\n",
    "            avg_reward = np.mean(reward_window)\n",
    "            if avg_reward > best_avg_reward:\n",
    "                best_avg_reward = avg_reward\n",
    "                best_model = policy_net\n",
    "                torch.save({\n",
    "                    'model_state_dict': best_model.state_dict(),\n",
    "                    'avg_reward': best_avg_reward,\n",
    "                    'episode': episode + 1\n",
    "                }, model_save_path)\n",
    "                print(f\"✅ Best model saved at episode {episode+1} | Avg Reward: {best_avg_reward:.4f}\")\n",
    "\n",
    "        if (episode + 1) % 500 == 0:\n",
    "            print(f\"Episode {episode+1} | Avg Reward: {np.mean(list(reward_window)):.4f}\")\n",
    "\n",
    "    return best_model if best_model else policy_net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb8c4e4",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6827439f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models for different deck counts\n",
    "ppo_models = {}\n",
    "\n",
    "for num_decks in range(1, 7):\n",
    "    print(f\"\\n=== Training PPO model for {num_decks} deck(s) ===\")\n",
    "    env = BlackjackEnv(numdecks=num_decks, natural=True)\n",
    "    model_save_path = f\"blackjack_ppo_decks_{num_decks}.pth\"\n",
    "    ppo_model = train_ppo(env, n_episodes=10000, model_save_path=model_save_path)\n",
    "    ppo_models[num_decks] = ppo_model\n",
    "    print(f\"Completed PPO training for {num_decks} deck(s)\")\n",
    "\n",
    "print(\"All PPO models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaaacf6",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dc1c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# === PPO Evaluation on Deck Sizes ===\n",
    "def evaluate_ppo_on_deck_sizes(models, num_games=10000, max_decks=6):\n",
    "    results = []\n",
    "\n",
    "    for num_deck in range(1, max_decks + 1):\n",
    "        env = BlackjackEnv(numdecks=num_deck, natural=True)\n",
    "        policy_net = models[num_deck]  # Get the specific model for this deck size\n",
    "\n",
    "        wins = 0\n",
    "        losses = 0\n",
    "        draws = 0\n",
    "        total_reward = 0\n",
    "\n",
    "        for game in range(num_games):\n",
    "            obs = env.reset(seed=42 + game)\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                # Process state to match training format\n",
    "                state = preprocess_state(obs)\n",
    "                \n",
    "                try:\n",
    "                    with torch.no_grad():\n",
    "                        state_tensor = torch.FloatTensor(state)\n",
    "                        logits, _ = policy_net(state_tensor)\n",
    "                        \n",
    "                        # Apply action masking\n",
    "                        masked_logits = select_valid_action(logits, obs, env)\n",
    "                        probs = torch.softmax(masked_logits, dim=-1)\n",
    "                        \n",
    "                        # For evaluation, choose the action with highest probability\n",
    "                        action = torch.argmax(probs).item()\n",
    "                    \n",
    "                    next_obs, reward, done, _ = env.step(action)\n",
    "                    episode_reward += reward\n",
    "                    obs = next_obs\n",
    "                    \n",
    "                except ValueError:\n",
    "                    # If error, try a fallback action\n",
    "                    action = 0  # Stick is usually safe\n",
    "                    next_obs, reward, done, _ = env.step(action)\n",
    "                    episode_reward += reward\n",
    "                    obs = next_obs\n",
    "\n",
    "            total_reward += episode_reward\n",
    "            if episode_reward > 0:\n",
    "                wins += 1\n",
    "            elif episode_reward < 0:\n",
    "                losses += 1\n",
    "            else:\n",
    "                draws += 1\n",
    "\n",
    "        # Store results\n",
    "        results.append({\n",
    "            \"Decks\": num_deck,\n",
    "            \"Games\": num_games,\n",
    "            \"Wins\": wins,\n",
    "            \"Draws\": draws,\n",
    "            \"Losses\": losses,\n",
    "            \"Total Reward\": round(total_reward, 4),\n",
    "            \"Win Rate (%)\": round((wins / num_games) * 100, 4),\n",
    "            \"Loss Rate (%)\": round((losses / num_games) * 100, 4),\n",
    "            \"Draw Rate (%)\": round((draws / num_games) * 100, 4),\n",
    "            \"Average Reward\": round(total_reward / num_games, 4)\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def evaluate_ppo_bankroll(models, num_games=10000, max_decks=6, initial_money=100):\n",
    "    \"\"\"\n",
    "    Evaluate PPO models with a bankroll simulation across different deck sizes\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for num_deck in range(1, max_decks + 1):\n",
    "        env = BlackjackEnv(numdecks=num_deck, natural=True)\n",
    "        policy_net = models[num_deck]  # Get the specific model for this deck size\n",
    "        policy_net.eval()  # Set the model to evaluation mode\n",
    "\n",
    "        money = initial_money\n",
    "        wins = 0\n",
    "        losses = 0\n",
    "        draws = 0\n",
    "        total_reward = 0\n",
    "        \n",
    "        # For tracking bankruptcy\n",
    "        games_played = 0\n",
    "        went_bankrupt = False\n",
    "\n",
    "        for game in range(1, num_games+1):\n",
    "            if money <= 0:\n",
    "                went_bankrupt = True\n",
    "                games_played = game - 1\n",
    "                break\n",
    "                \n",
    "            games_played = game\n",
    "            obs = env.reset()\n",
    "            done = False\n",
    "            \n",
    "            # Bet $1\n",
    "            money -= 1\n",
    "            episode_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                # Process state to match training format\n",
    "                state = preprocess_state(obs)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    state_tensor = torch.FloatTensor(state)\n",
    "                    logits, _ = policy_net(state_tensor)\n",
    "                    \n",
    "                    # Get valid actions for the current state\n",
    "                    player_cards, dealer_card, usable_ace, can_double = obs\n",
    "                    valid_actions = [0, 1]  # Stick, Hit are always valid\n",
    "                    \n",
    "                    if can_double:\n",
    "                        valid_actions.append(2)  # Double down if allowed\n",
    "                    \n",
    "                    # Check for split (same rank cards)\n",
    "                    if hasattr(env, 'current_hand') and hasattr(env, 'hands'):\n",
    "                        if env.current_hand < len(env.hands):\n",
    "                            current_hand = env.hands[env.current_hand]\n",
    "                            if len(current_hand) == 2 and card_value(current_hand[0]) == card_value(current_hand[1]):\n",
    "                                valid_actions.append(3)  # Split if allowed\n",
    "                    \n",
    "                    # Mask invalid actions\n",
    "                    masked_logits = logits.clone()\n",
    "                    for i in range(len(masked_logits)):\n",
    "                        if i not in valid_actions:\n",
    "                            masked_logits[i] = float('-inf')\n",
    "                    \n",
    "                    probs = torch.softmax(masked_logits, dim=-1)\n",
    "                    action = torch.argmax(probs).item()\n",
    "                \n",
    "                # Check if action is valid (safeguard)\n",
    "                if action not in valid_actions:\n",
    "                    action = 0  # Default to stick if somehow invalid\n",
    "                \n",
    "                # If doubling down, subtract another dollar\n",
    "                if action == 2:  # Double down\n",
    "                    money -= 1\n",
    "                \n",
    "                # Execute the action\n",
    "                try:\n",
    "                    next_obs, reward, done, _ = env.step(action)\n",
    "                    episode_reward += reward\n",
    "                    obs = next_obs\n",
    "                except ValueError:\n",
    "                    # Fallback if error\n",
    "                    action = 0  # Stick\n",
    "                    next_obs, reward, done, _ = env.step(action)\n",
    "                    episode_reward += reward\n",
    "                    obs = next_obs\n",
    "\n",
    "            # End of episode accounting\n",
    "            total_reward += episode_reward\n",
    "            \n",
    "            if episode_reward > 0:\n",
    "                wins += 1\n",
    "                # Calculate payout based on bet\n",
    "                if action == 2:  # Double down win\n",
    "                    money += 4  # Get 2x the doubled bet\n",
    "                else:\n",
    "                    money += 2  # Regular win\n",
    "            elif episode_reward < 0:\n",
    "                losses += 1\n",
    "                # Money already subtracted for bet\n",
    "            else:\n",
    "                draws += 1\n",
    "                if action == 2:  # Double down push\n",
    "                    money += 2  # Get doubled bet back\n",
    "                else:\n",
    "                    money += 1  # Get original bet back\n",
    "\n",
    "        # Store results\n",
    "        bankruptcy_message = f\"Bankrupt after {games_played} games\" if went_bankrupt else \"Solvent\"\n",
    "        \n",
    "        results.append({\n",
    "            \"Decks\": num_deck,\n",
    "            \"Games\": games_played,\n",
    "            \"Wins\": wins,\n",
    "            \"Draws\": draws,\n",
    "            \"Losses\": losses,\n",
    "            \"Total Reward\": round(total_reward, 4),\n",
    "            \"Win Rate (%)\": round((wins / games_played) * 100, 4) if games_played > 0 else 0,\n",
    "            \"Loss Rate (%)\": round((losses / games_played) * 100, 4) if games_played > 0 else 0,\n",
    "            \"Draw Rate (%)\": round((draws / games_played) * 100, 4) if games_played > 0 else 0,\n",
    "            \"Average Reward\": round(total_reward / games_played, 4) if games_played > 0 else 0,\n",
    "            \"Final Money\": money,\n",
    "            \"Status\": bankruptcy_message\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "630721a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PPO model for 1 deck(s)...\n",
      "✅ Successfully loaded model for 1 deck(s) from blackjack_ppo_decks_1.pth\n",
      "   Average reward during training: 0.4200\n",
      "   Saved at episode: 3576\n",
      "Loading PPO model for 2 deck(s)...\n",
      "✅ Successfully loaded model for 2 deck(s) from blackjack_ppo_decks_2.pth\n",
      "   Average reward during training: 0.4050\n",
      "   Saved at episode: 3796\n",
      "Loading PPO model for 3 deck(s)...\n",
      "✅ Successfully loaded model for 3 deck(s) from blackjack_ppo_decks_3.pth\n",
      "   Average reward during training: 0.4100\n",
      "   Saved at episode: 9592\n",
      "Loading PPO model for 4 deck(s)...\n",
      "✅ Successfully loaded model for 4 deck(s) from blackjack_ppo_decks_4.pth\n",
      "   Average reward during training: 0.4100\n",
      "   Saved at episode: 5973\n",
      "Loading PPO model for 5 deck(s)...\n",
      "✅ Successfully loaded model for 5 deck(s) from blackjack_ppo_decks_5.pth\n",
      "   Average reward during training: 0.3350\n",
      "   Saved at episode: 6977\n",
      "Loading PPO model for 6 deck(s)...\n",
      "✅ Successfully loaded model for 6 deck(s) from blackjack_ppo_decks_6.pth\n",
      "   Average reward during training: 0.4350\n",
      "   Saved at episode: 7180\n",
      "\n",
      "All models loaded. Running evaluation...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Decks</th>\n",
       "      <th>Games</th>\n",
       "      <th>Wins</th>\n",
       "      <th>Draws</th>\n",
       "      <th>Losses</th>\n",
       "      <th>Total Reward</th>\n",
       "      <th>Win Rate (%)</th>\n",
       "      <th>Loss Rate (%)</th>\n",
       "      <th>Draw Rate (%)</th>\n",
       "      <th>Average Reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>4616</td>\n",
       "      <td>661</td>\n",
       "      <td>4723</td>\n",
       "      <td>117.0</td>\n",
       "      <td>46.16</td>\n",
       "      <td>47.23</td>\n",
       "      <td>6.61</td>\n",
       "      <td>0.0117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>10000</td>\n",
       "      <td>4635</td>\n",
       "      <td>651</td>\n",
       "      <td>4714</td>\n",
       "      <td>154.5</td>\n",
       "      <td>46.35</td>\n",
       "      <td>47.14</td>\n",
       "      <td>6.51</td>\n",
       "      <td>0.0155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>10000</td>\n",
       "      <td>4799</td>\n",
       "      <td>677</td>\n",
       "      <td>4524</td>\n",
       "      <td>555.0</td>\n",
       "      <td>47.99</td>\n",
       "      <td>45.24</td>\n",
       "      <td>6.77</td>\n",
       "      <td>0.0555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>10000</td>\n",
       "      <td>4538</td>\n",
       "      <td>623</td>\n",
       "      <td>4839</td>\n",
       "      <td>-76.0</td>\n",
       "      <td>45.38</td>\n",
       "      <td>48.39</td>\n",
       "      <td>6.23</td>\n",
       "      <td>-0.0076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>10000</td>\n",
       "      <td>4751</td>\n",
       "      <td>716</td>\n",
       "      <td>4533</td>\n",
       "      <td>433.0</td>\n",
       "      <td>47.51</td>\n",
       "      <td>45.33</td>\n",
       "      <td>7.16</td>\n",
       "      <td>0.0433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>10000</td>\n",
       "      <td>4690</td>\n",
       "      <td>719</td>\n",
       "      <td>4591</td>\n",
       "      <td>343.0</td>\n",
       "      <td>46.90</td>\n",
       "      <td>45.91</td>\n",
       "      <td>7.19</td>\n",
       "      <td>0.0343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Decks  Games  Wins  Draws  Losses  Total Reward  Win Rate (%)  \\\n",
       "0      1  10000  4616    661    4723         117.0         46.16   \n",
       "1      2  10000  4635    651    4714         154.5         46.35   \n",
       "2      3  10000  4799    677    4524         555.0         47.99   \n",
       "3      4  10000  4538    623    4839         -76.0         45.38   \n",
       "4      5  10000  4751    716    4533         433.0         47.51   \n",
       "5      6  10000  4690    719    4591         343.0         46.90   \n",
       "\n",
       "   Loss Rate (%)  Draw Rate (%)  Average Reward  \n",
       "0          47.23           6.61          0.0117  \n",
       "1          47.14           6.51          0.0155  \n",
       "2          45.24           6.77          0.0555  \n",
       "3          48.39           6.23         -0.0076  \n",
       "4          45.33           7.16          0.0433  \n",
       "5          45.91           7.19          0.0343  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# Load the saved PPO models\n",
    "ppo_models = {}\n",
    "\n",
    "for num_decks in range(1, 7):\n",
    "    print(f\"Loading PPO model for {num_decks} deck(s)...\")\n",
    "    model_path = f\"blackjack_ppo_decks_{num_decks}.pth\"\n",
    "    \n",
    "    # Check if the model file exists\n",
    "    if os.path.exists(model_path):\n",
    "        # Create a new model instance with the correct architecture\n",
    "        input_dim = 4  # [player_sum, dealer_card, usable_ace, can_double]\n",
    "        output_dim = 4  # [stick, hit, double, split]\n",
    "        model = PPOActorCritic(input_dim, output_dim)\n",
    "        \n",
    "        # Load the saved weights\n",
    "        checkpoint = torch.load(model_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        # Set to evaluation mode\n",
    "        model.eval()\n",
    "        \n",
    "        # Store in the models dictionary\n",
    "        ppo_models[num_decks] = model\n",
    "        \n",
    "        print(f\"✅ Successfully loaded model for {num_decks} deck(s) from {model_path}\")\n",
    "        print(f\"   Average reward during training: {checkpoint['avg_reward']:.4f}\")\n",
    "        print(f\"   Saved at episode: {checkpoint['episode']}\")\n",
    "    else:\n",
    "        print(f\"❌ Model file not found: {model_path}\")\n",
    "        print(f\"   Creating a new untrained model for {num_decks} deck(s)\")\n",
    "        \n",
    "        # Create an untrained model as fallback\n",
    "        input_dim = 4\n",
    "        output_dim = 4\n",
    "        model = PPOActorCritic(input_dim, output_dim)\n",
    "        ppo_models[num_decks] = model\n",
    "\n",
    "print(\"\\nAll models loaded. Running evaluation...\")\n",
    "\n",
    "# Run the evaluation\n",
    "df_ppo = evaluate_ppo_on_deck_sizes(ppo_models, num_games=10000, max_decks=6)\n",
    "\n",
    "# Display the results\n",
    "df_ppo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87bd75ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Decks</th>\n",
       "      <th>Games</th>\n",
       "      <th>Wins</th>\n",
       "      <th>Draws</th>\n",
       "      <th>Losses</th>\n",
       "      <th>Total Reward</th>\n",
       "      <th>Win Rate (%)</th>\n",
       "      <th>Loss Rate (%)</th>\n",
       "      <th>Draw Rate (%)</th>\n",
       "      <th>Average Reward</th>\n",
       "      <th>Final Money</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>4651</td>\n",
       "      <td>610</td>\n",
       "      <td>4739</td>\n",
       "      <td>130.5</td>\n",
       "      <td>46.5100</td>\n",
       "      <td>47.3900</td>\n",
       "      <td>6.1000</td>\n",
       "      <td>0.0131</td>\n",
       "      <td>12</td>\n",
       "      <td>Solvent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>10000</td>\n",
       "      <td>4673</td>\n",
       "      <td>668</td>\n",
       "      <td>4659</td>\n",
       "      <td>249.5</td>\n",
       "      <td>46.7300</td>\n",
       "      <td>46.5900</td>\n",
       "      <td>6.6800</td>\n",
       "      <td>0.0249</td>\n",
       "      <td>114</td>\n",
       "      <td>Solvent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>10000</td>\n",
       "      <td>4777</td>\n",
       "      <td>717</td>\n",
       "      <td>4506</td>\n",
       "      <td>555.0</td>\n",
       "      <td>47.7700</td>\n",
       "      <td>45.0600</td>\n",
       "      <td>7.1700</td>\n",
       "      <td>0.0555</td>\n",
       "      <td>407</td>\n",
       "      <td>Solvent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4568</td>\n",
       "      <td>2095</td>\n",
       "      <td>278</td>\n",
       "      <td>2195</td>\n",
       "      <td>13.0</td>\n",
       "      <td>45.8625</td>\n",
       "      <td>48.0517</td>\n",
       "      <td>6.0858</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>0</td>\n",
       "      <td>Bankrupt after 4568 games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5000</td>\n",
       "      <td>2286</td>\n",
       "      <td>328</td>\n",
       "      <td>2386</td>\n",
       "      <td>14.5</td>\n",
       "      <td>45.7200</td>\n",
       "      <td>47.7200</td>\n",
       "      <td>6.5600</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0</td>\n",
       "      <td>Bankrupt after 5000 games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>10000</td>\n",
       "      <td>4713</td>\n",
       "      <td>675</td>\n",
       "      <td>4612</td>\n",
       "      <td>342.0</td>\n",
       "      <td>47.1300</td>\n",
       "      <td>46.1200</td>\n",
       "      <td>6.7500</td>\n",
       "      <td>0.0342</td>\n",
       "      <td>209</td>\n",
       "      <td>Solvent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Decks  Games  Wins  Draws  Losses  Total Reward  Win Rate (%)  \\\n",
       "0      1  10000  4651    610    4739         130.5       46.5100   \n",
       "1      2  10000  4673    668    4659         249.5       46.7300   \n",
       "2      3  10000  4777    717    4506         555.0       47.7700   \n",
       "3      4   4568  2095    278    2195          13.0       45.8625   \n",
       "4      5   5000  2286    328    2386          14.5       45.7200   \n",
       "5      6  10000  4713    675    4612         342.0       47.1300   \n",
       "\n",
       "   Loss Rate (%)  Draw Rate (%)  Average Reward  Final Money  \\\n",
       "0        47.3900         6.1000          0.0131           12   \n",
       "1        46.5900         6.6800          0.0249          114   \n",
       "2        45.0600         7.1700          0.0555          407   \n",
       "3        48.0517         6.0858          0.0028            0   \n",
       "4        47.7200         6.5600          0.0029            0   \n",
       "5        46.1200         6.7500          0.0342          209   \n",
       "\n",
       "                      Status  \n",
       "0                    Solvent  \n",
       "1                    Solvent  \n",
       "2                    Solvent  \n",
       "3  Bankrupt after 4568 games  \n",
       "4  Bankrupt after 5000 games  \n",
       "5                    Solvent  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ppo_bankroll = evaluate_ppo_bankroll(ppo_models, num_games=10000, max_decks=6, initial_money=100)\n",
    "df_ppo_bankroll"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study_venvs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
