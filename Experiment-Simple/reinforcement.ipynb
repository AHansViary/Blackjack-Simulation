{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "567f0958",
   "metadata": {},
   "source": [
    "# Set Environment (No Split, No Double Down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "071c5f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "import random\n",
    "\n",
    "# Full deck with distinct face cards\n",
    "CARDS = [1, 2, 3, 4, 5, 6, 7, 8, 9, '10', 'J', 'Q', 'K'] * 4\n",
    "\n",
    "def card_value(card):\n",
    "    return 10 if card in ['10', 'J', 'Q', 'K'] else card\n",
    "\n",
    "def draw_card(deck):\n",
    "    return deck.pop()\n",
    "\n",
    "def draw_hand(deck):\n",
    "    return [draw_card(deck), draw_card(deck)]\n",
    "\n",
    "def usable_ace(hand):\n",
    "    return 1 in hand and sum(card_value(c) for c in hand) + 10 <= 21\n",
    "\n",
    "def sum_hand(hand):\n",
    "    total = sum(card_value(c) for c in hand)\n",
    "    return total + 10 if usable_ace(hand) else total\n",
    "\n",
    "def is_bust(hand):\n",
    "    return sum_hand(hand) > 21\n",
    "\n",
    "def score(hand):\n",
    "    return 0 if is_bust(hand) else sum_hand(hand)\n",
    "\n",
    "def is_natural(hand):\n",
    "    return set(hand) == {1, '10'} or set(hand) == {1, 'J'} or set(hand) == {1, 'Q'} or set(hand) == {1, 'K'}\n",
    "\n",
    "class BlackjackEnv(gym.Env):\n",
    "    metadata = {\"render.modes\": [\"human\"]}\n",
    "\n",
    "    def __init__(self, numdecks=4, natural=True):\n",
    "        super().__init__()\n",
    "        self.action_space = spaces.Discrete(2)  # 0: Stick, 1: Hit\n",
    "        self.observation_space = spaces.Tuple((\n",
    "            spaces.Tuple((spaces.Discrete(32), spaces.Discrete(32))),  # Player hand (2 cards)\n",
    "            spaces.Discrete(11),  # Dealer's showing card\n",
    "            spaces.Discrete(2)    # Usable ace\n",
    "        ))\n",
    "\n",
    "        self.natural = natural\n",
    "        self.numdecks = numdecks\n",
    "        self.decks = CARDS * self.numdecks\n",
    "        random.shuffle(self.decks)\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        random.seed(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        if seed is not None:\n",
    "            self.seed(seed)\n",
    "\n",
    "        if self._deck_is_out():\n",
    "            self.decks = CARDS * self.numdecks\n",
    "            random.shuffle(self.decks)\n",
    "\n",
    "        self.dealer = draw_hand(self.decks)\n",
    "        first_hand = draw_hand(self.decks)\n",
    "        self.hands = [first_hand]\n",
    "        self.current_hand = 0\n",
    "        self.actionstaken = 0\n",
    "        self.hand_results = []\n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action), f\"Invalid action: {action}\"\n",
    "        if self._deck_is_out():\n",
    "            self.decks = CARDS * self.numdecks\n",
    "            random.shuffle(self.decks)\n",
    "\n",
    "        done = False\n",
    "        reward = 0\n",
    "        hand = self.hands[self.current_hand]\n",
    "\n",
    "        if action == 0:  # Stick\n",
    "            self._finalize_current_hand()\n",
    "\n",
    "        elif action == 1:  # Hit\n",
    "            hand.append(draw_card(self.decks))\n",
    "            if is_bust(hand):\n",
    "                self.hand_results.append(-1)\n",
    "                self._advance_hand()\n",
    "\n",
    "        self.actionstaken += 1\n",
    "\n",
    "        if self.current_hand >= len(self.hands):\n",
    "            while sum_hand(self.dealer) < 17:\n",
    "                self.dealer.append(draw_card(self.decks))\n",
    "\n",
    "            if len(self.hand_results) < len(self.hands):\n",
    "                self._finalize_current_hand()\n",
    "\n",
    "            reward = sum(self.hand_results)\n",
    "            done = True\n",
    "\n",
    "        return self._get_obs(), reward, done, {}\n",
    "\n",
    "    def _finalize_current_hand(self):\n",
    "        hand = self.hands[self.current_hand]\n",
    "        player_score = score(hand)\n",
    "        dealer_score = score(self.dealer)\n",
    "        result = float(player_score > dealer_score) - float(player_score < dealer_score)\n",
    "        if is_natural(hand) and result == 1 and self.natural:\n",
    "            result = 1.5\n",
    "        self.hand_results.append(result)\n",
    "        self._advance_hand()\n",
    "\n",
    "    def _advance_hand(self):\n",
    "        self.current_hand += 1\n",
    "        self.actionstaken = 0\n",
    "\n",
    "    def _get_obs(self):\n",
    "        if self.current_hand >= len(self.hands):\n",
    "            return ((0, 0), card_value(self.dealer[0]), 0)\n",
    "\n",
    "        hand = self.hands[self.current_hand]\n",
    "        padded = hand[:2] + [0] * (2 - len(hand))\n",
    "        return (\n",
    "            tuple(card_value(c) if c != 0 else 0 for c in padded[:2]),\n",
    "            card_value(self.dealer[0]),\n",
    "            int(usable_ace(hand))\n",
    "        )\n",
    "\n",
    "    def _deck_is_out(self):\n",
    "        return len(self.decks) < self.numdecks * len(CARDS) * 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5901ee",
   "metadata": {},
   "source": [
    "# Set the Simple DQN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf99e25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import copy\n",
    "import os\n",
    "\n",
    "# Define the Q-network\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "    \n",
    "def preprocess_state(state):\n",
    "    \"\"\"\n",
    "    Converts ((card1, card2), dealer_card, usable_ace) => [player_sum, dealer_card, usable_ace]\n",
    "    \"\"\"\n",
    "    player_cards, dealer_card, usable_ace = state\n",
    "    player_sum = sum(player_cards)\n",
    "    return np.array([player_sum, dealer_card, usable_ace], dtype=np.float32)\n",
    "\n",
    "# === Action Selection: Epsilon-Greedy ===\n",
    "def select_action(state, q_network, epsilon, action_space):\n",
    "    if random.random() < epsilon:\n",
    "        return action_space.sample()\n",
    "    with torch.no_grad():\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        q_values = q_network(state_tensor)\n",
    "        return q_values.argmax().item()\n",
    "    \n",
    "def select_action(state, q_network, epsilon, action_space):\n",
    "    if random.random() < epsilon:\n",
    "        return action_space.sample()\n",
    "    with torch.no_grad():\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        q_values = q_network(state_tensor)\n",
    "        return q_values.argmax().item()\n",
    "\n",
    "def train_dqn(env, n_episodes=5000, gamma=0.99, lr=1e-3, batch_size=64,\n",
    "              epsilon_start=1.0, epsilon_end=0.1, epsilon_decay=0.995,\n",
    "              model_save_path='best_blackjack_dqn.pth'):\n",
    "\n",
    "    input_dim = 3  # [player_sum, dealer_card, usable_ace]\n",
    "    output_dim = env.action_space.n\n",
    "\n",
    "    q_network = QNetwork(input_dim, output_dim)\n",
    "    optimizer = optim.Adam(q_network.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    replay_buffer = deque(maxlen=10000)\n",
    "    epsilon = epsilon_start\n",
    "    losses = []\n",
    "\n",
    "    best_model = None\n",
    "    best_avg_loss = float('inf')\n",
    "    loss_window = []\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        state = preprocess_state(state)\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = select_action(state, q_network, epsilon, env.action_space)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = preprocess_state(next_state)\n",
    "\n",
    "            replay_buffer.append((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                batch = random.sample(replay_buffer, batch_size)\n",
    "                states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "                states_tensor = torch.FloatTensor(np.array(states))\n",
    "                actions_tensor = torch.LongTensor(actions).unsqueeze(1)\n",
    "                rewards_tensor = torch.FloatTensor(rewards).unsqueeze(1)\n",
    "                next_states_tensor = torch.FloatTensor(np.array(next_states))\n",
    "                dones_tensor = torch.BoolTensor(dones).unsqueeze(1)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    next_q_values = q_network(next_states_tensor).max(1, keepdim=True)[0]\n",
    "                    targets = rewards_tensor + gamma * next_q_values * (~dones_tensor)\n",
    "\n",
    "                q_values = q_network(states_tensor).gather(1, actions_tensor)\n",
    "\n",
    "                loss = loss_fn(q_values, targets)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                losses.append(loss.item())\n",
    "                loss_window.append(loss.item())\n",
    "                if len(loss_window) > 100:\n",
    "                    loss_window.pop(0)\n",
    "\n",
    "        # Decay epsilon\n",
    "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "\n",
    "        # Save best model\n",
    "        if len(loss_window) == 100:\n",
    "            avg_loss = np.mean(loss_window)\n",
    "            if avg_loss < best_avg_loss:\n",
    "                best_avg_loss = avg_loss\n",
    "                best_model = copy.deepcopy(q_network)\n",
    "                torch.save({\n",
    "                    'model_state_dict': best_model.state_dict(),\n",
    "                    'avg_loss': best_avg_loss,\n",
    "                    'episode': episode + 1\n",
    "                }, model_save_path)\n",
    "                print(f\"✅ Best model saved at episode {episode+1} | Avg Loss: {best_avg_loss:.4f}\")\n",
    "\n",
    "        if (episode + 1) % 1000 == 0:\n",
    "            print(f\"Episode {episode+1} | Epsilon: {epsilon:.4f} | Recent Loss: {losses[-1]:.4f}\")\n",
    "\n",
    "    return best_model if best_model else q_network, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18a3743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models for different deck counts\n",
    "print(\"Training DQN models for different deck counts ===\")\n",
    "dqn_models = {}\n",
    "\n",
    "for num_decks in range(1, 7):\n",
    "    print(f\"\\n=== Training model for {num_decks} deck(s) ===\")\n",
    "    env = BlackjackEnv(numdecks=num_decks, natural=True)\n",
    "    model_save_path = f\"blackjack_dqn_decks_{num_decks}.pth\"\n",
    "    model, _ = train_dqn(env, n_episodes=10000, model_save_path=model_save_path)\n",
    "    dqn_models[num_decks] = model\n",
    "    print(f\"Completed training for {num_decks} deck(s)\")\n",
    "\n",
    "print(\"All models trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c758a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to load a single DQN model\n",
    "def load_dqn_model(num_decks):\n",
    "    \"\"\"\n",
    "    Load a single DQN model for the specified number of decks\n",
    "    \"\"\"\n",
    "    model_path = f\"blackjack_dqn_decks_{num_decks}.pth\"\n",
    "    \n",
    "    if os.path.exists(model_path):\n",
    "        # Create a new model with the correct dimensions\n",
    "        input_dim = 3  # [player_sum, dealer_card, usable_ace]\n",
    "        output_dim = 2  # [stick, hit]\n",
    "        model = QNetwork(input_dim, output_dim)\n",
    "        \n",
    "        # Load the saved weights\n",
    "        checkpoint = torch.load(model_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()  # Set to evaluation mode\n",
    "        \n",
    "        print(f\"  Successfully loaded model from {model_path}\")\n",
    "        return model\n",
    "    else:\n",
    "        print(f\"  ERROR: Model file not found at {model_path}\")\n",
    "        return None\n",
    "\n",
    "# Helper to preprocess state\n",
    "def preprocess_state(state):\n",
    "    player_cards, dealer_card, usable_ace = state\n",
    "    player_sum = sum(player_cards)\n",
    "    return np.array([player_sum, dealer_card, usable_ace], dtype=np.float32)\n",
    "\n",
    "# Evaluate a single DQN model on a specific deck size\n",
    "def evaluate_dqn_model(q_network, num_decks, num_games=10000):\n",
    "    \n",
    "    env = BlackjackEnv(numdecks=num_decks, natural=True)\n",
    "\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    draws = 0\n",
    "    total_reward = 0\n",
    "\n",
    "    for game in range(num_games):\n",
    "        obs = env.reset()\n",
    "        state = preprocess_state(obs)\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "                action = q_network(state_tensor).argmax().item()\n",
    "\n",
    "            next_obs, reward, done, _ = env.step(action)\n",
    "            state = preprocess_state(next_obs)\n",
    "            episode_reward += reward\n",
    "\n",
    "        total_reward += episode_reward\n",
    "        if episode_reward > 0:\n",
    "            wins += 1\n",
    "        elif episode_reward < 0:\n",
    "            losses += 1\n",
    "        else:\n",
    "            draws += 1\n",
    "\n",
    "    # Return results as a dictionary\n",
    "    return {\n",
    "        \"Decks\": num_decks,\n",
    "        \"Games\": num_games,\n",
    "        \"Wins\": wins,\n",
    "        \"Draws\": draws,\n",
    "        \"Losses\": losses,\n",
    "        \"Total Reward\": round(total_reward, 4),\n",
    "        \"Win Rate (%)\": round((wins / num_games) * 100, 4),\n",
    "        \"Loss Rate (%)\": round((losses / num_games) * 100, 4),\n",
    "        \"Draw Rate (%)\": round((draws / num_games) * 100, 4),\n",
    "        \"Average Reward\": round(total_reward / num_games, 4)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea5b832d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading DQN models...\n",
      "Loading model for 1 deck(s)...\n",
      "  Successfully loaded model from blackjack_dqn_decks_1.pth\n",
      "Loading model for 2 deck(s)...\n",
      "  Successfully loaded model from blackjack_dqn_decks_2.pth\n",
      "Loading model for 3 deck(s)...\n",
      "  Successfully loaded model from blackjack_dqn_decks_3.pth\n",
      "Loading model for 4 deck(s)...\n",
      "  Successfully loaded model from blackjack_dqn_decks_4.pth\n",
      "Loading model for 5 deck(s)...\n",
      "  Successfully loaded model from blackjack_dqn_decks_5.pth\n",
      "Loading model for 6 deck(s)...\n",
      "  Successfully loaded model from blackjack_dqn_decks_6.pth\n",
      "\n",
      "Evaluating DQN models...\n",
      "Evaluating model for 1 deck(s)...\n",
      "  Win Rate: 47.81%, Avg Reward: 0.0394\n",
      "Evaluating model for 2 deck(s)...\n",
      "  Win Rate: 47.49%, Avg Reward: 0.0429\n",
      "Evaluating model for 3 deck(s)...\n",
      "  Win Rate: 46.69%, Avg Reward: 0.0248\n",
      "Evaluating model for 4 deck(s)...\n",
      "  Win Rate: 47.12%, Avg Reward: 0.0328\n",
      "Evaluating model for 5 deck(s)...\n",
      "  Win Rate: 47.32%, Avg Reward: 0.0352\n",
      "Evaluating model for 6 deck(s)...\n",
      "  Win Rate: 46.74%, Avg Reward: 0.0231\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Decks</th>\n",
       "      <th>Games</th>\n",
       "      <th>Wins</th>\n",
       "      <th>Draws</th>\n",
       "      <th>Losses</th>\n",
       "      <th>Total Reward</th>\n",
       "      <th>Win Rate (%)</th>\n",
       "      <th>Loss Rate (%)</th>\n",
       "      <th>Draw Rate (%)</th>\n",
       "      <th>Average Reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>4781</td>\n",
       "      <td>608</td>\n",
       "      <td>4611</td>\n",
       "      <td>394.0</td>\n",
       "      <td>47.81</td>\n",
       "      <td>46.11</td>\n",
       "      <td>6.08</td>\n",
       "      <td>0.0394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>10000</td>\n",
       "      <td>4749</td>\n",
       "      <td>688</td>\n",
       "      <td>4563</td>\n",
       "      <td>429.0</td>\n",
       "      <td>47.49</td>\n",
       "      <td>45.63</td>\n",
       "      <td>6.88</td>\n",
       "      <td>0.0429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>10000</td>\n",
       "      <td>4669</td>\n",
       "      <td>699</td>\n",
       "      <td>4632</td>\n",
       "      <td>248.0</td>\n",
       "      <td>46.69</td>\n",
       "      <td>46.32</td>\n",
       "      <td>6.99</td>\n",
       "      <td>0.0248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>10000</td>\n",
       "      <td>4712</td>\n",
       "      <td>689</td>\n",
       "      <td>4599</td>\n",
       "      <td>328.5</td>\n",
       "      <td>47.12</td>\n",
       "      <td>45.99</td>\n",
       "      <td>6.89</td>\n",
       "      <td>0.0328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>10000</td>\n",
       "      <td>4732</td>\n",
       "      <td>653</td>\n",
       "      <td>4615</td>\n",
       "      <td>351.5</td>\n",
       "      <td>47.32</td>\n",
       "      <td>46.15</td>\n",
       "      <td>6.53</td>\n",
       "      <td>0.0352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>10000</td>\n",
       "      <td>4674</td>\n",
       "      <td>657</td>\n",
       "      <td>4669</td>\n",
       "      <td>230.5</td>\n",
       "      <td>46.74</td>\n",
       "      <td>46.69</td>\n",
       "      <td>6.57</td>\n",
       "      <td>0.0231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Decks  Games  Wins  Draws  Losses  Total Reward  Win Rate (%)  \\\n",
       "0      1  10000  4781    608    4611         394.0         47.81   \n",
       "1      2  10000  4749    688    4563         429.0         47.49   \n",
       "2      3  10000  4669    699    4632         248.0         46.69   \n",
       "3      4  10000  4712    689    4599         328.5         47.12   \n",
       "4      5  10000  4732    653    4615         351.5         47.32   \n",
       "5      6  10000  4674    657    4669         230.5         46.74   \n",
       "\n",
       "   Loss Rate (%)  Draw Rate (%)  Average Reward  \n",
       "0          46.11           6.08          0.0394  \n",
       "1          45.63           6.88          0.0429  \n",
       "2          46.32           6.99          0.0248  \n",
       "3          45.99           6.89          0.0328  \n",
       "4          46.15           6.53          0.0352  \n",
       "5          46.69           6.57          0.0231  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load models for each deck count\n",
    "print(\"\\nLoading DQN models...\")\n",
    "dqn_models = {}\n",
    "\n",
    "for num_decks in range(1, 7):\n",
    "    print(f\"Loading model for {num_decks} deck(s)...\")\n",
    "    model = load_dqn_model(num_decks)\n",
    "    if model is not None:\n",
    "        dqn_models[num_decks] = model\n",
    "\n",
    "# Evaluate each model and collect results\n",
    "print(\"\\nEvaluating DQN models...\")\n",
    "evaluation_results = []\n",
    "\n",
    "for num_decks, model in dqn_models.items():\n",
    "    print(f\"Evaluating model for {num_decks} deck(s)...\")\n",
    "    result = evaluate_dqn_model(model, num_decks, num_games=10000)\n",
    "    evaluation_results.append(result)\n",
    "    print(f\"  Win Rate: {result['Win Rate (%)']:.2f}%, Avg Reward: {result['Average Reward']:.4f}\")\n",
    "\n",
    "# Convert results to DataFrame\n",
    "df_dqn_results = pd.DataFrame(evaluation_results)\n",
    "df_dqn_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "086eada3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running DQN bankroll experiment...\n",
      "Completed simulation for 1 deck(s)\n",
      "  Win Rate: 47.60%, Final Money: $427.00\n",
      "Completed simulation for 2 deck(s)\n",
      "  Win Rate: 48.29%, Final Money: $671.00\n",
      "Completed simulation for 3 deck(s)\n",
      "  Win Rate: 47.43%, Final Money: $478.50\n",
      "Completed simulation for 4 deck(s)\n",
      "  Win Rate: 46.23%, Final Money: $263.50\n",
      "Completed simulation for 5 deck(s)\n",
      "  Win Rate: 47.31%, Final Money: $483.50\n",
      "Completed simulation for 6 deck(s)\n",
      "  Win Rate: 46.92%, Final Money: $424.50\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Decks</th>\n",
       "      <th>Games</th>\n",
       "      <th>Wins</th>\n",
       "      <th>Draws</th>\n",
       "      <th>Losses</th>\n",
       "      <th>Total Reward</th>\n",
       "      <th>Win Rate (%)</th>\n",
       "      <th>Loss Rate (%)</th>\n",
       "      <th>Draw Rate (%)</th>\n",
       "      <th>Average Reward</th>\n",
       "      <th>Final Money</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>4760</td>\n",
       "      <td>576</td>\n",
       "      <td>4664</td>\n",
       "      <td>327.0</td>\n",
       "      <td>47.60</td>\n",
       "      <td>46.64</td>\n",
       "      <td>5.76</td>\n",
       "      <td>0.0327</td>\n",
       "      <td>427.0</td>\n",
       "      <td>Solvent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>10000</td>\n",
       "      <td>4829</td>\n",
       "      <td>675</td>\n",
       "      <td>4496</td>\n",
       "      <td>571.0</td>\n",
       "      <td>48.29</td>\n",
       "      <td>44.96</td>\n",
       "      <td>6.75</td>\n",
       "      <td>0.0571</td>\n",
       "      <td>671.0</td>\n",
       "      <td>Solvent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>10000</td>\n",
       "      <td>4743</td>\n",
       "      <td>680</td>\n",
       "      <td>4577</td>\n",
       "      <td>378.5</td>\n",
       "      <td>47.43</td>\n",
       "      <td>45.77</td>\n",
       "      <td>6.80</td>\n",
       "      <td>0.0379</td>\n",
       "      <td>478.5</td>\n",
       "      <td>Solvent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>10000</td>\n",
       "      <td>4623</td>\n",
       "      <td>707</td>\n",
       "      <td>4670</td>\n",
       "      <td>163.5</td>\n",
       "      <td>46.23</td>\n",
       "      <td>46.70</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0163</td>\n",
       "      <td>263.5</td>\n",
       "      <td>Solvent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>10000</td>\n",
       "      <td>4731</td>\n",
       "      <td>698</td>\n",
       "      <td>4571</td>\n",
       "      <td>383.5</td>\n",
       "      <td>47.31</td>\n",
       "      <td>45.71</td>\n",
       "      <td>6.98</td>\n",
       "      <td>0.0384</td>\n",
       "      <td>483.5</td>\n",
       "      <td>Solvent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>10000</td>\n",
       "      <td>4692</td>\n",
       "      <td>718</td>\n",
       "      <td>4590</td>\n",
       "      <td>324.5</td>\n",
       "      <td>46.92</td>\n",
       "      <td>45.90</td>\n",
       "      <td>7.18</td>\n",
       "      <td>0.0324</td>\n",
       "      <td>424.5</td>\n",
       "      <td>Solvent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Decks  Games  Wins  Draws  Losses  Total Reward  Win Rate (%)  \\\n",
       "0      1  10000  4760    576    4664         327.0         47.60   \n",
       "1      2  10000  4829    675    4496         571.0         48.29   \n",
       "2      3  10000  4743    680    4577         378.5         47.43   \n",
       "3      4  10000  4623    707    4670         163.5         46.23   \n",
       "4      5  10000  4731    698    4571         383.5         47.31   \n",
       "5      6  10000  4692    718    4590         324.5         46.92   \n",
       "\n",
       "   Loss Rate (%)  Draw Rate (%)  Average Reward  Final Money   Status  \n",
       "0          46.64           5.76          0.0327        427.0  Solvent  \n",
       "1          44.96           6.75          0.0571        671.0  Solvent  \n",
       "2          45.77           6.80          0.0379        478.5  Solvent  \n",
       "3          46.70           7.07          0.0163        263.5  Solvent  \n",
       "4          45.71           6.98          0.0384        483.5  Solvent  \n",
       "5          45.90           7.18          0.0324        424.5  Solvent  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Helper function to preprocess state\n",
    "def preprocess_state(state):\n",
    "    player_cards, dealer_card, usable_ace = state\n",
    "    player_sum = sum(player_cards)\n",
    "    return np.array([player_sum, dealer_card, usable_ace], dtype=np.float32)\n",
    "\n",
    "# Define the bankroll evaluation function\n",
    "def evaluate_dqn_bankroll(models, num_games=10000, max_decks=6, initial_money=100):\n",
    "    \"\"\"\n",
    "    Evaluate DQN models with a bankroll simulation across different deck sizes\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for num_deck in range(1, max_decks + 1):\n",
    "        \n",
    "        env = BlackjackEnv(numdecks=num_deck, natural=True)\n",
    "        q_network = models[num_deck]  # Get the specific model for this deck size\n",
    "        q_network.eval()  # Set the model to evaluation mode\n",
    "\n",
    "        money = initial_money\n",
    "        wins = 0\n",
    "        losses = 0\n",
    "        draws = 0\n",
    "        total_reward = 0\n",
    "        \n",
    "        # For tracking bankruptcy\n",
    "        games_played = 0\n",
    "        went_bankrupt = False\n",
    "\n",
    "        for game in range(1, num_games+1):\n",
    "            if money <= 0:\n",
    "                went_bankrupt = True\n",
    "                games_played = game - 1\n",
    "                break\n",
    "                \n",
    "            games_played = game\n",
    "            obs = env.reset()\n",
    "            done = False\n",
    "            \n",
    "            # Bet $1\n",
    "            money -= 1\n",
    "            episode_reward = 0\n",
    "            doubled_down = False\n",
    "\n",
    "            while not done:\n",
    "                # Process state to match training format\n",
    "                state = preprocess_state(obs)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "                    q_values = q_network(state_tensor)\n",
    "                    \n",
    "                    # Get valid actions for the current state\n",
    "                    valid_actions = [0, 1]  # Stick, Hit are always valid\n",
    "                    \n",
    "                    # Mask invalid actions\n",
    "                    masked_q_values = q_values.clone()\n",
    "                    for i in range(q_values.size(1)):\n",
    "                        if i not in valid_actions:\n",
    "                            masked_q_values[0, i] = float('-inf')\n",
    "                    \n",
    "                    action = torch.argmax(masked_q_values, dim=1).item()\n",
    "                \n",
    "                # Check if action is valid (safeguard)\n",
    "                if action not in valid_actions:\n",
    "                    action = 0  # Default to stick if somehow invalid\n",
    "                \n",
    "                # Execute the action\n",
    "                try:\n",
    "                    next_obs, reward, done, _ = env.step(action)\n",
    "                    episode_reward += reward\n",
    "                    obs = next_obs\n",
    "                except Exception as e:\n",
    "                    # Fallback if error\n",
    "                    print(f\"Error executing action {action}: {e}\")\n",
    "                    action = 0  # Stick\n",
    "                    next_obs, reward, done, _ = env.step(action)\n",
    "                    episode_reward += reward\n",
    "                    obs = next_obs\n",
    "\n",
    "            # End of episode accounting\n",
    "            total_reward += episode_reward\n",
    "            \n",
    "            # Update wins/losses/draws and bankroll\n",
    "            if episode_reward > 0:\n",
    "                wins += 1\n",
    "                # Calculate payout\n",
    "                if doubled_down:\n",
    "                    money += 4  # Win 2x the doubled bet\n",
    "                else:\n",
    "                    if episode_reward > 1:  # Blackjack\n",
    "                        money += 2.5  # 3:2 payout\n",
    "                    else:\n",
    "                        money += 2  # Even money\n",
    "            elif episode_reward < 0:\n",
    "                losses += 1\n",
    "                # Money already subtracted for bet\n",
    "            else:\n",
    "                draws += 1\n",
    "                if doubled_down:\n",
    "                    money += 2  # Get doubled bet back\n",
    "                else:\n",
    "                    money += 1  # Get original bet back\n",
    "\n",
    "        # Store results\n",
    "        bankruptcy_message = f\"Bankrupt after {games_played} games\" if went_bankrupt else \"Solvent\"\n",
    "        \n",
    "        results.append({\n",
    "            \"Decks\": num_deck,\n",
    "            \"Games\": games_played,\n",
    "            \"Wins\": wins,\n",
    "            \"Draws\": draws,\n",
    "            \"Losses\": losses,\n",
    "            \"Total Reward\": round(total_reward, 4),\n",
    "            \"Win Rate (%)\": round((wins / games_played) * 100, 4) if games_played > 0 else 0,\n",
    "            \"Loss Rate (%)\": round((losses / games_played) * 100, 4) if games_played > 0 else 0,\n",
    "            \"Draw Rate (%)\": round((draws / games_played) * 100, 4) if games_played > 0 else 0,\n",
    "            \"Average Reward\": round(total_reward / games_played, 4) if games_played > 0 else 0,\n",
    "            \"Final Money\": round(money, 2),\n",
    "            \"Status\": bankruptcy_message\n",
    "        })\n",
    "        \n",
    "        print(f\"Completed simulation for {num_deck} deck(s)\")\n",
    "        print(f\"  Win Rate: {(wins/games_played)*100:.2f}%, Final Money: ${money:.2f}\")\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run the bankroll experiment\n",
    "print(\"\\nRunning DQN bankroll experiment...\")\n",
    "df_dqn_bankroll = evaluate_dqn_bankroll(dqn_models, num_games=10000, max_decks=6, initial_money=100)\n",
    "df_dqn_bankroll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3347af83",
   "metadata": {},
   "source": [
    "# Set PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "513ec849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# PPO Actor-Critic Network\n",
    "class PPOActorCritic(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(PPOActorCritic, self).__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.policy_head = nn.Linear(64, output_dim)\n",
    "        self.value_head = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared_out = self.shared(x)\n",
    "        logits = self.policy_head(shared_out)\n",
    "        value = self.value_head(shared_out)\n",
    "        return logits, value\n",
    "\n",
    "# Adjusted for BlackjackEnv\n",
    "def preprocess_state(state):\n",
    "    \"\"\"\n",
    "    Convert ((card1, card2), dealer_card, usable_ace) => [player_sum, dealer_card, usable_ace]\n",
    "    \"\"\"\n",
    "    player_cards, dealer_card, usable_ace = state\n",
    "    player_sum = sum(player_cards)\n",
    "    return np.array([player_sum, dealer_card, usable_ace], dtype=np.float32)\n",
    "\n",
    "# Compute GAE\n",
    "def compute_gae(rewards, values, dones, gamma=0.99, lam=0.95):\n",
    "    returns = []\n",
    "    advantages = []\n",
    "    gae = 0\n",
    "    next_value = 0\n",
    "\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * next_value * (1 - dones[step]) - values[step]\n",
    "        gae = delta + gamma * lam * (1 - dones[step]) * gae\n",
    "        advantages.insert(0, gae)\n",
    "        returns.insert(0, gae + values[step])\n",
    "        next_value = values[step]\n",
    "\n",
    "    return torch.FloatTensor(returns), torch.FloatTensor(advantages)\n",
    "\n",
    "# PPO Training Function\n",
    "def train_ppo(env, n_episodes=5000, gamma=0.99, lam=0.95, clip_eps=0.2,\n",
    "              lr=3e-4, epochs=4, batch_size=64, model_save_path='ppo_blackjack.pth'):\n",
    "\n",
    "    input_dim = 3\n",
    "    output_dim = env.action_space.n\n",
    "\n",
    "    policy_net = PPOActorCritic(input_dim, output_dim)\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "\n",
    "    memory = []\n",
    "\n",
    "    best_reward = float('-inf')\n",
    "    best_model = None\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        obs = env.reset()\n",
    "        state = preprocess_state(obs)\n",
    "        done = False\n",
    "        episode_data = []\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            state_tensor = torch.FloatTensor(state)\n",
    "            logits, value = policy_net(state_tensor)\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            dist = torch.distributions.Categorical(probs)\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action)\n",
    "\n",
    "            next_obs, reward, done, _ = env.step(action.item())\n",
    "            next_state = preprocess_state(next_obs)\n",
    "\n",
    "            episode_data.append((state, action.item(), reward, log_prob.item(), value.item(), done))\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        memory.extend(episode_data)\n",
    "\n",
    "        if len(memory) >= batch_size:\n",
    "            states, actions, rewards, old_log_probs, values, dones = zip(*memory)\n",
    "\n",
    "            returns, advantages = compute_gae(rewards, values, dones, gamma, lam)\n",
    "\n",
    "            states_tensor = torch.FloatTensor(np.array(states))\n",
    "            actions_tensor = torch.LongTensor(actions)\n",
    "            old_log_probs_tensor = torch.FloatTensor(old_log_probs)\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "            for _ in range(epochs):\n",
    "                logits, value_preds = policy_net(states_tensor)\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                dist = torch.distributions.Categorical(probs)\n",
    "\n",
    "                new_log_probs = dist.log_prob(actions_tensor)\n",
    "                ratio = torch.exp(new_log_probs - old_log_probs_tensor)\n",
    "\n",
    "                policy_loss = -torch.min(\n",
    "                    ratio * advantages,\n",
    "                    torch.clamp(ratio, 1 - clip_eps, 1 + clip_eps) * advantages\n",
    "                ).mean()\n",
    "\n",
    "                value_loss = nn.MSELoss()(value_preds.squeeze(), returns)\n",
    "\n",
    "                loss = policy_loss + 0.5 * value_loss - 0.01 * dist.entropy().mean()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            if episode_reward > best_reward:\n",
    "                best_reward = episode_reward\n",
    "                best_model = policy_net\n",
    "                torch.save({\n",
    "                    'model_state_dict': best_model.state_dict(),\n",
    "                    'avg_reward': best_reward,\n",
    "                    'episode': episode + 1\n",
    "                }, model_save_path)\n",
    "                print(f\"✅ Best model saved at episode {episode+1} | Episode Reward: {best_reward:.2f}\")\n",
    "\n",
    "            memory = []\n",
    "\n",
    "        if (episode + 1) % 1000 == 0:\n",
    "            print(f\"Episode {episode+1} | Last Episode Reward: {episode_reward:.2f}\")\n",
    "\n",
    "    return best_model if best_model else policy_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66dc1c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Best model saved at episode 43 | Episode Reward: -1.00\n",
      "✅ Best model saved at episode 87 | Episode Reward: 1.00\n",
      "✅ Best model saved at episode 348 | Episode Reward: 1.50\n",
      "Episode 1000 | Last Episode Reward: -1.00\n",
      "Episode 2000 | Last Episode Reward: 1.00\n",
      "Episode 3000 | Last Episode Reward: 1.00\n",
      "Episode 4000 | Last Episode Reward: -1.00\n",
      "Episode 5000 | Last Episode Reward: -1.00\n",
      "Episode 6000 | Last Episode Reward: 1.00\n",
      "Episode 7000 | Last Episode Reward: 1.00\n",
      "Episode 8000 | Last Episode Reward: -1.00\n",
      "Episode 9000 | Last Episode Reward: -1.00\n",
      "Episode 10000 | Last Episode Reward: 1.00\n"
     ]
    }
   ],
   "source": [
    "env = BlackjackEnv(numdecks=6, natural=True)\n",
    "ppo_model = train_ppo(env, n_episodes=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032d8df8",
   "metadata": {},
   "source": [
    "# Set Card Counting Strategy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study_venvs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
