{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "567f0958",
   "metadata": {},
   "source": [
    "# Set Environment (No Split, No Double Down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "071c5f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "import random\n",
    "\n",
    "# Full deck with distinct face cards\n",
    "CARDS = [1, 2, 3, 4, 5, 6, 7, 8, 9, '10', 'J', 'Q', 'K'] * 4\n",
    "\n",
    "def card_value(card):\n",
    "    return 10 if card in ['10', 'J', 'Q', 'K'] else card\n",
    "\n",
    "def draw_card(deck):\n",
    "    return deck.pop()\n",
    "\n",
    "def draw_hand(deck):\n",
    "    return [draw_card(deck), draw_card(deck)]\n",
    "\n",
    "def usable_ace(hand):\n",
    "    return 1 in hand and sum(card_value(c) for c in hand) + 10 <= 21\n",
    "\n",
    "def sum_hand(hand):\n",
    "    total = sum(card_value(c) for c in hand)\n",
    "    return total + 10 if usable_ace(hand) else total\n",
    "\n",
    "def is_bust(hand):\n",
    "    return sum_hand(hand) > 21\n",
    "\n",
    "def score(hand):\n",
    "    return 0 if is_bust(hand) else sum_hand(hand)\n",
    "\n",
    "def is_natural(hand):\n",
    "    return set(hand) == {1, '10'} or set(hand) == {1, 'J'} or set(hand) == {1, 'Q'} or set(hand) == {1, 'K'}\n",
    "\n",
    "class BlackjackEnv(gym.Env):\n",
    "    metadata = {\"render.modes\": [\"human\"]}\n",
    "\n",
    "    def __init__(self, numdecks=4, natural=True):\n",
    "        super().__init__()\n",
    "        self.action_space = spaces.Discrete(2)  # 0: Stick, 1: Hit\n",
    "        self.observation_space = spaces.Tuple((\n",
    "            spaces.Tuple((spaces.Discrete(32), spaces.Discrete(32))),  # Player hand (2 cards)\n",
    "            spaces.Discrete(11),  # Dealer's showing card\n",
    "            spaces.Discrete(2)    # Usable ace\n",
    "        ))\n",
    "\n",
    "        self.natural = natural\n",
    "        self.numdecks = numdecks\n",
    "        self.decks = CARDS * self.numdecks\n",
    "        random.shuffle(self.decks)\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        random.seed(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        if seed is not None:\n",
    "            self.seed(seed)\n",
    "\n",
    "        if self._deck_is_out():\n",
    "            self.decks = CARDS * self.numdecks\n",
    "            random.shuffle(self.decks)\n",
    "\n",
    "        self.dealer = draw_hand(self.decks)\n",
    "        first_hand = draw_hand(self.decks)\n",
    "        self.hands = [first_hand]\n",
    "        self.current_hand = 0\n",
    "        self.actionstaken = 0\n",
    "        self.hand_results = []\n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action), f\"Invalid action: {action}\"\n",
    "        if self._deck_is_out():\n",
    "            self.decks = CARDS * self.numdecks\n",
    "            random.shuffle(self.decks)\n",
    "\n",
    "        done = False\n",
    "        reward = 0\n",
    "        hand = self.hands[self.current_hand]\n",
    "\n",
    "        if action == 0:  # Stick\n",
    "            self._finalize_current_hand()\n",
    "\n",
    "        elif action == 1:  # Hit\n",
    "            hand.append(draw_card(self.decks))\n",
    "            if is_bust(hand):\n",
    "                self.hand_results.append(-1)\n",
    "                self._advance_hand()\n",
    "\n",
    "        self.actionstaken += 1\n",
    "\n",
    "        if self.current_hand >= len(self.hands):\n",
    "            while sum_hand(self.dealer) < 17:\n",
    "                self.dealer.append(draw_card(self.decks))\n",
    "\n",
    "            if len(self.hand_results) < len(self.hands):\n",
    "                self._finalize_current_hand()\n",
    "\n",
    "            reward = sum(self.hand_results)\n",
    "            done = True\n",
    "\n",
    "        return self._get_obs(), reward, done, {}\n",
    "\n",
    "    def _finalize_current_hand(self):\n",
    "        hand = self.hands[self.current_hand]\n",
    "        player_score = score(hand)\n",
    "        dealer_score = score(self.dealer)\n",
    "        result = float(player_score > dealer_score) - float(player_score < dealer_score)\n",
    "        if is_natural(hand) and result == 1 and self.natural:\n",
    "            result = 1.5\n",
    "        self.hand_results.append(result)\n",
    "        self._advance_hand()\n",
    "\n",
    "    def _advance_hand(self):\n",
    "        self.current_hand += 1\n",
    "        self.actionstaken = 0\n",
    "\n",
    "    def _get_obs(self):\n",
    "        if self.current_hand >= len(self.hands):\n",
    "            return ((0, 0), card_value(self.dealer[0]), 0)\n",
    "\n",
    "        hand = self.hands[self.current_hand]\n",
    "        padded = hand[:2] + [0] * (2 - len(hand))\n",
    "        return (\n",
    "            tuple(card_value(c) if c != 0 else 0 for c in padded[:2]),\n",
    "            card_value(self.dealer[0]),\n",
    "            int(usable_ace(hand))\n",
    "        )\n",
    "\n",
    "    def _deck_is_out(self):\n",
    "        return len(self.decks) < self.numdecks * len(CARDS) * 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5901ee",
   "metadata": {},
   "source": [
    "# Set the Simple DQN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf99e25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import copy\n",
    "import os\n",
    "\n",
    "# Define the Q-network\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "        nn.Linear(input_dim, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, output_dim)\n",
    "    )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "    \n",
    "def preprocess_state(state):\n",
    "    player_cards, dealer_card, usable_ace = state\n",
    "    player_sum = sum(player_cards)\n",
    "    return np.array([\n",
    "        (player_sum - 4) / 17.0,     # Normalize to [0, 1]\n",
    "        (dealer_card - 1) / 9.0,     # Normalize to [0, 1]\n",
    "        usable_ace\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "# === Action Selection: Epsilon-Greedy ===\n",
    "def select_action(state, q_network, epsilon, action_space):\n",
    "    if random.random() < epsilon:\n",
    "        return action_space.sample()\n",
    "    with torch.no_grad():\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        q_values = q_network(state_tensor)\n",
    "        return q_values.argmax().item()\n",
    "    \n",
    "def select_action(state, q_network, epsilon, action_space):\n",
    "    if random.random() < epsilon:\n",
    "        return action_space.sample()\n",
    "    with torch.no_grad():\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        q_values = q_network(state_tensor)\n",
    "        return q_values.argmax().item()\n",
    "\n",
    "def train_dqn(env, n_episodes=5000, gamma=0.99, lr=1e-3, batch_size=64,\n",
    "              epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995,\n",
    "              model_save_path='best_blackjack_dqn.pth'):\n",
    "\n",
    "    input_dim = 3  # [normalized player_sum, dealer_card, usable_ace]\n",
    "    output_dim = env.action_space.n\n",
    "\n",
    "    q_network = QNetwork(input_dim, output_dim)\n",
    "    target_network = copy.deepcopy(q_network)\n",
    "    target_network.eval()\n",
    "\n",
    "    optimizer = optim.Adam(q_network.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    replay_buffer = deque(maxlen=10000)\n",
    "    epsilon = epsilon_start\n",
    "    losses = []\n",
    "    total_rewards = []\n",
    "\n",
    "    best_model = None\n",
    "    best_avg_loss = float('inf')\n",
    "    loss_window = []\n",
    "\n",
    "    steps_done = 0\n",
    "    target_update_freq = 1000  # in steps\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        state = preprocess_state(env.reset())\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = select_action(state, q_network, epsilon, env.action_space)\n",
    "            next_state_raw, reward, done, _ = env.step(action)\n",
    "            next_state = preprocess_state(next_state_raw)\n",
    "            reward = np.clip(reward, -1.0, 1.0)\n",
    "\n",
    "            replay_buffer.append((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps_done += 1\n",
    "\n",
    "            # Learn if enough samples\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                batch = random.sample(replay_buffer, batch_size)\n",
    "                states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "                states_tensor = torch.FloatTensor(np.array(states))\n",
    "                actions_tensor = torch.LongTensor(actions).unsqueeze(1)\n",
    "                rewards_tensor = torch.FloatTensor(rewards).unsqueeze(1)\n",
    "                next_states_tensor = torch.FloatTensor(np.array(next_states))\n",
    "                dones_tensor = torch.BoolTensor(dones).unsqueeze(1)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    next_q_values = target_network(next_states_tensor).max(1, keepdim=True)[0]\n",
    "                    targets = rewards_tensor + gamma * next_q_values * (~dones_tensor)\n",
    "\n",
    "                q_values = q_network(states_tensor).gather(1, actions_tensor)\n",
    "\n",
    "                loss = loss_fn(q_values, targets)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                losses.append(loss.item())\n",
    "                loss_window.append(loss.item())\n",
    "                if len(loss_window) > 100:\n",
    "                    loss_window.pop(0)\n",
    "\n",
    "            # Update target network\n",
    "            if steps_done % target_update_freq == 0:\n",
    "                target_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "        # Epsilon decay\n",
    "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "        total_rewards.append(total_reward)\n",
    "\n",
    "        # Save best model based on average loss\n",
    "        if len(loss_window) == 100:\n",
    "            avg_loss = np.mean(loss_window)\n",
    "            if avg_loss < best_avg_loss:\n",
    "                best_avg_loss = avg_loss\n",
    "                best_model = copy.deepcopy(q_network)\n",
    "                torch.save({\n",
    "                    'model_state_dict': best_model.state_dict(),\n",
    "                    'avg_loss': best_avg_loss,\n",
    "                    'episode': episode + 1\n",
    "                }, model_save_path)\n",
    "                # print(f\"✅ Best model saved at episode {episode+1} | Avg Loss: {best_avg_loss:.4f}\")\n",
    "\n",
    "    checkpoint = torch.load(model_save_path)\n",
    "    # model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    # model.eval()\n",
    "    print(f\"Best Model at {checkpoint.get('episode', 'N/A')} | Avg Loss: {checkpoint.get('avg_loss', 'N/A')}\")\n",
    "\n",
    "    return best_model if best_model else q_network, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b18a3743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DQN models for different deck counts ===\n",
      "\n",
      "=== Training model for 1 deck(s) ===\n",
      "Best Model at 3211 | Avg Loss: 0.299788009673357\n",
      "Completed training for 1 deck(s)\n",
      "\n",
      "=== Training model for 2 deck(s) ===\n",
      "Best Model at 41036 | Avg Loss: 0.3109839116036892\n",
      "Completed training for 2 deck(s)\n",
      "\n",
      "=== Training model for 3 deck(s) ===\n",
      "Best Model at 3544 | Avg Loss: 0.3034180237352848\n",
      "Completed training for 3 deck(s)\n",
      "\n",
      "=== Training model for 4 deck(s) ===\n",
      "Best Model at 998 | Avg Loss: 0.3201422664523125\n",
      "Completed training for 4 deck(s)\n",
      "\n",
      "=== Training model for 5 deck(s) ===\n",
      "Best Model at 49395 | Avg Loss: 0.3064042545855045\n",
      "Completed training for 5 deck(s)\n",
      "\n",
      "=== Training model for 6 deck(s) ===\n",
      "Best Model at 27321 | Avg Loss: 0.3119866617023945\n",
      "Completed training for 6 deck(s)\n",
      "All models trained successfully!\n"
     ]
    }
   ],
   "source": [
    "# Train models for different deck counts\n",
    "print(\"Training DQN models for different deck counts ===\")\n",
    "dqn_models = {}\n",
    "\n",
    "for num_decks in range(1, 7):\n",
    "    print(f\"\\n=== Training model for {num_decks} deck(s) ===\")\n",
    "    env = BlackjackEnv(numdecks=num_decks, natural=False)\n",
    "    model_save_path = f\"blackjack_dqn_decks_{num_decks}.pth\"\n",
    "    model, _ = train_dqn(env, n_episodes=50000, model_save_path=model_save_path)\n",
    "    dqn_models[num_decks] = model\n",
    "    print(f\"Completed training for {num_decks} deck(s)\")\n",
    "\n",
    "print(\"All models trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c758a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading DQN models...\n",
      "Loading model for 1 deck(s)...\n",
      "  Successfully loaded model from blackjack_dqn_decks_1.pth\n",
      "Loading model for 2 deck(s)...\n",
      "  Successfully loaded model from blackjack_dqn_decks_2.pth\n",
      "Loading model for 3 deck(s)...\n",
      "  Successfully loaded model from blackjack_dqn_decks_3.pth\n",
      "Loading model for 4 deck(s)...\n",
      "  Successfully loaded model from blackjack_dqn_decks_4.pth\n",
      "Loading model for 5 deck(s)...\n",
      "  Successfully loaded model from blackjack_dqn_decks_5.pth\n",
      "Loading model for 6 deck(s)...\n",
      "  Successfully loaded model from blackjack_dqn_decks_6.pth\n",
      "\n",
      "Evaluating DQN models...\n",
      "Evaluating model for 1 deck(s)...\n",
      "  Win Rate: 46.18%, Avg Reward: -0.0096\n",
      "Evaluating model for 2 deck(s)...\n",
      "  Win Rate: 47.31%, Avg Reward: 0.0149\n",
      "Evaluating model for 3 deck(s)...\n",
      "  Win Rate: 47.57%, Avg Reward: 0.0219\n",
      "Evaluating model for 4 deck(s)...\n",
      "  Win Rate: 46.73%, Avg Reward: 0.0014\n",
      "Evaluating model for 5 deck(s)...\n",
      "  Win Rate: 44.73%, Avg Reward: -0.0306\n",
      "Evaluating model for 6 deck(s)...\n",
      "  Win Rate: 46.07%, Avg Reward: -0.0089\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Decks</th>\n",
       "      <th>Games</th>\n",
       "      <th>Wins</th>\n",
       "      <th>Draws</th>\n",
       "      <th>Losses</th>\n",
       "      <th>Total Reward</th>\n",
       "      <th>Win Rate (%)</th>\n",
       "      <th>Loss Rate (%)</th>\n",
       "      <th>Draw Rate (%)</th>\n",
       "      <th>Average Reward</th>\n",
       "      <th>Natural Player</th>\n",
       "      <th>Natural Dealer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>4618</td>\n",
       "      <td>668</td>\n",
       "      <td>4714</td>\n",
       "      <td>-96.0</td>\n",
       "      <td>46.18</td>\n",
       "      <td>47.14</td>\n",
       "      <td>6.68</td>\n",
       "      <td>-0.0096</td>\n",
       "      <td>465</td>\n",
       "      <td>497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>10000</td>\n",
       "      <td>4731</td>\n",
       "      <td>687</td>\n",
       "      <td>4582</td>\n",
       "      <td>149.0</td>\n",
       "      <td>47.31</td>\n",
       "      <td>45.82</td>\n",
       "      <td>6.87</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>479</td>\n",
       "      <td>433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>10000</td>\n",
       "      <td>4757</td>\n",
       "      <td>705</td>\n",
       "      <td>4538</td>\n",
       "      <td>219.0</td>\n",
       "      <td>47.57</td>\n",
       "      <td>45.38</td>\n",
       "      <td>7.05</td>\n",
       "      <td>0.0219</td>\n",
       "      <td>501</td>\n",
       "      <td>450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>10000</td>\n",
       "      <td>4673</td>\n",
       "      <td>668</td>\n",
       "      <td>4659</td>\n",
       "      <td>14.0</td>\n",
       "      <td>46.73</td>\n",
       "      <td>46.59</td>\n",
       "      <td>6.68</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>473</td>\n",
       "      <td>474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>10000</td>\n",
       "      <td>4473</td>\n",
       "      <td>748</td>\n",
       "      <td>4779</td>\n",
       "      <td>-306.0</td>\n",
       "      <td>44.73</td>\n",
       "      <td>47.79</td>\n",
       "      <td>7.48</td>\n",
       "      <td>-0.0306</td>\n",
       "      <td>457</td>\n",
       "      <td>487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>10000</td>\n",
       "      <td>4607</td>\n",
       "      <td>697</td>\n",
       "      <td>4696</td>\n",
       "      <td>-89.0</td>\n",
       "      <td>46.07</td>\n",
       "      <td>46.96</td>\n",
       "      <td>6.97</td>\n",
       "      <td>-0.0089</td>\n",
       "      <td>481</td>\n",
       "      <td>499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Decks  Games  Wins  Draws  Losses  Total Reward  Win Rate (%)  \\\n",
       "0      1  10000  4618    668    4714         -96.0         46.18   \n",
       "1      2  10000  4731    687    4582         149.0         47.31   \n",
       "2      3  10000  4757    705    4538         219.0         47.57   \n",
       "3      4  10000  4673    668    4659          14.0         46.73   \n",
       "4      5  10000  4473    748    4779        -306.0         44.73   \n",
       "5      6  10000  4607    697    4696         -89.0         46.07   \n",
       "\n",
       "   Loss Rate (%)  Draw Rate (%)  Average Reward  Natural Player  \\\n",
       "0          47.14           6.68         -0.0096             465   \n",
       "1          45.82           6.87          0.0149             479   \n",
       "2          45.38           7.05          0.0219             501   \n",
       "3          46.59           6.68          0.0014             473   \n",
       "4          47.79           7.48         -0.0306             457   \n",
       "5          46.96           6.97         -0.0089             481   \n",
       "\n",
       "   Natural Dealer  \n",
       "0             497  \n",
       "1             433  \n",
       "2             450  \n",
       "3             474  \n",
       "4             487  \n",
       "5             499  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to load a single DQN model\n",
    "def load_dqn_model(num_decks):\n",
    "    \"\"\"\n",
    "    Load a single DQN model for the specified number of decks\n",
    "    \"\"\"\n",
    "    model_path = f\"blackjack_dqn_decks_{num_decks}.pth\"\n",
    "    \n",
    "    if os.path.exists(model_path):\n",
    "        # Create a new model with the correct dimensions\n",
    "        input_dim = 3  # [player_sum, dealer_card, usable_ace]\n",
    "        output_dim = 2  # [stick, hit]\n",
    "        model = QNetwork(input_dim, output_dim)\n",
    "        \n",
    "        # Load the saved weights\n",
    "        checkpoint = torch.load(model_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()  # Set to evaluation mode\n",
    "        \n",
    "        print(f\"  Successfully loaded model from {model_path}\")\n",
    "        return model\n",
    "    else:\n",
    "        print(f\"  ERROR: Model file not found at {model_path}\")\n",
    "        return None\n",
    "\n",
    "# Helper to preprocess state\n",
    "def preprocess_state(state):\n",
    "    player_cards, dealer_card, usable_ace = state\n",
    "    player_sum = sum(player_cards)\n",
    "    return np.array([player_sum, dealer_card, usable_ace], dtype=np.float32)\n",
    "\n",
    "# Evaluate a single DQN model on a specific deck size\n",
    "def evaluate_dqn_model(q_network, num_decks, num_games=10000):\n",
    "    \n",
    "    env = BlackjackEnv(numdecks=num_decks, natural=False)\n",
    "\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    draws = 0\n",
    "    total_reward = 0\n",
    "    count_natural_player = 0\n",
    "    count_natural_dealer = 0\n",
    "\n",
    "    for game in range(1, num_games+1):\n",
    "        obs = env.reset(seed=game)\n",
    "        state = preprocess_state(obs)\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "                action = q_network(state_tensor).argmax().item()\n",
    "\n",
    "            next_obs, reward, done, _ = env.step(action)\n",
    "            if is_natural(env.hands[0]):\n",
    "                count_natural_player += 1\n",
    "            if is_natural(env.dealer):\n",
    "                count_natural_dealer += 1\n",
    "            state = preprocess_state(next_obs)\n",
    "            episode_reward += reward\n",
    "\n",
    "        total_reward += episode_reward\n",
    "        if episode_reward > 0:\n",
    "            wins += 1\n",
    "        elif episode_reward < 0:\n",
    "            losses += 1\n",
    "        else:\n",
    "            draws += 1\n",
    "\n",
    "    # Return results as a dictionary\n",
    "    return {\n",
    "        \"Decks\": num_decks,\n",
    "        \"Games\": num_games,\n",
    "        \"Wins\": wins,\n",
    "        \"Draws\": draws,\n",
    "        \"Losses\": losses,\n",
    "        \"Total Reward\": round(total_reward, 4),\n",
    "        \"Win Rate (%)\": round((wins / num_games) * 100, 4),\n",
    "        \"Loss Rate (%)\": round((losses / num_games) * 100, 4),\n",
    "        \"Draw Rate (%)\": round((draws / num_games) * 100, 4),\n",
    "        \"Average Reward\": round(total_reward / num_games, 4),\n",
    "        \"Natural Player\": count_natural_player,\n",
    "        \"Natural Dealer\": count_natural_dealer,\n",
    "    }\n",
    "    \n",
    "# Load models for each deck count\n",
    "print(\"\\nLoading DQN models...\")\n",
    "dqn_models = {}\n",
    "\n",
    "for num_decks in range(1, 7):\n",
    "    print(f\"Loading model for {num_decks} deck(s)...\")\n",
    "    model = load_dqn_model(num_decks)\n",
    "    if model is not None:\n",
    "        dqn_models[num_decks] = model\n",
    "\n",
    "# Evaluate each model and collect results\n",
    "print(\"\\nEvaluating DQN models...\")\n",
    "evaluation_results = []\n",
    "\n",
    "for num_decks, model in dqn_models.items():\n",
    "    print(f\"Evaluating model for {num_decks} deck(s)...\")\n",
    "    result = evaluate_dqn_model(model, num_decks, num_games=10000)\n",
    "    evaluation_results.append(result)\n",
    "    print(f\"  Win Rate: {result['Win Rate (%)']:.2f}%, Avg Reward: {result['Average Reward']:.4f}\")\n",
    "\n",
    "# Convert results to DataFrame\n",
    "df_dqn_results = pd.DataFrame(evaluation_results)\n",
    "df_dqn_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02589ef1",
   "metadata": {},
   "source": [
    "## Evaluation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "086eada3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running DQN bankroll experiment...\n",
      "Completed simulation for 1 deck(s)\n",
      "  Win Rate: 46.15%, Final Money: $0.00\n",
      "Completed simulation for 2 deck(s)\n",
      "  Win Rate: 47.31%, Final Money: $250.00\n",
      "Completed simulation for 3 deck(s)\n",
      "  Win Rate: 47.57%, Final Money: $319.00\n",
      "Completed simulation for 4 deck(s)\n",
      "  Win Rate: 46.73%, Final Money: $114.00\n",
      "Completed simulation for 5 deck(s)\n",
      "  Win Rate: 44.93%, Final Money: $0.00\n",
      "Completed simulation for 6 deck(s)\n",
      "  Win Rate: 46.50%, Final Money: $91.00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Decks</th>\n",
       "      <th>Games</th>\n",
       "      <th>Wins</th>\n",
       "      <th>Draws</th>\n",
       "      <th>Losses</th>\n",
       "      <th>Total Reward</th>\n",
       "      <th>Win Rate (%)</th>\n",
       "      <th>Loss Rate (%)</th>\n",
       "      <th>Draw Rate (%)</th>\n",
       "      <th>Average Reward</th>\n",
       "      <th>Final Money</th>\n",
       "      <th>Natural Player</th>\n",
       "      <th>Natural Dealer</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>9166</td>\n",
       "      <td>4230</td>\n",
       "      <td>606</td>\n",
       "      <td>4330</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>46.1488</td>\n",
       "      <td>47.2398</td>\n",
       "      <td>6.6114</td>\n",
       "      <td>-0.0109</td>\n",
       "      <td>0</td>\n",
       "      <td>420</td>\n",
       "      <td>451</td>\n",
       "      <td>Bankrupt after 9166 games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>10000</td>\n",
       "      <td>4731</td>\n",
       "      <td>688</td>\n",
       "      <td>4581</td>\n",
       "      <td>150.0</td>\n",
       "      <td>47.3100</td>\n",
       "      <td>45.8100</td>\n",
       "      <td>6.8800</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>250</td>\n",
       "      <td>481</td>\n",
       "      <td>433</td>\n",
       "      <td>Solvent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>10000</td>\n",
       "      <td>4757</td>\n",
       "      <td>705</td>\n",
       "      <td>4538</td>\n",
       "      <td>219.0</td>\n",
       "      <td>47.5700</td>\n",
       "      <td>45.3800</td>\n",
       "      <td>7.0500</td>\n",
       "      <td>0.0219</td>\n",
       "      <td>319</td>\n",
       "      <td>501</td>\n",
       "      <td>450</td>\n",
       "      <td>Solvent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>10000</td>\n",
       "      <td>4673</td>\n",
       "      <td>668</td>\n",
       "      <td>4659</td>\n",
       "      <td>14.0</td>\n",
       "      <td>46.7300</td>\n",
       "      <td>46.5900</td>\n",
       "      <td>6.6800</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>114</td>\n",
       "      <td>473</td>\n",
       "      <td>474</td>\n",
       "      <td>Solvent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3009</td>\n",
       "      <td>1352</td>\n",
       "      <td>205</td>\n",
       "      <td>1452</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>44.9319</td>\n",
       "      <td>48.2552</td>\n",
       "      <td>6.8129</td>\n",
       "      <td>-0.0332</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>140</td>\n",
       "      <td>Bankrupt after 3009 games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>10000</td>\n",
       "      <td>4650</td>\n",
       "      <td>691</td>\n",
       "      <td>4659</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>46.5000</td>\n",
       "      <td>46.5900</td>\n",
       "      <td>6.9100</td>\n",
       "      <td>-0.0009</td>\n",
       "      <td>91</td>\n",
       "      <td>468</td>\n",
       "      <td>501</td>\n",
       "      <td>Solvent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Decks  Games  Wins  Draws  Losses  Total Reward  Win Rate (%)  \\\n",
       "0      1   9166  4230    606    4330        -100.0       46.1488   \n",
       "1      2  10000  4731    688    4581         150.0       47.3100   \n",
       "2      3  10000  4757    705    4538         219.0       47.5700   \n",
       "3      4  10000  4673    668    4659          14.0       46.7300   \n",
       "4      5   3009  1352    205    1452        -100.0       44.9319   \n",
       "5      6  10000  4650    691    4659          -9.0       46.5000   \n",
       "\n",
       "   Loss Rate (%)  Draw Rate (%)  Average Reward  Final Money  Natural Player  \\\n",
       "0        47.2398         6.6114         -0.0109            0             420   \n",
       "1        45.8100         6.8800          0.0150          250             481   \n",
       "2        45.3800         7.0500          0.0219          319             501   \n",
       "3        46.5900         6.6800          0.0014          114             473   \n",
       "4        48.2552         6.8129         -0.0332            0             140   \n",
       "5        46.5900         6.9100         -0.0009           91             468   \n",
       "\n",
       "   Natural Dealer                     Status  \n",
       "0             451  Bankrupt after 9166 games  \n",
       "1             433                    Solvent  \n",
       "2             450                    Solvent  \n",
       "3             474                    Solvent  \n",
       "4             140  Bankrupt after 3009 games  \n",
       "5             501                    Solvent  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Helper function to preprocess state\n",
    "def preprocess_state(state):\n",
    "    player_cards, dealer_card, usable_ace = state\n",
    "    player_sum = sum(player_cards)\n",
    "    return np.array([player_sum, dealer_card, usable_ace], dtype=np.float32)\n",
    "\n",
    "# Define the bankroll evaluation function\n",
    "def evaluate_dqn_bankroll(models, num_games=10000, max_decks=6, initial_money=100):\n",
    "    \"\"\"\n",
    "    Evaluate DQN models with a bankroll simulation across different deck sizes\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for num_deck in range(1, max_decks + 1):\n",
    "        \n",
    "        env = BlackjackEnv(numdecks=num_deck, natural=False)\n",
    "        q_network = models[num_deck]  # Get the specific model for this deck size\n",
    "        q_network.eval()  # Set the model to evaluation mode\n",
    "\n",
    "        money = initial_money\n",
    "        wins = 0\n",
    "        losses = 0\n",
    "        draws = 0\n",
    "        total_reward = 0\n",
    "        count_natural_player = 0\n",
    "        count_natural_dealer = 0\n",
    "        \n",
    "        # For tracking bankruptcy\n",
    "        games_played = 0\n",
    "        went_bankrupt = False\n",
    "\n",
    "        for game in range(1, num_games+1):\n",
    "            if money <= 0:\n",
    "                went_bankrupt = True\n",
    "                games_played = game - 1\n",
    "                break\n",
    "                \n",
    "            games_played = game\n",
    "            obs = env.reset(seed=game)\n",
    "            done = False\n",
    "            \n",
    "            # Bet $1\n",
    "            money -= 1\n",
    "            episode_reward = 0\n",
    "            doubled_down = False\n",
    "\n",
    "            while not done:\n",
    "                # Process state to match training format\n",
    "                state = preprocess_state(obs)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "                    q_values = q_network(state_tensor)\n",
    "                    \n",
    "                    # Get valid actions for the current state\n",
    "                    valid_actions = [0, 1]  # Stick, Hit are always valid\n",
    "                    \n",
    "                    # Mask invalid actions\n",
    "                    masked_q_values = q_values.clone()\n",
    "                    for i in range(q_values.size(1)):\n",
    "                        if i not in valid_actions:\n",
    "                            masked_q_values[0, i] = float('-inf')\n",
    "                    \n",
    "                    action = torch.argmax(masked_q_values, dim=1).item()\n",
    "                \n",
    "                # Check if action is valid (safeguard)\n",
    "                if action not in valid_actions:\n",
    "                    action = 0  # Default to stick if somehow invalid\n",
    "                \n",
    "                # Execute the action\n",
    "                try:\n",
    "                    next_obs, reward, done, _ = env.step(action)\n",
    "                    if is_natural(env.hands[0]):\n",
    "                        count_natural_player += 1\n",
    "                    if is_natural(env.dealer):\n",
    "                        count_natural_dealer += 1\n",
    "                    episode_reward += reward\n",
    "                    obs = next_obs\n",
    "                except Exception as e:\n",
    "                    # Fallback if error\n",
    "                    print(f\"Error executing action {action}: {e}\")\n",
    "                    action = 0  # Stick\n",
    "                    next_obs, reward, done, _ = env.step(action)\n",
    "                    \n",
    "                    episode_reward += reward\n",
    "                    obs = next_obs\n",
    "\n",
    "                if done:\n",
    "                    total_reward += episode_reward\n",
    "                    \n",
    "                    if episode_reward > 0:\n",
    "                        wins += 1\n",
    "                        money += 2\n",
    "                    elif episode_reward < 0:\n",
    "                        losses += 1\n",
    "                    else:\n",
    "                        draws += 1\n",
    "                        money += 1\n",
    "\n",
    "        # Store results\n",
    "        bankruptcy_message = f\"Bankrupt after {games_played} games\" if went_bankrupt else \"Solvent\"\n",
    "        \n",
    "        results.append({\n",
    "            \"Decks\": num_deck,\n",
    "            \"Games\": games_played,\n",
    "            \"Wins\": wins,\n",
    "            \"Draws\": draws,\n",
    "            \"Losses\": losses,\n",
    "            \"Total Reward\": round(total_reward, 4),\n",
    "            \"Win Rate (%)\": round((wins / games_played) * 100, 4) if games_played > 0 else 0,\n",
    "            \"Loss Rate (%)\": round((losses / games_played) * 100, 4) if games_played > 0 else 0,\n",
    "            \"Draw Rate (%)\": round((draws / games_played) * 100, 4) if games_played > 0 else 0,\n",
    "            \"Average Reward\": round(total_reward / games_played, 4) if games_played > 0 else 0,\n",
    "            \"Final Money\": round(money, 2),\n",
    "            \"Natural Player\": count_natural_player,\n",
    "            \"Natural Dealer\": count_natural_dealer,\n",
    "            \"Status\": bankruptcy_message\n",
    "        })\n",
    "        \n",
    "        print(f\"Completed simulation for {num_deck} deck(s)\")\n",
    "        print(f\"  Win Rate: {(wins/games_played)*100:.2f}%, Final Money: ${money:.2f}\")\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run the bankroll experiment\n",
    "print(\"\\nRunning DQN bankroll experiment...\")\n",
    "df_dqn_bankroll = evaluate_dqn_bankroll(dqn_models, num_games=10000, max_decks=6, initial_money=100)\n",
    "df_dqn_bankroll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3347af83",
   "metadata": {},
   "source": [
    "# Set PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "513ec849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# PPO Actor-Critic Network\n",
    "class PPOActorCritic(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(PPOActorCritic, self).__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.policy_head = nn.Linear(64, output_dim)\n",
    "        self.value_head = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared_out = self.shared(x)\n",
    "        logits = self.policy_head(shared_out)\n",
    "        value = self.value_head(shared_out)\n",
    "        return logits, value\n",
    "\n",
    "# Adjusted for BlackjackEnv\n",
    "def preprocess_state(state):\n",
    "    \"\"\"\n",
    "    Convert ((card1, card2), dealer_card, usable_ace) => [player_sum, dealer_card, usable_ace]\n",
    "    \"\"\"\n",
    "    player_cards, dealer_card, usable_ace = state\n",
    "    player_sum = sum(player_cards)\n",
    "    return np.array([player_sum, dealer_card, usable_ace], dtype=np.float32)\n",
    "\n",
    "# Compute GAE\n",
    "def compute_gae(rewards, values, dones, gamma=0.99, lam=0.95):\n",
    "    returns = []\n",
    "    advantages = []\n",
    "    gae = 0\n",
    "    next_value = 0\n",
    "\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * next_value * (1 - dones[step]) - values[step]\n",
    "        gae = delta + gamma * lam * (1 - dones[step]) * gae\n",
    "        advantages.insert(0, gae)\n",
    "        returns.insert(0, gae + values[step])\n",
    "        next_value = values[step]\n",
    "\n",
    "    return torch.FloatTensor(returns), torch.FloatTensor(advantages)\n",
    "\n",
    "# PPO Training Function\n",
    "def train_ppo(env, n_episodes=5000, gamma=0.99, lam=0.95, clip_eps=0.2,\n",
    "              lr=3e-4, epochs=4, batch_size=64, model_save_path='ppo_blackjack.pth'):\n",
    "\n",
    "    input_dim = 3\n",
    "    output_dim = env.action_space.n\n",
    "\n",
    "    policy_net = PPOActorCritic(input_dim, output_dim)\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "\n",
    "    memory = []\n",
    "    # Add a reward window for tracking rolling average\n",
    "    reward_window = deque(maxlen=100)\n",
    "\n",
    "    best_avg_reward = float('-inf')\n",
    "    best_model = None\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        obs = env.reset()\n",
    "        state = preprocess_state(obs)\n",
    "        done = False\n",
    "        episode_data = []\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            state_tensor = torch.FloatTensor(state)\n",
    "            logits, value = policy_net(state_tensor)\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            dist = torch.distributions.Categorical(probs)\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action)\n",
    "\n",
    "            next_obs, reward, done, _ = env.step(action.item())\n",
    "            next_state = preprocess_state(next_obs)\n",
    "\n",
    "            episode_data.append((state, action.item(), reward, log_prob.item(), value.item(), done))\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        memory.extend(episode_data)\n",
    "        # Add episode reward to the rolling window\n",
    "        reward_window.append(episode_reward)\n",
    "\n",
    "        if len(memory) >= batch_size:\n",
    "            states, actions, rewards, old_log_probs, values, dones = zip(*memory)\n",
    "\n",
    "            returns, advantages = compute_gae(rewards, values, dones, gamma, lam)\n",
    "\n",
    "            states_tensor = torch.FloatTensor(np.array(states))\n",
    "            actions_tensor = torch.LongTensor(actions)\n",
    "            old_log_probs_tensor = torch.FloatTensor(old_log_probs)\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "            for _ in range(epochs):\n",
    "                logits, value_preds = policy_net(states_tensor)\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                dist = torch.distributions.Categorical(probs)\n",
    "\n",
    "                new_log_probs = dist.log_prob(actions_tensor)\n",
    "                ratio = torch.exp(new_log_probs - old_log_probs_tensor)\n",
    "\n",
    "                policy_loss = -torch.min(\n",
    "                    ratio * advantages,\n",
    "                    torch.clamp(ratio, 1 - clip_eps, 1 + clip_eps) * advantages\n",
    "                ).mean()\n",
    "\n",
    "                value_loss = nn.MSELoss()(value_preds.squeeze(), returns)\n",
    "\n",
    "                loss = policy_loss + 0.5 * value_loss - 0.01 * dist.entropy().mean()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            memory = []\n",
    "\n",
    "        # Save model based on rolling average instead of single episode reward\n",
    "        if len(reward_window) == reward_window.maxlen:  # Wait until window is full\n",
    "            avg_reward = np.mean(reward_window)\n",
    "            if avg_reward > best_avg_reward:\n",
    "                best_avg_reward = avg_reward\n",
    "                best_model = policy_net\n",
    "                torch.save({\n",
    "                    'model_state_dict': best_model.state_dict(),\n",
    "                    'avg_reward': best_avg_reward,\n",
    "                    'episode': episode + 1\n",
    "                }, model_save_path)\n",
    "                # print(f\"✅ Best model saved at episode {episode+1} | Avg Reward: {best_avg_reward:.4f}\")\n",
    "\n",
    "        if (episode + 1) % 10000 == 0:\n",
    "            current_avg = np.mean(list(reward_window)) if reward_window else 0\n",
    "            print(f\"Episode {episode+1} | Avg Reward: {current_avg:.4f}\")\n",
    "    \n",
    "    checkpoint = torch.load(model_save_path)\n",
    "    # model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    # model.eval()\n",
    "    print(f\"Best Model at {checkpoint.get('episode', 'N/A')} | Avg Reward: {checkpoint.get('avg_reward', 'N/A')}\")\n",
    "    \n",
    "    # checkpoint = torch.load(model_path)\n",
    "    #         model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    #         model.eval()\n",
    "    #         print(f\"✅ Loaded model for {deck} deck(s) | Avg Reward: {checkpoint.get('avg_reward', 'N/A')}\")\n",
    "    #     else:\n",
    "    return best_model if best_model else policy_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ab8f75fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training PPO model for 1 deck(s) ===\n",
      "Episode 10000 | Avg Reward: -0.0300\n",
      "Episode 20000 | Avg Reward: -0.1000\n",
      "Episode 30000 | Avg Reward: 0.1900\n",
      "Episode 40000 | Avg Reward: 0.0500\n",
      "Episode 50000 | Avg Reward: -0.0100\n",
      "Best Model at 29893 | Avg Reward: 0.42\n",
      "Completed PPO training for 1 deck(s)\n",
      "\n",
      "=== Training PPO model for 2 deck(s) ===\n",
      "Episode 10000 | Avg Reward: 0.0200\n",
      "Episode 20000 | Avg Reward: -0.0600\n",
      "Episode 30000 | Avg Reward: -0.0200\n",
      "Episode 40000 | Avg Reward: 0.1100\n",
      "Episode 50000 | Avg Reward: 0.1600\n",
      "Best Model at 10305 | Avg Reward: 0.37\n",
      "Completed PPO training for 2 deck(s)\n",
      "\n",
      "=== Training PPO model for 3 deck(s) ===\n",
      "Episode 10000 | Avg Reward: -0.0500\n",
      "Episode 20000 | Avg Reward: 0.0100\n",
      "Episode 30000 | Avg Reward: 0.0400\n",
      "Episode 40000 | Avg Reward: 0.1300\n",
      "Episode 50000 | Avg Reward: 0.1300\n",
      "Best Model at 14370 | Avg Reward: 0.42\n",
      "Completed PPO training for 3 deck(s)\n",
      "\n",
      "=== Training PPO model for 4 deck(s) ===\n",
      "Episode 10000 | Avg Reward: 0.1700\n",
      "Episode 20000 | Avg Reward: 0.0700\n",
      "Episode 30000 | Avg Reward: 0.1700\n",
      "Episode 40000 | Avg Reward: 0.0800\n",
      "Episode 50000 | Avg Reward: 0.0500\n",
      "Best Model at 46880 | Avg Reward: 0.49\n",
      "Completed PPO training for 4 deck(s)\n",
      "\n",
      "=== Training PPO model for 5 deck(s) ===\n",
      "Episode 10000 | Avg Reward: 0.1800\n",
      "Episode 20000 | Avg Reward: -0.0800\n",
      "Episode 30000 | Avg Reward: -0.0600\n",
      "Episode 40000 | Avg Reward: -0.0100\n",
      "Episode 50000 | Avg Reward: 0.0100\n",
      "Best Model at 43899 | Avg Reward: 0.45\n",
      "Completed PPO training for 5 deck(s)\n",
      "\n",
      "=== Training PPO model for 6 deck(s) ===\n",
      "Episode 10000 | Avg Reward: 0.0100\n",
      "Episode 20000 | Avg Reward: -0.0700\n",
      "Episode 30000 | Avg Reward: 0.0600\n",
      "Episode 40000 | Avg Reward: -0.0700\n",
      "Episode 50000 | Avg Reward: 0.0700\n",
      "Best Model at 42887 | Avg Reward: 0.41\n",
      "Completed PPO training for 6 deck(s)\n",
      "All PPO models trained successfully!\n"
     ]
    }
   ],
   "source": [
    "# Train models for different deck counts\n",
    "ppo_models = {}\n",
    "\n",
    "for num_decks in range(1, 7):\n",
    "    print(f\"\\n=== Training PPO model for {num_decks} deck(s) ===\")\n",
    "    env = BlackjackEnv(numdecks=num_decks, natural=False)\n",
    "    model_save_path = f\"blackjack_ppo_decks_{num_decks}.pth\"\n",
    "    ppo_model = train_ppo(env, n_episodes=50000, model_save_path=model_save_path)\n",
    "    ppo_models[num_decks] = ppo_model\n",
    "    print(f\"Completed PPO training for {num_decks} deck(s)\")\n",
    "\n",
    "print(\"All PPO models trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dd75b27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10000 | Avg Reward: 0.1100\n",
      "Episode 20000 | Avg Reward: 0.1100\n",
      "Episode 30000 | Avg Reward: 0.1000\n",
      "Episode 40000 | Avg Reward: 0.0600\n",
      "Episode 50000 | Avg Reward: 0.1600\n",
      "Best Model at 14932 | Avg Reward: 0.4\n"
     ]
    }
   ],
   "source": [
    "env = BlackjackEnv(numdecks=3, natural=False)\n",
    "model_save_path = f\"blackjack_ppo_decks_{num_decks}.pth\"\n",
    "ppo_model = train_ppo(env, n_episodes=50000, model_save_path=model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "840e67b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# === PPO Evaluation on Deck Sizes ===\n",
    "def evaluate_ppo_on_deck_sizes(models, num_games=10000, max_decks=6):\n",
    "    results = []\n",
    "\n",
    "    for num_deck in range(1, max_decks + 1):\n",
    "        env = BlackjackEnv(numdecks=num_deck, natural=False)\n",
    "        policy_net = models[num_deck]  # Get the specific model for this deck size\n",
    "\n",
    "        wins = 0\n",
    "        losses = 0\n",
    "        draws = 0\n",
    "        total_reward = 0\n",
    "        count_natural_player = 0\n",
    "        count_natural_dealer = 0\n",
    "\n",
    "        for game in range(1, num_games+1):\n",
    "            obs = env.reset(seed=game)\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                # Process state to match training format\n",
    "                state = preprocess_state(obs)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    state_tensor = torch.FloatTensor(state)\n",
    "                    logits, _ = policy_net(state_tensor)\n",
    "                    probs = torch.softmax(logits, dim=-1)\n",
    "                    \n",
    "                    # For evaluation, choose the action with highest probability\n",
    "                    action = torch.argmax(probs).item()\n",
    "                    \n",
    "                    # Keep action in valid range [0, 1] for simple env\n",
    "                    action = min(action, 1)  \n",
    "                \n",
    "                next_obs, reward, done, _ = env.step(action)\n",
    "                episode_reward += reward\n",
    "                obs = next_obs\n",
    "                \n",
    "                # Check for naturals when game ends\n",
    "                if done:\n",
    "                    if hasattr(env, 'hands') and len(env.hands) > 0:\n",
    "                        if is_natural(env.hands[0]):\n",
    "                            count_natural_player += 1\n",
    "                    \n",
    "                    if hasattr(env, 'dealer'):\n",
    "                        if is_natural(env.dealer):\n",
    "                            count_natural_dealer += 1\n",
    "\n",
    "            total_reward += episode_reward\n",
    "            if episode_reward > 0:\n",
    "                wins += 1\n",
    "            elif episode_reward < 0:\n",
    "                losses += 1\n",
    "            else:\n",
    "                draws += 1\n",
    "\n",
    "        # Store results\n",
    "        results.append({\n",
    "            \"Decks\": num_deck,\n",
    "            \"Games\": num_games,\n",
    "            \"Wins\": wins,\n",
    "            \"Draws\": draws,\n",
    "            \"Losses\": losses,\n",
    "            \"Total Reward\": round(total_reward, 4),\n",
    "            \"Win Rate (%)\": round((wins / num_games) * 100, 4),\n",
    "            \"Loss Rate (%)\": round((losses / num_games) * 100, 4),\n",
    "            \"Draw Rate (%)\": round((draws / num_games) * 100, 4),\n",
    "            \"Average Reward\": round(total_reward / num_games, 4),\n",
    "            \"Natural Player\": count_natural_player,\n",
    "            \"Natural Dealer\": count_natural_dealer\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def evaluate_ppo_bankroll(models, num_games=10000, max_decks=6, initial_money=100):\n",
    "    \"\"\"\n",
    "    Evaluate PPO models with a bankroll simulation across different deck sizes\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for num_deck in range(1, max_decks + 1):\n",
    "        env = BlackjackEnv(numdecks=num_deck, natural=False)\n",
    "        policy_net = models[num_deck]  # Get the specific model for this deck size\n",
    "        policy_net.eval()  # Set the model to evaluation mode\n",
    "\n",
    "        money = initial_money\n",
    "        wins = 0\n",
    "        losses = 0\n",
    "        draws = 0\n",
    "        total_reward = 0\n",
    "        count_natural_player = 0\n",
    "        count_natural_dealer = 0\n",
    "        \n",
    "        # For tracking bankruptcy\n",
    "        games_played = 0\n",
    "        went_bankrupt = False\n",
    "\n",
    "        for game in range(1, num_games+1):\n",
    "            if money <= 0:\n",
    "                went_bankrupt = True\n",
    "                games_played = game - 1\n",
    "                break\n",
    "                \n",
    "            games_played = game\n",
    "            obs = env.reset(seed=game)\n",
    "            done = False\n",
    "            \n",
    "            # Bet $1\n",
    "            money -= 1\n",
    "            episode_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                # Process state to match training format\n",
    "                state = preprocess_state(obs)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    state_tensor = torch.FloatTensor(state)\n",
    "                    logits, _ = policy_net(state_tensor)\n",
    "                    probs = torch.softmax(logits, dim=-1)\n",
    "                    \n",
    "                    # For simple environment, use only actions 0 and 1\n",
    "                    masked_logits = logits.clone()\n",
    "                    for i in range(len(masked_logits)):\n",
    "                        if i > 1:  # Only keep stick (0) and hit (1)\n",
    "                            masked_logits[i] = float('-inf')\n",
    "                    \n",
    "                    probs = torch.softmax(masked_logits, dim=-1)\n",
    "                    action = torch.argmax(probs).item()\n",
    "                \n",
    "                # Execute the action\n",
    "                next_obs, reward, done, _ = env.step(action)\n",
    "                episode_reward += reward\n",
    "                obs = next_obs\n",
    "                \n",
    "                # Check for naturals when game ends\n",
    "                if done:\n",
    "                    if hasattr(env, 'hands') and len(env.hands) > 0:\n",
    "                        if is_natural(env.hands[0]):\n",
    "                            count_natural_player += 1\n",
    "                    \n",
    "                    if hasattr(env, 'dealer'):\n",
    "                        if is_natural(env.dealer):\n",
    "                            count_natural_dealer += 1\n",
    "\n",
    "            # End of episode accounting\n",
    "            total_reward += episode_reward\n",
    "            \n",
    "            # Update wins/losses/draws and bankroll\n",
    "            if episode_reward > 0:\n",
    "                wins += 1\n",
    "                money += 2  # Return original $1 bet plus $1 winnings\n",
    "            elif episode_reward < 0:\n",
    "                losses += 1\n",
    "                # Money already subtracted for bet\n",
    "            else:\n",
    "                draws += 1\n",
    "                money += 1  # Return original bet for a push\n",
    "\n",
    "        # Store results\n",
    "        bankruptcy_message = f\"Bankrupt after {games_played} games\" if went_bankrupt else \"Solvent\"\n",
    "        \n",
    "        results.append({\n",
    "            \"Decks\": num_deck,\n",
    "            \"Games\": games_played,\n",
    "            \"Wins\": wins,\n",
    "            \"Draws\": draws,\n",
    "            \"Losses\": losses,\n",
    "            \"Total Reward\": round(total_reward, 4),\n",
    "            \"Win Rate (%)\": round((wins / games_played) * 100, 4) if games_played > 0 else 0,\n",
    "            \"Loss Rate (%)\": round((losses / games_played) * 100, 4) if games_played > 0 else 0,\n",
    "            \"Draw Rate (%)\": round((draws / games_played) * 100, 4) if games_played > 0 else 0,\n",
    "            \"Average Reward\": round(total_reward / games_played, 4) if games_played > 0 else 0,\n",
    "            \"Final Money\": round(money, 2),\n",
    "            \"Natural Player\": count_natural_player,\n",
    "            \"Natural Dealer\": count_natural_dealer,\n",
    "            \"Status\": bankruptcy_message\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "66dc1c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded model for 1 deck(s) | Avg Reward: 0.42\n",
      "✅ Loaded model for 2 deck(s) | Avg Reward: 0.37\n",
      "✅ Loaded model for 3 deck(s) | Avg Reward: 0.42\n",
      "✅ Loaded model for 4 deck(s) | Avg Reward: 0.49\n",
      "✅ Loaded model for 5 deck(s) | Avg Reward: 0.45\n",
      "✅ Loaded model for 6 deck(s) | Avg Reward: 0.38\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Decks</th>\n",
       "      <th>Games</th>\n",
       "      <th>Wins</th>\n",
       "      <th>Draws</th>\n",
       "      <th>Losses</th>\n",
       "      <th>Total Reward</th>\n",
       "      <th>Win Rate (%)</th>\n",
       "      <th>Loss Rate (%)</th>\n",
       "      <th>Draw Rate (%)</th>\n",
       "      <th>Average Reward</th>\n",
       "      <th>Natural Player</th>\n",
       "      <th>Natural Dealer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>4685</td>\n",
       "      <td>683</td>\n",
       "      <td>4632</td>\n",
       "      <td>53.0</td>\n",
       "      <td>46.85</td>\n",
       "      <td>46.32</td>\n",
       "      <td>6.83</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>503</td>\n",
       "      <td>478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>10000</td>\n",
       "      <td>4686</td>\n",
       "      <td>656</td>\n",
       "      <td>4658</td>\n",
       "      <td>28.0</td>\n",
       "      <td>46.86</td>\n",
       "      <td>46.58</td>\n",
       "      <td>6.56</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>513</td>\n",
       "      <td>463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>10000</td>\n",
       "      <td>4621</td>\n",
       "      <td>645</td>\n",
       "      <td>4734</td>\n",
       "      <td>-113.0</td>\n",
       "      <td>46.21</td>\n",
       "      <td>47.34</td>\n",
       "      <td>6.45</td>\n",
       "      <td>-0.0113</td>\n",
       "      <td>469</td>\n",
       "      <td>486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>10000</td>\n",
       "      <td>4676</td>\n",
       "      <td>683</td>\n",
       "      <td>4641</td>\n",
       "      <td>35.0</td>\n",
       "      <td>46.76</td>\n",
       "      <td>46.41</td>\n",
       "      <td>6.83</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>483</td>\n",
       "      <td>503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>10000</td>\n",
       "      <td>4613</td>\n",
       "      <td>758</td>\n",
       "      <td>4629</td>\n",
       "      <td>-16.0</td>\n",
       "      <td>46.13</td>\n",
       "      <td>46.29</td>\n",
       "      <td>7.58</td>\n",
       "      <td>-0.0016</td>\n",
       "      <td>483</td>\n",
       "      <td>502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>10000</td>\n",
       "      <td>4731</td>\n",
       "      <td>655</td>\n",
       "      <td>4614</td>\n",
       "      <td>117.0</td>\n",
       "      <td>47.31</td>\n",
       "      <td>46.14</td>\n",
       "      <td>6.55</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>491</td>\n",
       "      <td>476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Decks  Games  Wins  Draws  Losses  Total Reward  Win Rate (%)  \\\n",
       "0      1  10000  4685    683    4632          53.0         46.85   \n",
       "1      2  10000  4686    656    4658          28.0         46.86   \n",
       "2      3  10000  4621    645    4734        -113.0         46.21   \n",
       "3      4  10000  4676    683    4641          35.0         46.76   \n",
       "4      5  10000  4613    758    4629         -16.0         46.13   \n",
       "5      6  10000  4731    655    4614         117.0         47.31   \n",
       "\n",
       "   Loss Rate (%)  Draw Rate (%)  Average Reward  Natural Player  \\\n",
       "0          46.32           6.83          0.0053             503   \n",
       "1          46.58           6.56          0.0028             513   \n",
       "2          47.34           6.45         -0.0113             469   \n",
       "3          46.41           6.83          0.0035             483   \n",
       "4          46.29           7.58         -0.0016             483   \n",
       "5          46.14           6.55          0.0117             491   \n",
       "\n",
       "   Natural Dealer  \n",
       "0             478  \n",
       "1             463  \n",
       "2             486  \n",
       "3             503  \n",
       "4             502  \n",
       "5             476  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_ppo_models(base_path, max_decks=6):\n",
    "    \"\"\"Load all PPO models for different deck sizes\"\"\"\n",
    "    models = {}\n",
    "    for deck in range(1, max_decks + 1):\n",
    "        # Define input dim for your specific environment\n",
    "        input_dim = 3  # player_sum, dealer_card, usable_ace\n",
    "        output_dim = 2  # Stick, Hit\n",
    "        \n",
    "        model = PPOActorCritic(input_dim, output_dim)\n",
    "        model_path = f\"blackjack_ppo_decks_{deck}.pth\"\n",
    "        \n",
    "        if os.path.exists(model_path):\n",
    "            checkpoint = torch.load(model_path)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            model.eval()\n",
    "            print(f\"✅ Loaded model for {deck} deck(s) | Avg Reward: {checkpoint.get('avg_reward', 'N/A')}\")\n",
    "        else:\n",
    "            print(f\"⚠️ No model found for {deck} deck(s) at {model_path}\")\n",
    "        \n",
    "        models[deck] = model\n",
    "    \n",
    "    return models\n",
    "\n",
    "max_decks = 6\n",
    "ppo_models = load_ppo_models(\"./\", max_decks)\n",
    "df_ppo_eval = evaluate_ppo_on_deck_sizes(ppo_models, num_games=10000, max_decks=max_decks)\n",
    "df_ppo_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "738f4c37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Decks</th>\n",
       "      <th>Games</th>\n",
       "      <th>Wins</th>\n",
       "      <th>Draws</th>\n",
       "      <th>Losses</th>\n",
       "      <th>Total Reward</th>\n",
       "      <th>Win Rate (%)</th>\n",
       "      <th>Loss Rate (%)</th>\n",
       "      <th>Draw Rate (%)</th>\n",
       "      <th>Average Reward</th>\n",
       "      <th>Final Money</th>\n",
       "      <th>Natural Player</th>\n",
       "      <th>Natural Dealer</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>4698</td>\n",
       "      <td>682</td>\n",
       "      <td>4620</td>\n",
       "      <td>78.0</td>\n",
       "      <td>46.98</td>\n",
       "      <td>46.2000</td>\n",
       "      <td>6.8200</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>178</td>\n",
       "      <td>507</td>\n",
       "      <td>478</td>\n",
       "      <td>Solvent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>10000</td>\n",
       "      <td>4686</td>\n",
       "      <td>656</td>\n",
       "      <td>4658</td>\n",
       "      <td>28.0</td>\n",
       "      <td>46.86</td>\n",
       "      <td>46.5800</td>\n",
       "      <td>6.5600</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>128</td>\n",
       "      <td>513</td>\n",
       "      <td>463</td>\n",
       "      <td>Solvent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>6941</td>\n",
       "      <td>3188</td>\n",
       "      <td>465</td>\n",
       "      <td>3288</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>45.93</td>\n",
       "      <td>47.3707</td>\n",
       "      <td>6.6993</td>\n",
       "      <td>-0.0144</td>\n",
       "      <td>0</td>\n",
       "      <td>324</td>\n",
       "      <td>331</td>\n",
       "      <td>Bankrupt after 6941 games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>10000</td>\n",
       "      <td>4691</td>\n",
       "      <td>713</td>\n",
       "      <td>4596</td>\n",
       "      <td>95.0</td>\n",
       "      <td>46.91</td>\n",
       "      <td>45.9600</td>\n",
       "      <td>7.1300</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>195</td>\n",
       "      <td>481</td>\n",
       "      <td>483</td>\n",
       "      <td>Solvent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>10000</td>\n",
       "      <td>4613</td>\n",
       "      <td>758</td>\n",
       "      <td>4629</td>\n",
       "      <td>-16.0</td>\n",
       "      <td>46.13</td>\n",
       "      <td>46.2900</td>\n",
       "      <td>7.5800</td>\n",
       "      <td>-0.0016</td>\n",
       "      <td>84</td>\n",
       "      <td>483</td>\n",
       "      <td>502</td>\n",
       "      <td>Solvent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>10000</td>\n",
       "      <td>4731</td>\n",
       "      <td>655</td>\n",
       "      <td>4614</td>\n",
       "      <td>117.0</td>\n",
       "      <td>47.31</td>\n",
       "      <td>46.1400</td>\n",
       "      <td>6.5500</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>217</td>\n",
       "      <td>491</td>\n",
       "      <td>476</td>\n",
       "      <td>Solvent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Decks  Games  Wins  Draws  Losses  Total Reward  Win Rate (%)  \\\n",
       "0      1  10000  4698    682    4620          78.0         46.98   \n",
       "1      2  10000  4686    656    4658          28.0         46.86   \n",
       "2      3   6941  3188    465    3288        -100.0         45.93   \n",
       "3      4  10000  4691    713    4596          95.0         46.91   \n",
       "4      5  10000  4613    758    4629         -16.0         46.13   \n",
       "5      6  10000  4731    655    4614         117.0         47.31   \n",
       "\n",
       "   Loss Rate (%)  Draw Rate (%)  Average Reward  Final Money  Natural Player  \\\n",
       "0        46.2000         6.8200          0.0078          178             507   \n",
       "1        46.5800         6.5600          0.0028          128             513   \n",
       "2        47.3707         6.6993         -0.0144            0             324   \n",
       "3        45.9600         7.1300          0.0095          195             481   \n",
       "4        46.2900         7.5800         -0.0016           84             483   \n",
       "5        46.1400         6.5500          0.0117          217             491   \n",
       "\n",
       "   Natural Dealer                     Status  \n",
       "0             478                    Solvent  \n",
       "1             463                    Solvent  \n",
       "2             331  Bankrupt after 6941 games  \n",
       "3             483                    Solvent  \n",
       "4             502                    Solvent  \n",
       "5             476                    Solvent  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ppo_bankroll = evaluate_ppo_bankroll(ppo_models, num_games=10000, max_decks=max_decks, initial_money=100)\n",
    "df_ppo_bankroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e33eb3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "slice() cannot be applied to a 0-dim tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 86\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(results)\n\u001b[0;32m     85\u001b[0m evaluate_strategy_adherence(ppo_models, max_decks\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m)\n\u001b[1;32m---> 86\u001b[0m \u001b[43mevaluate_strategy_adherence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdqn_models\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_decks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[25], line 67\u001b[0m, in \u001b[0;36mevaluate_strategy_adherence\u001b[1;34m(models, max_decks)\u001b[0m\n\u001b[0;32m     65\u001b[0m     logits, _ \u001b[38;5;241m=\u001b[39m policy_net(state_tensor)\n\u001b[0;32m     66\u001b[0m     probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 67\u001b[0m     agent_action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(\u001b[43mprobs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m)\u001b[38;5;241m.\u001b[39mitem()  \u001b[38;5;66;03m# Only consider hit/stand\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m agent_action \u001b[38;5;241m==\u001b[39m basic_action:\n\u001b[0;32m     70\u001b[0m     agreement_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mIndexError\u001b[0m: slice() cannot be applied to a 0-dim tensor."
     ]
    }
   ],
   "source": [
    "def evaluate_strategy_adherence(models, max_decks=6):\n",
    "    \"\"\"Compare agent's strategy against basic strategy for blackjack\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Basic strategy table (simplified example)\n",
    "    # Format: {player_sum: {dealer_upcard: action}}\n",
    "    # 0 = stand, 1 = hit\n",
    "    basic_strategy = {\n",
    "        # Hard totals\n",
    "        21: {card: 0 for card in range(1, 11)},  # Always stand on 21\n",
    "        20: {card: 0 for card in range(1, 11)},  # Always stand on 20\n",
    "        19: {card: 0 for card in range(1, 11)},  # Always stand on 19\n",
    "        18: {card: 0 for card in range(1, 11)},  # Always stand on 18\n",
    "        17: {card: 0 for card in range(1, 11)},  # Always stand on 17\n",
    "        16: {1: 1, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 1, 8: 1, 9: 1, 10: 1},  # Stand vs 2-6, hit vs 7-A\n",
    "        15: {1: 1, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 1, 8: 1, 9: 1, 10: 1},\n",
    "        14: {1: 1, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 1, 8: 1, 9: 1, 10: 1},\n",
    "        13: {1: 1, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 1, 8: 1, 9: 1, 10: 1},\n",
    "        12: {1: 1, 2: 1, 3: 1, 4: 0, 5: 0, 6: 0, 7: 1, 8: 1, 9: 1, 10: 1},\n",
    "        11: {card: 1 for card in range(1, 11)},  # Always hit on 11 or less\n",
    "        10: {card: 1 for card in range(1, 11)},\n",
    "        9: {card: 1 for card in range(1, 11)},\n",
    "        8: {card: 1 for card in range(1, 11)},\n",
    "        # Soft totals with usable_ace=1\n",
    "        'A,9': {card: 0 for card in range(1, 11)},  # Always stand\n",
    "        'A,8': {card: 0 for card in range(1, 11)},  # Always stand\n",
    "        'A,7': {1: 1, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 1, 8: 1, 9: 1, 10: 1},\n",
    "        'A,6': {1: 1, 2: 1, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 1, 10: 1},\n",
    "        'A,5': {1: 1, 2: 1, 3: 1, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 1},\n",
    "        'A,4': {1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 0, 7: 0, 8: 0, 9: 0, 10: 1},\n",
    "        'A,3': {1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 0, 9: 0, 10: 1},\n",
    "        'A,2': {1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 0, 10: 1},\n",
    "    }\n",
    "    \n",
    "    for num_deck in range(1, max_decks + 1):\n",
    "        policy_net = models[num_deck]\n",
    "        policy_net.eval()\n",
    "        \n",
    "        agreement_count = 0\n",
    "        total_decisions = 0\n",
    "        \n",
    "        # Test all player sums (4-21) vs all dealer upcards (1-10)\n",
    "        for player_sum in range(4, 22):\n",
    "            for dealer_card in range(1, 11):\n",
    "                for usable_ace in [0, 1]:\n",
    "                    # Skip invalid combinations\n",
    "                    if usable_ace == 1 and player_sum < 12:\n",
    "                        continue\n",
    "                    \n",
    "                    # Get basic strategy recommendation\n",
    "                    if usable_ace == 1 and player_sum <= 21:\n",
    "                        # For soft hands\n",
    "                        ace_value = player_sum - 11\n",
    "                        key = f'A,{ace_value}' if f'A,{ace_value}' in basic_strategy else player_sum\n",
    "                    else:\n",
    "                        key = player_sum\n",
    "                    \n",
    "                    if key in basic_strategy and dealer_card in basic_strategy[key]:\n",
    "                        basic_action = basic_strategy[key][dealer_card]\n",
    "                        \n",
    "                        # Get agent's decision\n",
    "                        state = np.array([player_sum, dealer_card, usable_ace], dtype=np.float32)\n",
    "                        with torch.no_grad():\n",
    "                            state_tensor = torch.FloatTensor(state)\n",
    "                            logits, _ = policy_net(state_tensor)\n",
    "                            probs = torch.softmax(logits, dim=-1)\n",
    "                            agent_action = torch.argmax(probs[:2]).item()  # Only consider hit/stand\n",
    "                        \n",
    "                        if agent_action == basic_action:\n",
    "                            agreement_count += 1\n",
    "                        \n",
    "                        total_decisions += 1\n",
    "        \n",
    "        agreement_percentage = (agreement_count / total_decisions) * 100 if total_decisions > 0 else 0\n",
    "        \n",
    "        results.append({\n",
    "            \"Decks\": num_deck,\n",
    "            \"Total Decisions\": total_decisions,\n",
    "            \"Strategy Agreement\": agreement_count,\n",
    "            \"Agreement Percentage\": round(agreement_percentage, 2),\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "evaluate_strategy_adherence(ppo_models, max_decks=6)\n",
    "evaluate_strategy_adherence(dqn_models, max_decks=6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study_venvs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
